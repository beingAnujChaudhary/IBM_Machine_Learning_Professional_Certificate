{"cells":[{"cell_type":"markdown","source":["## Supervised Machine Learning: Expanded View ü§ñ\n","\n","Supervised machine learning forms the bedrock of many predictive tasks we encounter daily. The core idea is learning from examples where we already know the \"right answer.\"\n","\n","-----\n"],"metadata":{"id":"Xv6mDcPusUE4"}},{"cell_type":"markdown","source":["\n","\n","### The Essence of Machine Learning and AI\n","\n","Machine learning (ML) empowers computers to learn from data without being explicitly programmed for each specific task. This is a significant departure from traditional statistical modeling, where we often have a clearer understanding of the underlying processes and select models to approximate them. In ML, especially when processes are complex or poorly understood, the model itself is derived or approximated directly from the data, enabling predictions about future, unseen data.\n","\n","ML is a key subfield of Artificial Intelligence (AI). AI, in its broadest sense, deals with machines replicating human-like or rational thinking and actions. Machine learning specifically contributes to the \"thought processes\" aspect of AI. The ability to \"learn\" is pinpointed as the fundamental capability from which other AI concepts like thinking, decision-making, perceiving, and reasoning emerge. Everyday applications are abundant, including spam filters, web search rankings, optimized delivery routes, fraud detection, and personalized movie recommendations.\n"],"metadata":{"id":"9VQ_WvWMI8P8"}},{"cell_type":"markdown","source":["\n","#### What is a \"Model\"?\n","\n","In ML, a **model** is a simplified representation that captures the essential aspects of a more complex reality or \"larger thing\". A good model judiciously omits unimportant details while preserving crucial information and relationships. It's about reducing complexity to a manageable level for understanding and representation.\n","\n","**Example:**\n","Imagine predicting house prices. A model might consider square footage and number of bedrooms but ignore the color of the mailbox or the type of flowers in the garden, as these are likely less important for the price.\n","\n","-----\n"],"metadata":{"id":"siHT8bM9JSCk"}},{"cell_type":"markdown","source":["### Parameters vs. Hyperparameters: The Nitty-Gritty\n","\n","Understanding the distinction between parameters and hyperparameters is crucial for effective model building:\n","\n","  \n"],"metadata":{"id":"JJZIFc5NJdMu"}},{"cell_type":"markdown","source":["* **Parameters (or fit parameters)**: These are values *learned* by the model from the training data. They define the specific mapping the model discovers. For instance, in linear regression ($Y = \\beta_0 + \\beta_1X$), $\\beta_0$ (intercept) and $\\beta_1$ (slope) are parameters estimated by the model to define the relationship between input $X$ and outcome $Y$."],"metadata":{"id":"G4dKHUJoJ2sZ"}},{"cell_type":"markdown","source":["* **Hyperparameters**: These are settings or configurations *chosen by the practitioner* before the training process begins. They are not learned from the data directly but guide the learning process itself. Adjusting hyperparameters is a way to optimize a model's performance. An example is deciding whether a linear regression model should fit an intercept or not.\n"],"metadata":{"id":"vG_jMCANJ1CD"}},{"cell_type":"markdown","source":["\n","\n","**Further Examples of Hyperparameters:**\n","\n","  * **k-Nearest Neighbors (k-NN)**: The number of neighbors ('k') to consider.\n","  * **Decision Trees**: The maximum depth of the tree.\n","  * **Neural Networks**: The learning rate, the number of hidden layers, or the number of neurons per layer.\n","\n","-----\n"],"metadata":{"id":"5hMTKeKWKe0p"}},{"cell_type":"markdown","source":["### Core of Supervised Learning: Learning with Labels\n","\n","Supervised machine learning relies on **labeled data**, meaning the input data includes known outcomes or target variables. The primary goal is to train a model on this labeled dataset so it can accurately predict outcomes for new, unseen (unlabeled) data.\n","\n"],"metadata":{"id":"Ev_tLXJjKjEg"}},{"cell_type":"markdown","source":["\n","Supervised learning is broadly categorized based on the nature of the outcome variable:\n","\n"],"metadata":{"id":"HNFF03N4KyqY"}},{"cell_type":"markdown","source":["1.  **Regression**: Used when predicting a **numeric (continuous) value**.\n","      * Examples: Predicting stock prices, box office revenue, a person's exact age, house prices, or the number of attendees at an event.\n","      * **Additional Example**: Predicting the amount of rainfall tomorrow (e.g., in millimeters).\n"],"metadata":{"id":"8TjVtmO2K034"}},{"cell_type":"markdown","source":["2.  **Classification**: Used when the outcome variable is **categorical (discrete classes or labels)**.\n","      * Examples: Face recognition (identifying a specific person), customer churn (predicting if a customer will leave), determining if an email is spam or not, predicting the next word in a sentence, identifying if a transaction is fraudulent, or classifying political affiliation.\n","      * **Additional Example**: Sentiment analysis (classifying a movie review as \"positive,\" \"negative,\" or \"neutral\").\n","\n","For classification, key components include quantifiable features, labeled target variables (which often require significant human effort to create), and a method to measure similarity between new data points and learned ones.\n","\n","-----\n"],"metadata":{"id":"rqmkguKjK5Ps"}},{"cell_type":"markdown","source":["### The Supervised Learning Framework üèóÔ∏è\n","\n","The general process can be represented by the equation:\n","$y_{\\text{predicted}} = f(\\text{parameters}, X)$\n","Where:\n","\n","* $X$: Represents the input features or independent variables.\n","* $y$: The actual outcome variable from the training data.\n","* $\\text{parameters}$ (e.g., $\\Omega$): These are the values the model learns from the data during training.\n","* $f$: The machine learning model or function that uses the learned parameters and input $X$ to generate a prediction.\n","* $y_{\\text{predicted}}$ (or $y_P$): The value predicted by the model for a given input $X$.\n","\n","\n","The model is **trained** on past data (pairs of $X$ and $y$) to find the optimal parameters that best describe the relationship between $X$ and $y$. Generally, the more data available, the better the model can learn these parameters. Once trained, this model, equipped with its learned parameters, can be used to make predictions ($y_P$) on new observations (new $X$ values for which $y$ is unknown).\n"],"metadata":{"id":"eeGKIOslLFpk"}},{"cell_type":"markdown","source":["\n","#### Generalization: The True Test\n","\n","A model's ability to perform well on **new, unseen data** is called **generalization**. To ensure and evaluate this, it's standard practice to train the model on a subset of the available data (the **training set**) and then test its performance on a separate, untouched subset (the **holdout set** or **test set**). This is commonly achieved using a **train-test split**. This evaluation on unseen data provides a more realistic estimate of how the model will perform in real-world scenarios before deployment.\n","\n","-----"],"metadata":{"id":"uuXtvneTL9Bt"}},{"cell_type":"markdown","source":["### Guiding the Learning: Loss Functions and Parameter Updates\n","\n","A **loss function** (often denoted as $J$) is critical in the training process. It quantifies how \"good\" or \"bad\" the model's predictions ($y_P$) are compared to the actual values ($y$). Essentially, it measures the error or discrepancy.\n","\n","The ultimate goal of training is usually to find the set of parameters that **minimizes this loss function**. The **update rule** dictates how the model's parameters are adjusted iteratively during training, typically aiming to reduce the loss with each adjustment.\n","\n","**Example: A Simple Loss Intuition**\n","If a model predicts a house price of USD 300,000 and the actual price is USD 320,000, the error is USD 20,000. If it predicts USD 315,000, the error is USD 5,000. The loss function would assign a higher penalty to the first prediction than the second.\n","\n","-----\n"],"metadata":{"id":"g6JKQyKXL-nK"}},{"cell_type":"markdown","source":["### The Balancing Act: Interpretation vs. Prediction üéØ\n","\n","When developing supervised ML models, a key consideration is the trade-off between **interpretation** and **prediction**. The project's objective often dictates which to prioritize.\n"],"metadata":{"id":"KU_V1G6nMmT5"}},{"cell_type":"markdown","source":["### **Interpretation Objective**\n","\n","- **Goal**: Gain insights about the data and understand the underlying system mechanisms.\n","- **Focus**:\n","  - Analyzing model parameters (e.g., coefficients in linear regression).\n","  - Identifying the most influential features on the outcome.\n","- **Model Choice**:\n","  - Prefer simpler, more transparent models (e.g., linear models, decision trees).\n","  - May sacrifice some predictive accuracy for interpretability.\n","- **Examples**:\n","  - Understanding sales drivers from customer demographics.\n","  - Assessing marketing spend impact on revenue."],"metadata":{"id":"haIip9oNMsun"}},{"cell_type":"markdown","source":["### **Prediction Objective**  \n","\n","- **Goal**: Achieve the highest possible accuracy in predicting outcomes (minimize difference between $y_P$ and $y$).  \n","- **Focus**:  \n","  - Quantitative performance metrics (e.g., accuracy, precision, recall)\n","- **Model Choice**:  \n","  - May require complex models (deep learning, ensemble methods).  \n","  - Potential trade-off: Interpretability (\"black box\" effect).  \n","- **Examples**:  \n","  - Customer churn prediction.  \n","  - Fraud detection.  \n","  - Loan default forecasting."],"metadata":{"id":"4zYI4RKmM3-l"}},{"cell_type":"markdown","source":["While one objective might be primary, many projects benefit from a balance. Insights from interpretation can enhance predictions (e.g., through feature engineering), and strong predictive performance can increase confidence in the interpretations. However, not all models excel at both; simple models like linear regression are highly interpretable, whereas complex models like deep learning often are not. The choice depends on the specific business need.\n"],"metadata":{"id":"WlKUVbjOMYFZ"}},{"cell_type":"markdown","source":["\n","-----\n","\n","## Linear Regression: The Workhorse üêé\n","\n","Linear Regression is often the first supervised learning model practitioners learn due to its simplicity and interpretability. It's a type of **regression model**, meaning it's used when the target variable is continuous.\n","\n","A linear regression model attempts to establish a **linear relationship** between a continuous target variable and one or more input features (scaled variables). For a single feature, this relationship is typically expressed by the equation:\n","($Y = \\beta_0 + \\beta_1X$), $\\beta_0$ (intercept) and $\\beta_1$ (slope) Where:\n","\n","  * $Y$: The target variable (e.g., box office revenue).\n","  * $X$: The input feature (e.g., marketing budget).\n","  * $\\beta_0$: The **intercept coefficient**. This is the predicted value of $Y$ when $X$ is zero.\n","  * $\\beta_1$: The **slope coefficient**. This represents the change in $Y$ for a one-unit increase in $X$.\n","\n","The goal of training a linear regression model is to find the optimal values for these parameters ($\\beta_0$ and $\\beta_1$) by minimizing a cost function.\n","\n","-----\n"],"metadata":{"id":"ZRMSX7-7OIg-"}},{"cell_type":"markdown","source":["### Measuring Mistakes: Error and Loss in Linear Regression\n","\n","The **error** (or **residual**) for a single observation is the difference between:\n","- Actual value: $y_{\\text{obs}}$\n","- Predicted value: $y_{\\text{predicted}}$ (or $Y_{\\beta}(X)$)\n","\n","$$\n","\\text{Error} = y_{\\text{obs}} - y_{\\text{predicted}}\n","$$\n","\n","Errors can be positive or negative. To prevent these from canceling each other out and to measure the magnitude of error, we typically use measures based on the:\n","\n","  - absolute value (**L1 norm**) or,\n","  - more commonly, the squared value (**L2 norm** or Euclidean distance).\n","\n","#### Mean Squared Error (MSE)\n","The most common cost function for linear regression:\n","\n","$$\n","\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - y_{\\text{predicted},i})^2\n","$$\n","\n","Where:\n","- $m$ = number of observations\n","- MSE averages the squared errors across all training data\n","\n","**Key Properties:**\n","\n","   MSE is the average of the squared errors for all observations in the training set.\n","   \n","   Minimizing MSE finds the line that best fits the data by reducing the average squared distance between the data points and the regression line. The cost function $J$ is often written with a $\\frac{1}{2m}$ factor (instead of $\\frac{1}{m}$) for mathematical convenience during optimization with gradient descent, as the derivative cancels out the '2'.\n"],"metadata":{"id":"lwIH3fZLO56j"}},{"cell_type":"markdown","source":["\n","---\n","\n","### üìä Coefficient of Determination ($R^2$)\n","\n","Another crucial metric for evaluating regression models is the **Coefficient of Determination**, denoted as $ R^2 $ .\n","\n","#### üîç What is $ R^2 $?\n","\n","The coefficient of determination measures the **proportion of the variance** in the dependent variable that can be explained by the independent variable(s) in the model . In other words, it tells us how well the model fits the observed data.\n","\n","Mathematically, it is defined as:\n","\n","$$\n","R^2 = 1 - \\frac{\\text{SSE}}{\\text{TSS}}\n","$$\n","\n","Where:\n","- **SSE (Sum of Squared Errors)**:  \n","  $$\n","  \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n","  $$  \n","  This represents the unexplained variation by the model .\n","  \n","- **TSS (Total Sum of Squares)**:  \n","  $$\n","  \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n","  $$  \n","  where $ \\bar{y} $ is the mean of the actual values. TSS represents the total variability in the outcome variable.\n","\n","#### üß† Interpreting $ R^2 $\n","\n","- The value of $ R^2 $ ranges from **0 to 1**.\n","    - A value of **0** means the model explains none of the variability in the response variable.\n","    - A value of **1** means the model perfectly predicts the response variable.\n","    - **Negative values** are possible when the model performs worse than simply predicting the mean of the target variable.\n","\n","- A higher $ R^2 $ indicates a better fit of the model to the data, meaning a larger proportion of the variance in $ Y $ is captured by the model.\n","\n","#### ‚ö†Ô∏è Limitations of $ R^2 $\n","\n","- **Does not penalize for overfitting**: Adding more predictors will generally increase or keep $ R^2 $ the same, even if those predictors add no real predictive power. For example, if a new feature has no relationship with the target, its coefficient might still slightly improve $ R^2 $ due to random chance.\n","\n","- **Not always comparable across models**: When comparing models with different numbers of features, $ R^2 $ should not be used alone. Instead, consider using **Adjusted $ R^2 $**, which adjusts for the number of predictors in the model .\n","\n","#### ‚úÖ General Applicability\n","\n","- $ R^2 $ is **not limited to linear regression**. It can be used to evaluate the performance of any regression model, including polynomial, decision tree, or ensemble models .\n","\n","\n","\n","This metric is essential for understanding the goodness-of-fit of your regression models but should always be interpreted alongside other evaluation metrics like **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**.\n","---\n"],"metadata":{"id":"BZftP3bYRWtO"}},{"cell_type":"markdown","source":["\n","### Best Practices in Modeling üèÖ\n","\n","Some general best practices for modeling include:\n","\n","1.  **Establish a cost function** to minimize. This provides an objective way to compare different models.\n","2.  **Develop multiple models**. This could involve trying different algorithms or different hyperparameters for the same algorithm.\n","3.  **Compare results** using the cost function and other relevant metrics (like $R^2$ for regression).\n","4.  **Choose the best model** based on the data and the specific project objective (e.g., prediction accuracy vs. interpretability).\n","\n"],"metadata":{"id":"C1rybk04Si-R"}},{"cell_type":"markdown","source":["\n","---\n","## Preparing Data for Linear Regression üõ†Ô∏è\n","\n","Linear regression, while robust, benefits significantly from careful data preparation. Certain data characteristics and preprocessing steps can enhance model performance, the reliability of its coefficients, and the validity of statistical inferences drawn.\n","\n","---\n","### Key Data Preparation Steps\n","\n","* **Normality of Residuals**:\n","    * It's a common misconception that the outcome variable ($y$) or predictor variables ($X$) must be normally distributed. While this can be beneficial, it's not a strict requirement for linear regression itself.\n","    * However, a crucial assumption for valid statistical inference (e.g., calculating p-values, confidence intervals for coefficients) is that the **residuals (the differences between observed and predicted values) are approximately normally distributed**.\n","    * You can check residual normality using visual tools like Q-Q plots and histograms, or statistical tests such as the Shapiro-Wilk or Kolmogorov-Smirnov tests.\n","\n","* **Transforming the Target Variable ($y$)**:\n","    * If residuals exhibit non-normality or if the variance of residuals isn't constant (heteroscedasticity), transforming the target variable ($y$) can often resolve these issues. Common transformations include:\n","        * **Log Transformation** (e.g., $\\log(y)$): Effective when the data is right-skewed, the relationship between variables appears multiplicative, or when the error variance tends to increase with the mean of $y$. This transformation requires $y > 0$.\n","        * **Square Root Transformation** (e.g., $\\sqrt{y}$): Often applied to count data or when the variance is proportional to the mean. This requires $y \\ge 0$.\n","        * **Box-Cox Transformation**: This is a versatile family of power transformations parameterized by $\\lambda$. The optimal $\\lambda$ is typically estimated from the data to find the transformation that best stabilizes variance and makes the variable's distribution closer to normal. The transformation is defined as:\n","            $$y'(\\lambda) = \\begin{cases} \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\log(y) & \\text{if } \\lambda = 0 \\end{cases}$$\n","            This transformation generally requires $y > 0$.\n","    * **Inverse Transformation**: When you transform the target variable before training the model, its predictions will be on that transformed scale. To interpret these predictions in the original units of $y$ or to evaluate model performance accurately (e.g., calculating RMSE in original units), you must apply an **inverse transformation** to the predictions. Performance metrics should ideally be reported on the original, understandable scale.\n","\n","* **Feature Scaling (Standardization/Normalization)**:\n","    * **Standardization** (e.g., using `StandardScaler`) rescales features to have a mean of 0 and a standard deviation of 1 ($z = \\frac{X - \\mu}{\\sigma}$). **Normalization** (e.g., using `MinMaxScaler`) scales features to a specific range, typically [0, 1].\n","    * While Ordinary Least Squares (OLS) linear regression can handle features on different scales (its coefficients will adjust accordingly), scaling is highly beneficial or even crucial in several scenarios:\n","        * **Regularized Regression**: Techniques like Ridge, Lasso, and Elastic Net include penalty terms based on coefficient magnitudes. Scaling ensures these penalties are applied fairly, preventing features with inherently larger scales from being disproportionately penalized.\n","        * **Gradient Descent Optimization**: Many machine learning algorithms, including linear regression solved via gradient descent, converge faster and more reliably when features are on a similar scale. Scaling prevents features with larger numeric ranges from dominating gradient updates.\n","        * **Interpreting Coefficient Magnitudes**: If features are on vastly different scales, directly comparing their coefficients to gauge relative importance is misleading. Scaling brings them to a comparable scale, making their magnitudes more (though still imperfectly) indicative of their influence on the outcome.\n","\n","* **Handling Categorical Features**:\n","    * Linear regression models require all input features to be numeric. Therefore, categorical features (e.g., 'product category', 'geographical region') must be converted into a numerical format. Common strategies include:\n","        * **One-Hot Encoding**: This creates new binary (0 or 1) columns for each unique category level. Each category level effectively becomes a new feature.\n","        * **Dummy Coding**: Similar to one-hot encoding, but it typically omits one category level, making it the reference category. This avoids perfect multicollinearity, especially when an intercept term is included in the regression model.\n","        * Other methods like ordinal encoding (if the categories have an inherent order) or target encoding might also be suitable depending on the specific context.\n","\n","* **Polynomial Features**:\n","    * Standard linear regression models assume a linear relationship between predictors and the outcome. To capture non-linear patterns, you can engineer **polynomial features**.\n","    * This involves creating new features by raising existing features to a power (e.g., $X^2, X^3$) or by creating interaction terms between features (e.g., $X_1 \\times X_2$).\n","    * The model remains linear with respect to these *newly created* features, but the overall relationship it can model between the original $X$ and $y$ becomes non-linear.\n","    * **Caution**: Adding too many polynomial terms or using very high degrees can lead to **overfitting**, where the model fits the noise in the training data too closely and performs poorly on unseen data.\n","\n","* **Addressing Outliers and Multicollinearity**:\n","    * **Outliers**: Linear regression can be sensitive to extreme values (outliers), as they can disproportionately affect the estimated regression line and coefficients. It's wise to detect outliers (using visualizations like box plots, or statistical methods) and decide on an appropriate strategy, such as removal (if they are data entry errors), transformation, or using more robust regression techniques.\n","    * **Multicollinearity**: This occurs when independent variables are highly correlated with each other. High multicollinearity doesn't bias the model's predictions but can inflate the variance of coefficient estimates, making them unstable and difficult to interpret individually. It can be detected using a correlation matrix or Variance Inflation Factor (VIF).\n","\n","---\n"],"metadata":{"id":"yDSRMEdjTN5N"}},{"cell_type":"markdown","source":["### Crucial Order of Operations: Train-Test Split First!\n","\n","It's **critically important** to perform the **train-test split *before*** any data preprocessing steps that learn parameters from the data. This applies to operations such as:\n","\n","* Fitting a `StandardScaler` or `MinMaxScaler`.\n","* Determining the $\\lambda$ parameter for a Box-Cox transformation.\n","* Calculating statistics (like mean or median) for imputing missing values.\n","* Feature selection techniques that evaluate feature importance based on the data.\n","\n","**The Correct Workflow:**\n","\n","1.  **Split Data**: Divide your entire dataset into a training set and a test set.\n","2.  **Fit Preprocessing Tools on Training Data**: Fit your scalers, transformers, imputers, etc., ***only* on the training data**. These tools will learn the necessary parameters (e.g., mean, standard deviation, $\\lambda$ for Box-Cox) exclusively from this training subset.\n","3.  **Transform Both Sets**: Use the **same *fitted*** tools to transform both the training data and the test data.\n","\n","Adhering to this order prevents **data leakage**. Data leakage occurs when information from the test set (e.g., its statistical properties) unintentionally influences the model training or preprocessing phases. This leads to overly optimistic performance estimates on your test set, and your model will likely perform worse on genuinely new, unseen data.\n","\n","---"],"metadata":{"id":"iYzhgrzXVSWj"}},{"cell_type":"markdown","source":["\n","\n","## Interactive Python Examples üêç\n","\n","Let's bring some of these concepts to life with interactive Python snippets using `scikit-learn`.\n","\n","### Example 1: Basic Linear Regression Prediction\n","\n","This example demonstrates a simple linear regression, trains it on a small dataset, and then predicts a value based on user input"],"metadata":{"id":"N2pS2rBxVT1H"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.linear_model import LinearRegression\n","import matplotlib.pyplot as plt\n","\n","# --- Step 1: Prepare some simple data ---\n","# Imagine data for years of experience (X) vs. salary (y in $1000s)\n","X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1) # Years of Experience\n","y_train = np.array([30, 35, 45, 50, 60, 65, 75, 80])         # Salary in $1000s\n","\n","print(\"Training Data (X - Experience):\\n\", X_train)\n","print(\"Training Data (y - Salary):\\n\", y_train)\n","print(\"-\" * 30)\n","\n","# --- Step 2: Visualize the data (Optional but helpful) ---\n","plt.figure(figsize=(6,4))\n","plt.scatter(X_train, y_train, color='blue', label='Actual Data Points')\n","plt.xlabel(\"Years of Experience\")\n","plt.ylabel(\"Salary ($ thousands)\")\n","plt.title(\"Experience vs. Salary\")\n","plt.grid(True)\n","# plt.show() # Usually plt.show() is here, but for interactive console, might skip direct show\n","\n","# --- Step 3: Initiate and Train the Linear Regression model ---\n","lr_model = LinearRegression() # [cite: 103]\n","lr_model.fit(X_train, y_train) # [cite: 106]\n","\n","print(\"Model training complete.\")\n","print(f\"Learned Intercept (Beta 0): {lr_model.intercept_:.2f}\") # [cite: 109]\n","print(f\"Learned Coefficient (Beta 1 for Experience): {lr_model.coef_[0]:.2f}\") # [cite: 109]\n","print(\"-\" * 30)\n","\n","# --- Step 4: Interactive Prediction ---\n","try:\n","    user_experience_str = input(\"Enter years of experience for salary prediction (e.g., 3.5): \")\n","    user_experience = float(user_experience_str)\n","\n","    if user_experience < 0:\n","        print(\"Years of experience cannot be negative.\")\n","    else:\n","        # Needs to be a 2D array for scikit-learn's predict method [cite: 110]\n","        new_X = np.array([[user_experience]])\n","        predicted_salary = lr_model.predict(new_X) # [cite: 110]\n","\n","        print(f\"Predicted Salary for {user_experience} years of experience: ${predicted_salary[0]:.2f}k\")\n","\n","        # Plotting the regression line and the new prediction\n","        plt.plot(X_train, lr_model.predict(X_train), color='red', label='Regression Line')\n","        plt.scatter([user_experience], predicted_salary, color='green', s=100, zorder=5, label=f'Prediction for {user_experience} yrs')\n","        plt.legend()\n","        plt.show()\n","\n","except ValueError:\n","    print(\"Invalid input. Please enter a numeric value for years of experience.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data (X - Experience):\n"," [[1]\n"," [2]\n"," [3]\n"," [4]\n"," [5]\n"," [6]\n"," [7]\n"," [8]]\n","Training Data (y - Salary):\n"," [30 35 45 50 60 65 75 80]\n","------------------------------\n","Model training complete.\n","Learned Intercept (Beta 0): 21.79\n","Learned Coefficient (Beta 1 for Experience): 7.38\n","------------------------------\n","Enter years of experience for salary prediction (e.g., 3.5): 15\n","Predicted Salary for 15.0 years of experience: $132.50k\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"execution_count":2,"metadata":{"id":"huCkPe54sUE8","colab":{"base_uri":"https://localhost:8080/","height":737},"executionInfo":{"status":"ok","timestamp":1748829047488,"user_tz":-330,"elapsed":4806,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"51f5c8e9-515c-46ab-a264-8a1756e5b0d1"}},{"cell_type":"markdown","source":["**Reasoning behind the output:**\n","\n","  * The model learns an intercept ($\\beta_0$) and a coefficient ($\\beta_1$) from the training data (`X_train`, `y_train`) by minimizing the Mean Squared Error.\n","  * The intercept is the predicted salary for 0 years of experience.\n","  * The coefficient indicates how much the salary is expected to increase for each additional year of experience.\n","  * When you provide new \"years of experience,\" the model uses the formula: `predicted_salary = intercept + coefficient * user_experience` to make a prediction. The plot visualizes this learned linear relationship and where the new prediction falls.\n","\n","-----\n","\n","### Example 2: Train-Test Split Demonstration\n","\n","This shows how to split data into training and testing sets, a fundamental step for model evaluation."],"metadata":{"id":"bopEHH38sUE9"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# --- Step 1: Sample Data ---\n","# Imagine 10 data points: features (X) and a target (y)\n","X_data = np.array([[1,10], [2,12], [3,11], [4,15], [5,14],\n","                   [6,18], [7,19], [8,17], [9,22], [10,21]])\n","y_data = np.array([5, 7, 6, 9, 8, 11, 12, 10, 14, 13])\n","\n","print(\"Original X_data shape:\", X_data.shape)\n","print(\"Original y_data shape:\", y_data.shape)\n","print(\"-\" * 30)\n","\n","# --- Step 2: Interactive Test Size Input ---\n","test_size_percentage_str = input(\"Enter the percentage for the test set (e.g., 20 for 20%): \")\n","\n","try:\n","    test_size_percentage = float(test_size_percentage_str)\n","    if not (0 < test_size_percentage < 100):\n","        print(\"Please enter a percentage between 0 and 100 (exclusive). Using default 25%.\")\n","        test_proportion = 0.25\n","    else:\n","        test_proportion = test_size_percentage / 100.0\n","\n","    # --- Step 3: Perform the Train-Test Split ---\n","    # random_state ensures reproducibility of the split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X_data, y_data, test_size=test_proportion, random_state=42\n","    )\n","\n","    print(f\"Splitting data with test_size = {test_proportion:.2f}\")\n","    print(\"-\" * 30)\n","    print(\"Shape of X_train:\", X_train.shape)\n","    print(\"Shape of X_test:\", X_test.shape)\n","    print(\"Shape of y_train:\", y_train.shape)\n","    print(\"Shape of y_test:\", y_test.shape)\n","    print(\"-\" * 30)\n","    print(\"X_train sample:\\n\", X_train[:2]) # Show first 2 samples\n","    print(\"X_test sample:\\n\", X_test[:2])  # Show first 2 samples\n","\n","except ValueError:\n","    print(\"Invalid input. Please enter a numeric value for the percentage.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Original X_data shape: (10, 2)\n","Original y_data shape: (10,)\n","------------------------------\n","Enter the percentage for the test set (e.g., 20 for 20%): 15\n","Splitting data with test_size = 0.15\n","------------------------------\n","Shape of X_train: (8, 2)\n","Shape of X_test: (2, 2)\n","Shape of y_train: (8,)\n","Shape of y_test: (2,)\n","------------------------------\n","X_train sample:\n"," [[ 6 18]\n"," [ 1 10]]\n","X_test sample:\n"," [[ 9 22]\n"," [ 2 12]]\n"]}],"execution_count":3,"metadata":{"id":"S_48LvxRsUE9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748829133977,"user_tz":-330,"elapsed":2483,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"ccc943d2-e11d-4f7d-f850-1a09f9c79564"}},{"cell_type":"markdown","source":["**Reasoning behind the output:**\n","\n","  * `train_test_split` randomly shuffles and divides the `X_data` and `y_data` into two sets: training and testing.\n","  * `test_size` (e.g., `0.25` or based on user input) determines the proportion of data allocated to the test set. The remaining data forms the training set.\n","  * `random_state=42` is a seed for the random number generator. Using the same seed ensures that you get the exact same split every time you run the code, which is useful for reproducibility. If you omit it, you'll get a different random split each time.\n","  * The output shows the number of samples (rows) in each new array, reflecting the split ratio. The training set is used to fit the model, and the test set is used to evaluate its performance on unseen data.\n","\n","-----\n","\n","### Example 3: StandardScaler in Action\n","\n","This demonstrates how `StandardScaler` transforms features to have zero mean and unit variance."],"metadata":{"id":"oylNditlsUE-"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# --- Step 1: Sample Data with different scales ---\n","# Feature 1: Small values, Feature 2: Large values\n","data = np.array([\n","    [1, 100],\n","    [2, 200],\n","    [3, 300],\n","    [4, 400],\n","    [5, 500]\n","])\n","\n","# For demonstration, let's assume this 'data' is our training set for fitting the scaler\n","# In a real scenario, you'd split first, then fit scaler on X_train only [cite: 85]\n","X_train_sample = data\n","\n","print(\"Original Training Data (X_train_sample):\\n\", X_train_sample)\n","print(f\"Mean of original Feature 1: {X_train_sample[:, 0].mean():.2f}, Std Dev: {X_train_sample[:, 0].std():.2f}\")\n","print(f\"Mean of original Feature 2: {X_train_sample[:, 1].mean():.2f}, Std Dev: {X_train_sample[:, 1].std():.2f}\")\n","print(\"-\" * 30)\n","\n","# --- Step 2: Initialize and Fit StandardScaler ---\n","scaler = StandardScaler()\n","# Fit the scaler ONLY on the training data [cite: 85]\n","scaler.fit(X_train_sample)\n","\n","print(\"Scaler fitted on training data.\")\n","print(f\"Scaler learned mean: {scaler.mean_}\")\n","print(f\"Scaler learned scale (std dev): {scaler.scale_}\") # This is the standard deviation\n","print(\"-\" * 30)\n","\n","# --- Step 3: Transform the training data ---\n","X_train_scaled = scaler.transform(X_train_sample)\n","print(\"Scaled Training Data (X_train_scaled):\\n\", X_train_scaled)\n","print(f\"Mean of scaled Feature 1: {X_train_scaled[:, 0].mean():.2f}, Std Dev: {X_train_scaled[:, 0].std():.2f}\")\n","print(f\"Mean of scaled Feature 2: {X_train_scaled[:, 1].mean():.2f}, Std Dev: {X_train_scaled[:, 1].std():.2f}\")\n","print(\"-\" * 30)\n","\n","# --- Step 4: Interactive: Transform new, unseen data (like a test set) ---\n","print(\"Now, let's see how the FITTED scaler transforms new data.\")\n","try:\n","    feat1_str = input(\"Enter a value for Feature 1 for new data point (e.g., 6): \")\n","    feat2_str = input(\"Enter a value for Feature 2 for new data point (e.g., 600): \")\n","\n","    feat1 = float(feat1_str)\n","    feat2 = float(feat2_str)\n","\n","    new_data_point = np.array([[feat1, feat2]])\n","    print(\"\\nNew data point (original scale):\\n\", new_data_point)\n","\n","    # Use the SAME FITTED scaler to transform the new data [cite: 85]\n","    new_data_point_scaled = scaler.transform(new_data_point)\n","    print(\"New data point (scaled by the trained scaler):\\n\", new_data_point_scaled)\n","\n","except ValueError:\n","    print(\"Invalid input. Please enter numeric values for the features.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Original Training Data (X_train_sample):\n"," [[  1 100]\n"," [  2 200]\n"," [  3 300]\n"," [  4 400]\n"," [  5 500]]\n","Mean of original Feature 1: 3.00, Std Dev: 1.41\n","Mean of original Feature 2: 300.00, Std Dev: 141.42\n","------------------------------\n","Scaler fitted on training data.\n","Scaler learned mean: [  3. 300.]\n","Scaler learned scale (std dev): [  1.41421356 141.42135624]\n","------------------------------\n","Scaled Training Data (X_train_scaled):\n"," [[-1.41421356 -1.41421356]\n"," [-0.70710678 -0.70710678]\n"," [ 0.          0.        ]\n"," [ 0.70710678  0.70710678]\n"," [ 1.41421356  1.41421356]]\n","Mean of scaled Feature 1: 0.00, Std Dev: 1.00\n","Mean of scaled Feature 2: 0.00, Std Dev: 1.00\n","------------------------------\n","Now, let's see how the FITTED scaler transforms new data.\n","Enter a value for Feature 1 for new data point (e.g., 6): 15\n","Enter a value for Feature 2 for new data point (e.g., 600): 5\n","\n","New data point (original scale):\n"," [[15.  5.]]\n","New data point (scaled by the trained scaler):\n"," [[ 8.48528137 -2.085965  ]]\n"]}],"execution_count":4,"metadata":{"id":"F8aV88nHsUE-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748829169635,"user_tz":-330,"elapsed":7379,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"95952593-5999-452f-8a07-63056a175f03"}},{"cell_type":"markdown","source":["**Reasoning behind the output:**\n","\n","  * `StandardScaler` calculates the mean and standard deviation for each feature in the **training data** during the `fit()` step.\n","  * When `transform()` is called, it applies the formula: `scaled_value = (original_value - mean) / standard_deviation` using the *learned* mean and standard deviation.\n","  * The result is that the transformed training features will have a mean close to 0 and a standard deviation close to 1.\n","  * Crucially, when new data (like a test set or the interactive input) is encountered, it is transformed using the **mean and standard deviation learned from the original training set**. This ensures consistency and prevents data leakage from the new data into the scaling parameters.\n","\n","-----\n","\n","### Example 4: Calculating Mean Squared Error (MSE) Interactively\n","\n","This example calculates the MSE for a few given actual and predicted values."],"metadata":{"id":"Z4dEfk1JsUE_"}},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_mse(actual_values, predicted_values):\n","    \"\"\"Calculates Mean Squared Error.\"\"\"\n","    actual_values = np.array(actual_values)\n","    predicted_values = np.array(predicted_values)\n","\n","    if len(actual_values) != len(predicted_values):\n","        return \"Error: Actual and predicted lists must have the same number of values.\"\n","    if len(actual_values) == 0:\n","        return \"Error: Cannot calculate MSE for empty lists.\"\n","\n","    squared_errors = (actual_values - predicted_values)**2 # [cite: 64] for squaring errors\n","    mse = np.mean(squared_errors) # [cite: 66] for averaging squared errors\n","    return mse\n","\n","# --- Interactive Part ---\n","print(\"Let's calculate Mean Squared Error (MSE).\")\n","print(\"Enter a few actual values and their corresponding predicted values.\")\n","\n","actuals_input = input(\"Enter actual values separated by commas (e.g., 10,15,20): \")\n","predicteds_input = input(\"Enter predicted values separated by commas (e.g., 11,14,22): \")\n","\n","try:\n","    actual_list = [float(val.strip()) for val in actuals_input.split(',')]\n","    predicted_list = [float(val.strip()) for val in predicteds_input.split(',')]\n","\n","    mse_result = calculate_mse(actual_list, predicted_list)\n","\n","    if isinstance(mse_result, str): # Check if it's an error message\n","        print(mse_result)\n","    else:\n","        print(\"-\" * 30)\n","        print(\"Actual Values:\", actual_list)\n","        print(\"Predicted Values:\", predicted_list)\n","        print(f\"Individual Squared Errors: { (np.array(actual_list) - np.array(predicted_list))**2 }\")\n","        print(f\"Mean Squared Error (MSE): {mse_result:.4f}\") # [cite: 65]\n","        print(\"-\" * 30)\n","        if mse_result == 0:\n","            print(\"Perfect predictions! MSE is 0.\")\n","        elif mse_result < 10: # Arbitrary threshold for \"low\"\n","            print(\"The MSE is relatively low, indicating predictions are close to actuals.\")\n","        else:\n","            print(\"The MSE is higher, indicating some notable differences between predictions and actuals.\")\n","\n","except ValueError:\n","    print(\"Invalid input. Please ensure you enter comma-separated numbers.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Let's calculate Mean Squared Error (MSE).\n","Enter a few actual values and their corresponding predicted values.\n","Enter actual values separated by commas (e.g., 10,15,20): 15\n","Enter predicted values separated by commas (e.g., 11,14,22): 20\n","------------------------------\n","Actual Values: [15.0]\n","Predicted Values: [20.0]\n","Individual Squared Errors: [25.]\n","Mean Squared Error (MSE): 25.0000\n","------------------------------\n","The MSE is higher, indicating some notable differences between predictions and actuals.\n"]}],"execution_count":5,"metadata":{"id":"wiNx4y-msUE_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748829207577,"user_tz":-330,"elapsed":5168,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"775b4d3c-3acf-4600-8bf1-09d9fc98cb05"}},{"cell_type":"markdown","source":["**Reasoning behind the output:**\n","\n","  * The code first takes comma-separated actual and predicted values from the user.\n","  * It then calculates the squared difference between each corresponding actual and predicted value[cite: 64]. This ensures errors don't cancel out and penalizes larger errors more heavily.\n","  * Finally, it computes the average of these squared differences to get the MSE[cite: 66].\n","  * A lower MSE indicates that the model's predictions are, on average, closer to the actual values, signifying a better fit to the data used for calculation[cite: 67].\n","\n"],"metadata":{"id":"5IJgJPy9sUFA"}},{"cell_type":"markdown","source":["## Exercises ‚úçÔ∏è\n","\n","-----\n","\n","**Question 1:**\n","Why is \"learning from data\" a defining characteristic of machine learning compared to traditional programming?\n","\n","  * **Hint:** Think about how a system's logic is developed in both approaches.\n","\n","<details>\n","  <summary>Solution</summary>\n","  In traditional programming, humans explicitly write the rules and logic for the system to follow. The system's behavior is predetermined by these hardcoded instructions. In machine learning, the system learns the logic and patterns directly from data. Instead of being explicitly programmed for a specific task, the model is trained on examples, and it deduces the underlying rules itself to make predictions or decisions.\n","</details>\n","\n","-----\n","\n","**Question 2:**\n","If AI is about machines replicating human-like thinking, what specific aspect of this \"thinking\" does machine learning primarily address?\n","\n","  * **Hint:** Consider the foundational capability needed before reasoning or decision-making can occur effectively.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Machine learning primarily addresses the aspect of **learning from experience or data**. Just as humans learn from observations and experiences to improve their understanding and decision-making, machine learning algorithms learn patterns and relationships from data to build models that can make predictions or perform tasks. This ability to learn is a foundational component of more complex \"thinking\" processes like reasoning or planning.\n","</details>\n","\n","-----\n","\n","**Question 3:**\n","A weather forecasting system predicts the exact amount of rainfall (in mm) for the next day. Is this a regression or a classification task? Why?\n","\n","  * **Hint:** What type of value is \"amount of rainfall\"?\n","\n","<details>\n","  <summary>Solution</summary>\n","  This is a **regression** task. The system is predicting a continuous numerical value (amount of rainfall in mm). Classification tasks, in contrast, predict a discrete category or class (e.g., \"rainy,\" \"sunny,\" \"cloudy\").\n","</details>\n","\n","-----\n","\n","**Question 4:**\n","Explain why a model that perfectly fits all training data points (achieving zero error on the training set) might not be the \"best\" model.\n","\n","  * **Hint:** Consider the model's performance on data it has never seen before and the concept of generalization.\n","\n","<details>\n","  <summary>Solution</summary>\n","  A model that perfectly fits all training data points might be **overfitting**. This means it has learned not only the underlying patterns in the data but also the noise and specific quirks of that particular training set. While it performs exceptionally well on the data it has seen, it may fail to **generalize** to new, unseen data. The \"best\" model is typically one that generalizes well, meaning it performs accurately on data it hasn't encountered during training.\n","</details>\n","\n","-----\n","\n","**Question 5:**\n","What is the role of a hyperparameter like `fit_intercept` in a linear regression model, and how does it differ from a learned parameter like a coefficient ($\\beta_1$)?\n","\n","  * **Hint:** One is a pre-training choice, the other is a result of training.\n","\n","<details>\n","  <summary>Solution</summary>\n","  A **hyperparameter** like `fit_intercept` is a configuration setting for the model that is chosen *before* the training process begins. It dictates aspects of the model's structure or the learning process (e.g., whether to calculate an intercept term or not). It's not learned from the data.\n","  A **learned parameter** like a coefficient ($\\beta_1$) is a value that the model *learns* from the training data during the training process. The algorithm adjusts these parameters to minimize the chosen loss function. The specific values of $\\beta_0$ (if `fit_intercept=True`) and $\\beta_1$ are results of the model fitting the data.\n","</details>\n","\n","-----\n","\n","**Question 6:**\n","Describe a scenario where the primary goal of a machine learning project might be **interpretation** rather than achieving the absolute highest **prediction** accuracy.\n","\n","  * **Hint:** Think about situations where understanding \"why\" is more important than just knowing \"what.\"\n","\n","<details>\n","  <summary>Solution</summary>\n","  A scenario could be a medical research project trying to understand the factors contributing to a particular disease. While predicting who might get the disease is important, the primary goal might be to identify and understand the impact of various genetic, lifestyle, or environmental factors ($\\beta$ coefficients in a regression model). This understanding (interpretation) can lead to new prevention strategies or treatment targets, even if the predictive accuracy of the model isn't perfect. Another example is a business trying to understand why customer churn is happening, so they can address the root causes, rather than just predicting which customers will churn next.\n","</details>\n","\n","-----\n","\n","**Question 7:**\n","In the equation $Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2$, if $\\beta_1$ is 5.0, what does this value signify in practical terms, assuming $X_1$ is \"advertising spend in dollars\" and $Y$ is \"units sold\"?\n","\n","  * **Hint:** How does $Y$ change when $X_1$ changes by one unit, holding other factors constant?\n","\n","<details>\n","  <summary>Solution</summary>\n","  If $\\beta_1$ is 5.0, it signifies that for every one-dollar increase in advertising spend ($X_1$), the number of units sold ($Y$) is expected to increase by 5.0 units, assuming all other factors (like $X_2$) are held constant.\n","</details>\n","\n","-----\n","\n","**Question 8:**\n","Why is the Mean Squared Error (MSE) often preferred over Mean Absolute Error (MAE) as a loss function in linear regression, especially from a mathematical/optimization perspective?\n","\n","  * **Hint:** Consider the differentiability and the penalty for large errors.\n","\n","<details>\n","  <summary>Solution</summary>\n","  MSE is often preferred over MAE for a few key reasons:\n","\n","  1.  **Differentiability:** The squared term in MSE results in a loss function that is continuously differentiable everywhere. The absolute value function in MAE is not differentiable at zero, which can make optimization using gradient-based methods more complex.\n","  2.  **Penalty for Large Errors:** MSE penalizes larger errors more heavily than smaller errors because the errors are squared. For example, an error of 4 contributes $4^2=16$ to the sum, while an error of 2 contributes $2^2=4$. This characteristic can be desirable if large errors are particularly problematic.\n","  3.  **Mathematical Convenience:** The derivatives of MSE are simpler to work with, leading to straightforward analytical solutions (like the normal equation for OLS regression) and smoother optimization paths for iterative methods.\n","</details>\n","\n","-----\n","\n","**Question 9:**\n","If a linear regression model has an $R^2$ value of 0.05, what does this suggest about the model's ability to explain the variation in the target variable?\n","\n","  * **Hint:** Is a small percentage of variation explained good or bad?\n","\n","<details>\n","  <summary>Solution</summary>\n","  An $R^2$ value of 0.05 suggests that the model explains only 5% of the variation in the target variable using the included independent variables. This is generally considered a very low $R^2$ value, indicating a poor fit. It means that 95% of the variability in the outcome is not accounted for by the model, and other factors (or a different model structure) are likely responsible for that variation.\n","</details>\n","\n","-----\n","\n","**Question 10:**\n","Why must the `fit()` method of a `StandardScaler` be called only on the training data, and not on the combined (train + test) dataset?\n","\n","  * **Hint:** Think about the term \"data leakage\" and its implications for model evaluation.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The `fit()` method of a `StandardScaler` must be called only on the training data to prevent **data leakage**. When `fit()` is called, the scaler calculates the mean and standard deviation of the data it sees. If it sees the test data during this fitting process, information from the test set (its statistical properties) \"leaks\" into the preprocessing step. This means the model indirectly learns from the test set, making the evaluation of the model's performance on this test set overly optimistic and not a true reflection of how it would perform on genuinely unseen data. The scaler should learn parameters only from the training data and then use those same parameters to transform both the training and the test sets.\n","</details>\n","\n","-----\n","\n","**Question 11:**\n","If you apply a log transformation to your target variable $y$ (creating $y' = \\log(y)$) before training a regression model, and your model predicts a value $y'_{pred}$, what is the final step needed to get the prediction in the original units of $y$?\n","\n","  * **Hint:** What is the mathematical inverse of a logarithm?\n","\n","<details>\n","  <summary>Solution</summary>\n","  The final step needed to get the prediction in the original units of $y$ is to apply the **inverse transformation** of the logarithm, which is exponentiation. If the natural logarithm ($\\ln$) was used, the inverse is $e^{y'_{pred}}$. If base-10 logarithm ($\\log_{10}$) was used, the inverse is $10^{y'_{pred}}$.\n","</details>\n","\n","-----\n","\n","**Question 12:**\n","What is the primary purpose of a \"holdout set\" or \"test set\" in the supervised learning workflow?\n","\n","  * **Hint:** It helps assess how well the model will perform in a specific situation.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The primary purpose of a \"holdout set\" or \"test set\" is to provide an **unbiased evaluation of the final model's performance** on data it has not seen during training. It helps assess how well the model will **generalize** to new, independent data, which is crucial for understanding its real-world effectiveness.\n","</details>\n","\n","-----\n","\n","**Question 13:**\n","Can a model be highly predictive but offer little insight into *why* it makes those predictions? Give an example of such a model type.\n","\n","  * **Hint:** Consider models often referred to as \"black boxes.\"\n","\n","<details>\n","  <summary>Solution</summary>\n","  Yes, a model can be highly predictive but offer little insight into why it makes those predictions. These are often referred to as \"black-box\" models. An example is a **deep neural network** or complex **ensemble methods** like Gradient Boosting or Random Forests (especially with many trees). While they can achieve high accuracy, their internal workings involve numerous complex interactions between features, making it difficult to pinpoint the exact contribution of individual variables to a specific prediction in a simple, intuitive way.\n","</details>\n","\n","-----\n","\n","**Question 14:**\n","A company wants to identify customers who are likely to stop using their service (churn). They have historical data on customer behavior and whether they churned or not. What type of supervised learning problem is this?\n","\n","  * **Hint:** Is \"churn\" (yes/no) a continuous value or a category?\n","\n","<details>\n","  <summary>Solution</summary>\n","  This is a **classification** problem. The goal is to predict a categorical outcome: whether a customer will churn (e.g., \"yes\" or \"no\", or \"churn\" vs. \"not churn\").\n","</details>\n","\n","-----\n","\n","**Question 15:**\n","How does the availability of \"more data\" generally impact the process of a supervised machine learning model learning its parameters?\n","\n","  * **Hint:** Does more data typically help or hinder the learning of true underlying patterns?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Generally, the availability of \"more data\" (especially representative and good quality data) **helps** a supervised machine learning model learn its parameters more effectively. More data provides more examples for the model to learn the true underlying patterns and relationships, reducing the risk of overfitting to noise in a smaller dataset and often leading to better generalization and more robust parameter estimates.\n","</details>\n","\n","-----\n","\n","**Question 16:**\n","What does it mean for a model to \"capture the underlying reality or territory\" while omitting unimportant details?\n","\n","  * **Hint:** Think about the balance between complexity and representativeness.\n","\n","<details>\n","  <summary>Solution</summary>\n","  It means the model successfully identifies and represents the key, systematic patterns and relationships present in the data (the \"underlying reality\" or \"signal\") without being overly influenced by random noise or irrelevant specifics (unimportant details). A good model achieves a balance: it's complex enough to capture the essential structure but simple enough to avoid overfitting and to generalize well to new data. It's like a good map that shows important roads and landmarks but omits every single tree or pebble.\n","</details>\n","\n","-----\n","\n","**Question 17:**\n","If you're building a classification model to detect spam emails, what are two essential components you would need?\n","\n","  * **Hint:** Consider the input data format and the type of outcome variable.\n","\n","<details>\n","  <summary>Solution</summary>\n","  To build a classification model for spam email detection, you would need:\n","\n","  1.  **Features ($X$)**: Numerical representations of the emails (e.g., word frequencies, presence of certain keywords, email sender information, etc.).\n","  2.  **Labeled Target Variable ($y$)**: A categorical variable indicating whether each email is spam or not spam (e.g., 1 for spam, 0 for not spam).\n","</details>\n","\n","-----\n","\n","**Question 18:**\n","What is the fundamental goal when a machine learning model is being \"trained\" with respect to its loss function?\n","\n","  * **Hint:** What value of the loss function is most desirable?\n","\n","<details>\n","  <summary>Solution</summary>\n","  The fundamental goal during model training is to **minimize the loss function**. The loss function measures the discrepancy between the model's predictions and the actual target values. A lower loss value indicates that the model's predictions are closer to the true values, signifying a better fit to the training data.\n","</details>\n","\n","-----\n","\n","**Question 19:**\n","Explain \"data leakage\" in the context of applying a Box-Cox transformation. When should the lambda parameter be determined?\n","\n","  * **Hint:** The transformation parameter should only learn from a specific subset of data.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Data leakage in the context of a Box-Cox transformation occurs if the optimal lambda parameter (which dictates the nature of the transformation) is determined using data that includes the test set (or any data outside the training set). The lambda parameter should be determined **only from the training data**. If lambda is calculated using the entire dataset before splitting, or using the test set, information about the test set's distribution \"leaks\" into the transformation process. This makes the model's performance on the test set appear better than it would on genuinely unseen data because the transformation was optimized with knowledge of that test data.\n","</details>\n","\n","-----\n","\n","**Question 20:**\n","If adding a new feature to a linear regression model causes the $R^2$ to increase from 0.70 to 0.71, but the feature's coefficient is not statistically significant, how might you interpret this?\n","\n","  * **Hint:** Consider the properties of $R^2$ versus actual predictive improvement.\n","\n","<details>\n","  <summary>Solution</summary>\n","  This might be interpreted as follows:\n","  The slight increase in $R^2$ (from 0.70 to 0.71) is expected because $R^2$ generally does not decrease when a new feature is added, even if that feature is irrelevant. The model can assign a very small coefficient to it or the increase might be due to chance correlation in the sample.\n","  However, the fact that the feature's coefficient is **not statistically significant** suggests that there isn't strong evidence that this new feature has a true, reliable relationship with the target variable in the broader population. The improvement in $R^2$ is likely minor and may not represent a meaningful improvement in the model's actual predictive power or its ability to explain the outcome. It might be better to consider a simpler model without this feature, or to use Adjusted $R^2$ which penalizes the addition of non-significant predictors.\n","</details>\n","\n","-----\n","\n","**Question 21:**\n","What is the difference between an \"error\" (or residual) for a single data point and the \"loss function\" for the entire model in linear regression?\n","\n","  * **Hint:** One is a specific instance of deviation, the other is an aggregate measure.\n","\n","<details>\n","  <summary>Solution</summary>\n","  An **error (or residual)** for a single data point is the specific difference between the actual observed value ($y_i$) and the value predicted by the model ($y_{predicted,i}$) for that particular observation. It's a measure of how wrong the model was for one instance.\n","  The **loss function** (e.g., Mean Squared Error) is an aggregate measure that summarizes the errors across *all* data points in a dataset (typically the training set). It provides a single number that quantifies the overall performance or \"badness of fit\" of the model for the entire dataset. The model is trained by adjusting its parameters to minimize this aggregate loss.\n","</details>\n","\n","-----\n","\n","**Question 22:**\n","Why might transforming a non-normally distributed target variable (e.g., using a log or Box-Cox transform) be beneficial for linear regression, even if the target variable itself doesn't *need* to be normal?\n","\n","  * **Hint:** Think about the assumptions concerning the model's errors.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Transforming a non-normally distributed target variable can be beneficial because it can help satisfy the linear regression assumption that the **residuals (errors) are normally distributed** and have constant variance (homoscedasticity). Even if the original target variable isn't normal, a transformation might lead to a model whose errors *are* normally distributed and homoscedastic. Fulfilling these assumptions is important for the validity of statistical inferences like hypothesis tests on coefficients and confidence intervals.\n","</details>\n","\n","-----\n","\n","**Question 23:**\n","Provide an example of a hyperparameter you might tune for a classification model (other than linear regression).\n","\n","  * **Hint:** Think about models like k-NN or Decision Trees.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Examples of hyperparameters for other classification models include:\n","\n","  * For a **k-Nearest Neighbors (k-NN)** model: The number of neighbors, `k`.\n","  * For a **Decision Tree** model: The `max_depth` of the tree, or `min_samples_split`, or the criterion used for splitting (e.g., 'gini' or 'entropy').\n","  * For a **Support Vector Machine (SVM)**: The `C` parameter (regularization strength) or the type of `kernel` (e.g., 'linear', 'rbf').\n","</details>\n","\n","-----\n","\n","**Question 24:**\n","In the general supervised learning framework $y_P = f(\\text{parameters}, X)$, what does the function 'f' represent conceptually?\n","\n","  * **Hint:** It's the core logic learned by the machine.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Conceptually, the function 'f' represents the **mapping or relationship** that the machine learning model has learned from the training data. It encapsulates the logic, patterns, and rules discovered by the algorithm that transform the input features ($X$), using the learned parameters, into a prediction ($y_P$). It's the core of the model's \"intelligence.\"\n","</details>\n","\n","-----\n","\n","**Question 25:**\n","Why is simply minimizing the sum of raw errors (positive and negative) generally not a good objective for training a linear regression model?\n","\n","  * **Hint:** What happens if one error is +10 and another is -10?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Minimizing the sum of raw errors is generally not a good objective because positive and negative errors can cancel each other out. For example, if one prediction has an error of +10 and another has an error of -10, the sum of these errors is 0, suggesting a perfect fit for these two points, which is clearly not the case. This would allow a model with very large but offsetting errors to appear better than a model with consistently small errors. Loss functions like Mean Squared Error (MSE) or Mean Absolute Error (MAE) address this by squaring the errors or taking their absolute value, ensuring that all errors contribute positively to the total loss.\n","</details>\n","\n","-----\n","\n","**Question 26:**\n","If a project's primary goal is to deploy a system that automatically approves or denies loan applications with the highest possible accuracy, would you prioritize model interpretability or predictive power more? Why?\n","\n","  * **Hint:** Consider the direct business impact of correct vs. incorrect decisions.\n","\n","<details>\n","  <summary>Solution</summary>\n","  In this scenario, you would likely prioritize **predictive power** more, while still aiming for some level of interpretability for regulatory reasons or fairness assessments. The primary goal is to achieve the highest possible accuracy in approving good loans and denying bad ones because incorrect decisions have direct and significant financial consequences (e.g., defaults on bad loans, missed opportunities on good loans). While understanding *why* a decision was made is important (and may be legally required), the system's ability to make correct decisions (high accuracy, precision, recall) would be the paramount concern for the business objective.\n","</details>\n","\n","-----\n","\n","**Question 27:**\n","How can insights gained from an \"interpretation-focused\" modeling approach potentially lead to improvements in a \"prediction-focused\" model?\n","\n","  * **Hint:** Think about feature selection or feature engineering.\n","\n","<details>\n","  <summary>Solution</summary>\n","  Insights from an interpretation-focused model can lead to improvements in a prediction-focused model in several ways:\n","\n","  1.  **Feature Selection:** By understanding which features are most influential and have a meaningful relationship with the outcome, you can select a more relevant subset of features for the predictive model, potentially reducing noise and complexity.\n","  2.  **Feature Engineering:** Interpretive models might reveal non-linear relationships or interactions between variables. This understanding can guide the creation of new, more informative features (e.g., polynomial terms, interaction terms) that can boost the performance of a predictive model.\n","  3.  **Identifying Data Issues:** An interpretable model might highlight strange or counter-intuitive relationships, which could point to data quality problems or errors in data collection that, once fixed, improve any subsequent model.\n","  4.  **Domain Knowledge Refinement:** Understanding the model's logic can refine domain knowledge, which can then be used to guide the development of better predictive models.\n","</details>\n","\n","-----\n","\n","**Question 28:**\n","What is the \"Total Sum of Squares (TSS)\" in the context of $R^2$, and what does it represent about the target variable?\n","\n","  * **Hint:** It measures the variability before any model is applied.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The **Total Sum of Squares (TSS)** is the sum of the squared differences between each actual observed value of the target variable ($y_i$) and the overall mean of the target variable ($\\bar{y}$).\n","  $\\text{TSS} = \\sum (y_i - \\bar{y})^2$\n","  It represents the **total variance** or **total variability** in the target variable. Conceptually, it's the amount of error you would have if you were to predict every observation using just the mean of the target variable (i.e., a very simple baseline model).\n","</details>\n","\n","-----\n","\n","**Question 29:**\n","Describe a situation where polynomial features might be added to a linear regression model. What problem are they trying to solve?\n","\n","  * **Hint:** What if the true relationship between X and Y isn't a straight line?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Polynomial features might be added to a linear regression model when the true underlying relationship between an independent variable ($X$) and the dependent variable ($Y$) is suspected to be **non-linear**. For example, the relationship between advertising spend ($X$) and sales ($Y$) might initially increase rapidly but then level off (diminishing returns), forming a curve rather than a straight line.\n","  They are trying to solve the problem of **underfitting** that would occur if a simple linear model (straight line) is forced to fit a curved relationship. By adding polynomial terms (like $X^2, X^3$), the model can fit more complex, curved patterns in the data while still being linear with respect to the *coefficients* of these new polynomial features.\n","</details>\n","\n","-----\n","\n","**Question 30:**\n","If you train a linear regression model and find that the residuals are significantly skewed, what assumption of linear regression might be violated, and what could be a potential remedy?\n","\n","  * **Hint:** This relates to the distribution of errors and potential data transformations.\n","\n","<details>\n","  <summary>Solution</summary>\n","  If the residuals are significantly skewed, the assumption of **normality of residuals** might be violated. This assumption is important for the validity of statistical inferences (like p-values and confidence intervals) associated with the model's coefficients.\n","  A potential remedy could be to **transform the target variable ($y$)** using functions like a logarithm, square root, or a Box-Cox transformation. Such transformations can sometimes stabilize the variance and make the distribution of the residuals more symmetric and closer to normal. Alternatively, one might consider transforming some of the predictor variables or using a different type of model (e.g., a generalized linear model) that doesn't rely on this specific assumption.\n","</details>\n","\n","-----\n","\n","**Question 31:**\n","What does it mean for a dataset to be \"labeled\" in supervised machine learning?\n","\n","  * **Hint:** Each input data point is accompanied by something specific.\n","\n","<details>\n","  <summary>Solution</summary>\n","  For a dataset to be \"labeled\" in supervised machine learning means that each input data point (or set of features, $X$) is accompanied by a **corresponding correct output or target value ($y$)**. This target value is the \"ground truth\" that the model tries to learn to predict. For example, in an email spam detection dataset, each email (input) would be labeled as either \"spam\" or \"not spam\" (target).\n","</details>\n","\n","-----\n","\n","**Question 32:**\n","Is choosing the type of machine learning model (e.g., linear regression vs. a decision tree) considered setting a parameter or a hyperparameter?\n","\n","  * **Hint:** Is this learned from data or decided beforehand?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Choosing the type of machine learning model (e.g., linear regression vs. a decision tree) is considered setting a **hyperparameter** (or more accurately, a model choice that precedes hyperparameter tuning). It's a high-level decision made by the data scientist *before* the training process begins and is not learned from the data itself. Once the model type is chosen, specific hyperparameters for that model (like the depth of a decision tree) are then set or tuned.\n","</details>\n","\n","-----\n","\n","**Question 33:**\n","If a linear regression model for predicting house prices based on square footage has an intercept ($\\beta_0$) of 50,000, what does this theoretically represent?\n","\n","  * **Hint:** What is the price when the square footage is zero? Is this always practical?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Theoretically, an intercept ($\\beta_0$) of 50,000 represents the **predicted house price when the square footage ($X$) is zero**.\n","  However, this interpretation is often not practical or meaningful in real-world scenarios. A house with zero square footage doesn't make sense, and the model is typically built using data within a certain range of square footage values. Extrapolating to zero square footage is often far outside this range, so the intercept primarily serves as a baseline adjustment to help the regression line best fit the observed data points. It's a mathematical necessity for the line, but its direct interpretation at $X=0$ might be unrealistic.\n","</details>\n","\n","-----\n","\n","**Question 34:**\n","Why is it important to use a \"cost function\" as one of the modeling best practices?\n","\n","  * **Hint:** How do you objectively compare different attempts to model the data?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Using a \"cost function\" (or loss function) is important because it provides an **objective, quantitative measure of how well the model is performing**. It defines what the model is trying to optimize during training (i.e., minimize this cost). This allows for:\n","\n","  1.  **Model Training:** Guiding the learning algorithm to adjust parameters to minimize errors.\n","  2.  **Model Comparison:** Objectively comparing different models or different versions of the same model (e.g., with different hyperparameters).\n","  3.  **Model Evaluation:** Assessing the final performance of the model on unseen data.\n","  Without a cost function, it would be difficult to systematically improve the model or know if one model is truly better than another.\n","</details>\n","\n","-----\n","\n","**Question 35:**\n","If a classification model is designed to predict if a patient has a certain disease (yes/no), and it achieves 99% accuracy, why might this single metric be misleading without further context?\n","\n","  * **Hint:** Consider a scenario where the disease is very rare.\n","\n","<details>\n","  <summary>Solution</summary>\n","  A 99% accuracy might be misleading, especially if the disease is very rare (an **imbalanced dataset**). For instance, if only 1% of the population has the disease, a model that simply predicts \"no disease\" for every patient would achieve 99% accuracy. However, this model would be useless for actually detecting the disease, as it would miss every single positive case. In such scenarios, other metrics like precision, recall (sensitivity), F1-score, or metrics from a confusion matrix are much more informative about the model's true performance in identifying the positive class.\n","</details>\n","\n","-----\n","\n","**Question 36:**\n","When would you choose a regression model over a classification model?\n","\n","  * **Hint:** It depends on the nature of the variable you are trying to predict.\n","\n","<details>\n","  <summary>Solution</summary>\n","  You would choose a **regression model** when the goal is to predict a **continuous numerical value**. Examples include predicting house prices, stock prices, temperature, or the amount of rainfall.\n","  You would choose a **classification model** when the goal is to predict a **discrete categorical label** or class. Examples include predicting whether an email is spam or not, whether a customer will churn, or identifying the type of an object in an image.\n","</details>\n","\n","-----\n","\n","**Question 37:**\n","What is the risk of not using a train-test split and instead evaluating your model on the same data it was trained on?\n","\n","  * **Hint:** The model might look good on paper but fail in practice.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The primary risk is **overfitting** and obtaining an **overly optimistic and misleading assessment of the model's performance**. A model evaluated on the data it was trained on will often appear to perform very well because it has essentially \"memorized\" that specific data, including its noise. However, this performance metric doesn't indicate how well the model will generalize to new, unseen data. The model might look good on paper (high accuracy on training data) but fail significantly when deployed in a real-world scenario with new data.\n","</details>\n","\n","-----\n","\n","**Question 38:**\n","Give an example of how machine learning is used in \"ranking websites for web search.\"\n","\n","  * **Hint:** What factors might a search engine learn to prioritize?\n","\n","<details>\n","  <summary>Solution</summary>\n","  In web search ranking, machine learning models (often referred to as \"Learning to Rank\" algorithms) are trained on vast amounts of data about user queries, web pages, and user interactions. The model learns to predict the relevance of a webpage to a given query by considering hundreds or even thousands of features (signals).\n","  Examples of factors (features) it might learn to prioritize include:\n","\n","  * The presence and location of keywords from the query on the webpage (e.g., in titles, headings, body text).\n","  * The authority or trustworthiness of the website (e.g., based on backlinks from other reputable sites - PageRank).\n","  * User engagement signals (e.g., click-through rate for that page for similar queries, dwell time).\n","  * Freshness or recency of the content.\n","  * Page load speed and mobile-friendliness.\n","  The ML model learns the optimal weights for these features to produce a ranked list of results that is most likely to satisfy the user's search intent.\n","</details>\n","\n","-----\n","\n","**Question 39:**\n","A model is described as a \"small thing that captures a larger thing.\" What \"larger thing\" does a linear regression model attempt to capture?\n","\n","  * **Hint:** It's about the connection between variables.\n","\n","<details>\n","  <summary>Solution</summary>\n","  A linear regression model attempts to capture the **underlying linear relationship or association** between one or more independent variables (predictors) and a continuous dependent variable (outcome). The \"larger thing\" is the true, systematic pattern of how the dependent variable changes as the independent variables change in the overall population or system from which the data was sampled. The model (the equation with its coefficients) is the \"small thing\" that approximates this larger, often more complex, reality.\n","</details>\n","\n","-----\n","\n","**Question 40:**\n","Why is the `StandardScaler` fitted on the training data and then used to *transform* both training and test data, rather than fitting it separately on both?\n","\n","  * **Hint:** Consistency in transformation is key for valid comparison.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The `StandardScaler` is fitted only on the training data to learn the mean and standard deviation specific to that training set. These learned parameters (mean and standard deviation) define the exact transformation.\n","  If you were to fit the scaler separately on the test data, it would learn different mean and standard deviation values based on the test set's distribution. This would mean the training and test data are transformed using *different* rules and scales.\n","  To ensure a valid and unbiased evaluation of the model, the **same transformation** (based on training data parameters) must be applied to both sets. This maintains consistency and prevents data leakage from the test set into the preprocessing pipeline, ensuring that the test set truly represents unseen data processed in the same way future real-world data would be.\n","</details>\n","\n","-----\n","\n","**Question 41:**\n","If your primary goal is to understand which specific marketing channels (e.g., TV, radio, social media) give the best return on investment, would a highly complex, \"black-box\" model be the best first choice? Why or why not?\n","\n","  * **Hint:** Consider the need to understand the influence of individual features.\n","\n","<details>\n","  <summary>Solution</summary>\n","  No, a highly complex, \"black-box\" model would likely **not** be the best first choice. The primary goal here is **interpretation** ‚Äì understanding the specific impact of each marketing channel. Black-box models (like deep neural networks or complex ensembles) excel at prediction but often make it difficult to isolate and quantify the effect of individual input features (like spending on TV vs. radio). A simpler, more interpretable model like linear regression (possibly with transformations or interaction terms if needed) would be a better starting point, as its coefficients can directly indicate the relationship between spending on each channel and the return on investment, allowing for actionable insights.\n","</details>\n","\n","-----\n","\n","**Question 42:**\n","What is the conceptual difference between the \"Sum Squared Error (SSE)\" and the \"Mean Squared Error (MSE)\"?\n","\n","  * **Hint:** One is a total, the other is an average.\n","\n","<details>\n","  <summary>Solution</summary>\n","  The conceptual difference is that:\n","\n","  * **Sum Squared Error (SSE)** is the **total sum** of the squared differences between the actual and predicted values across all observations in a dataset. Its value depends on the number of data points; more data points will generally lead to a larger SSE, even if the average error per point is small.\n","      $\\text{SSE} = \\sum (y_i - y_{predicted,i})^2$\n","  * **Mean Squared Error (MSE)** is the **average** of the squared differences. It's calculated by dividing the SSE by the number of observations ($m$). This normalizes the error measure, making it easier to compare model performance across datasets of different sizes.\n","      $\\text{MSE} = \\frac{1}{m} \\sum (y_i - y_{predicted,i})^2 = \\frac{\\text{SSE}}{m}$\n","</details>\n","\n","-----\n","\n","**Question 43:**\n","Can $R^2$ be negative? If so, what would a negative $R^2$ imply about the model's performance?\n","\n","  * **Hint:** How does the model compare to simply predicting the average of the target variable?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Yes, $R^2$ can be negative. A negative $R^2$ implies that the model performs **worse** than a simple baseline model that always predicts the mean of the target variable ($\\bar{y}$).\n","  Recall that $R^2 = 1 - \\frac{\\text{SSE}}{\\text{TSS}}$. If the Sum of Squared Errors (SSE) from your model is greater than the Total Sum of Squares (TSS) ‚Äì meaning your model's predictions are, on average, further away from the actual values than the mean is ‚Äì then the ratio $\\frac{\\text{SSE}}{\\text{TSS}}$ will be greater than 1, making $R^2$ negative. This indicates a very poor model fit.\n","</details>\n","\n","-----\n","\n","**Question 44:**\n","What is the purpose of an \"update rule\" during model training?\n","\n","  * **Hint:** How do parameters change to improve the model?\n","\n","<details>\n","  <summary>Solution</summary>\n","  The purpose of an \"update rule\" during model training is to specify **how the model's parameters (e.g., coefficients in linear regression) should be adjusted at each step of the learning process to minimize the loss function**. Based on the calculated error or loss, the update rule (often derived from an optimization algorithm like gradient descent) dictates the direction and magnitude of the change to be made to each parameter, iteratively moving the model towards a better fit for the data.\n","</details>\n","\n","-----\n","\n","**Question 45:**\n","If a feature has absolutely no relationship with the target variable, what would you ideally expect its coefficient ($\\beta_1$) to be in a linear regression model? Will adding such a feature typically decrease $R^2$?\n","\n","  * **Hint:** Consider how the model would treat an irrelevant input.\n","\n","<details>\n","  <summary>Solution</summary>\n","  If a feature has absolutely no relationship with the target variable, you would ideally expect its coefficient ($\\beta_1$) in a linear regression model to be **zero** (or very close to zero, within the bounds of statistical noise if estimated from a sample).\n","  Adding such an irrelevant feature will **typically not decrease $R^2$**. $R^2$ tends to stay the same or increase slightly when new variables are added, regardless of their actual predictive power, because the model can always assign a near-zero coefficient to the irrelevant feature, or the feature might pick up some chance correlation in the specific training sample. This is why Adjusted $R^2$ is often preferred, as it penalizes the addition of useless predictors.\n","</details>\n","\n","-----\n","\n","**Question 46:**\n","What is a practical benefit of using `random_state` when performing a train-test split?\n","\n","  * **Hint:** Think about debugging or sharing your work.\n","\n","<details>\n","  <summary>Solution</summary>\n","  A practical benefit of using `random_state` (or a seed for the random number generator) when performing a train-test split is **reproducibility**. It ensures that every time you run your code, the data will be split in exactly the same way. This is very helpful for:\n","\n","  * Debugging your code.\n","  * Comparing the performance of different models or hyperparameter settings fairly, as you know the splits are identical.\n","  * Sharing your work with others, allowing them to replicate your results.\n","</details>\n","\n","-----\n","\n","**Question 47:**\n","If a linear regression model is trained to predict weight (kg) from height (cm), and the slope coefficient ($\\beta_1$) is 0.8, how would you interpret this?\n","\n","  * **Hint:** For every one unit change in height...\n","\n","<details>\n","  <summary>Solution</summary>\n","  If the slope coefficient ($\\beta_1$) is 0.8, it means that for every **one-centimeter increase in height**, the model predicts an **increase of 0.8 kilograms in weight**, assuming all other factors (if any were included in the model) remain constant.\n","</details>\n","\n","-----\n","\n","**Question 48:**\n","Why is it stated that collecting labeled target variables for classification often requires \"human effort to accumulate\"?\n","\n","  * **Hint:** Who provides the \"correct answers\" for the model to learn from?\n","\n","<details>\n","  <summary>Solution</summary>\n","  Collecting labeled target variables for classification often requires \"human effort to accumulate\" because humans are typically needed to provide the \"ground truth\" or the correct category for each data instance. For example, to create a dataset for image classification, humans might need to look at thousands of images and manually label each one as \"cat,\" \"dog,\" \"car,\" etc. For spam detection, emails might need to be manually reviewed and tagged. This process of manual annotation by domain experts or annotators can be time-consuming and expensive.\n","</details>\n","\n","-----\n","\n","**Question 49:**\n","If a model has high variance, how is it likely to perform on the training data versus the test data?\n","\n","  * **Hint:** High variance models are often too complex for the data.\n","\n","<details>\n","  <summary>Solution</summary>\n","  If a model has high variance, it means it is highly sensitive to the specific data it was trained on.\n","\n","  * It is likely to perform **very well on the training data**, possibly fitting it almost perfectly (low training error). This is because it has learned the noise and specific details of the training set.\n","  * However, it is likely to perform **poorly on the test data** (high test error). It will not generalize well to new, unseen data because it has overfit the training data.\n","  High variance is a characteristic of overly complex models that capture noise rather than just the underlying signal.\n","</details>\n","\n","-----\n","\n","**Question 50:**\n","Between a simple linear regression model and a deep neural network, which is generally considered more of a \"black box\" and why?\n","\n","  * **Hint:** Which one makes it harder to understand the relationship between specific inputs and the output?\n","\n","<details>\n","  <summary>Solution</summary>\n","  A **deep neural network** is generally considered more of a \"black box.\"\n","  **Why:**\n","\n","  * **Linear Regression:** The relationship between inputs and output is straightforward and transparent. Each coefficient ($\\beta_i$) directly quantifies the change in the output for a one-unit change in the corresponding input feature, holding other features constant. You can easily see the weight and direction of influence for each feature.\n","  * **Deep Neural Networks:** These models consist of many layers of interconnected nodes (neurons) with non-linear activation functions. The input features are transformed through these layers in highly complex ways. While the network might be very accurate, it's often very difficult to trace how specific input features contribute to a particular output or to understand the learned relationships in an intuitive human-readable way. The sheer number of parameters and the non-linear transformations make direct interpretation challenging.\n","</details>\n","\n","-----"],"metadata":{"id":"HOpQeMKpW7At"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
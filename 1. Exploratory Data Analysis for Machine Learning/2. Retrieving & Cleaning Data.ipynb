{"cells":[{"cell_type":"markdown","source":["\n","## Exploratory Data Analysis for Machine Learning: Retrieving and Cleaning Data\n","\n","The journey in machine learning often begins with obtaining and preparing your data.  This initial phase is critical because the quality of your data directly dictates the potential performance of your models‚Äîa principle often summarized as \"garbage-in, garbage-out.\"  Let's explore how to retrieve data from various sources and then the essential techniques for cleaning it.\n"],"metadata":{"id":"dsil_F3P1HnU"}},{"cell_type":"markdown","source":["-----\n","\n","### Retrieving Data üìä\n","\n","Data can reside in a multitude of locations and formats. Common sources include flat files like CSV and JSON, relational (SQL) and non-relational (NoSQL) databases, web APIs, and various cloud storage solutions. The **Pandas** library in Python is an indispensable tool for reading and manipulating this data."],"metadata":{"id":"ds_4E9v9PcQb"}},{"cell_type":"markdown","source":["\n","\n","#### 1\\. CSV (Comma Separated Values) Files\n","\n","CSV files are simple text files where data is stored in a tabular format, with values typically separated by commas.\n","\n","**Core Process:**\n","\n","1.  Import the `pandas` library.\n","2.  Specify the path to your CSV file.\n","3.  Use the `pd.read_csv()` function to load the data into a Pandas DataFrame.\n","\n","**Key `read_csv()` Arguments:**\n","\n","  * `sep` or `delimiter`: Specifies the character used to separate values if it's not a comma (e.g., `'\\t'` for tab-separated, `'\\s+'` for variable spaces).\n","  * `header`: Indicates which row to use as column names (e.g., `0` for the first row, `None` if no header).\n","  * `names`: Allows you to provide a list of custom column names.\n","  * `na_values`: A list of strings or values to recognize as missing (NaN).\n","  * `skiprows`: Number of lines to skip at the start of the file or a list of row numbers to skip.\n","  * `nrows`: Number of rows of file to read. Useful for reading a subset of large files.\n","  * `encoding`: Specifies the file encoding (e.g., 'utf-8', 'latin1') if you encounter decoding errors.\n","\n","**Interactive Example: Reading a CSV File**\n","\n","Let's simulate reading a CSV file. We'll ask for some parameters to customize how we read it."],"metadata":{"id":"2fUTq5MdPlhh"}},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","# Simulate user input\n","print(\"Let's simulate reading a CSV file!\")\n","file_content_choice = input(\"Enter 'comma' for comma-separated or 'tab' for tab-separated content: \").lower()\n","has_header_str = input(\"Does the file have a header row? (yes/no): \").lower()\n","\n","# Simulated file content based on user choice\n","if file_content_choice == 'tab':\n","    simulated_csv_content = \"Name\\tAge\\tCity\\nAlice\\t30\\tNew York\\nBob\\t24\\tParis\\nCharlie\\t22\\tLondon\"\n","    separator = '\\t'\n","    print(\"\\nUsing simulated TAB-separated content:\")\n","    print(simulated_csv_content)\n","else:\n","    simulated_csv_content = \"Name,Age,City\\nAlice,30,New York\\nBob,24,Paris\\nCharlie,22,London\"\n","    separator = ','\n","    print(\"\\nUsing simulated COMMA-separated content:\")\n","    print(simulated_csv_content)\n","\n","header_option = 0 if has_header_str == 'yes' else None\n","\n","print(f\"\\nAttempting to read with separator='{separator}' and header={header_option}\")\n","\n","try:\n","    # Use io.StringIO to treat the string as a file\n","    df = pd.read_csv(io.StringIO(simulated_csv_content), sep=separator, header=header_option)\n","    if header_option is None and not df.empty: # If no header, Pandas assigns default integer column names\n","        custom_names = input(f\"No header specified. Enter {df.shape[1]} custom column names separated by commas (e.g., Col1,Col2,Col3): \").split(',')\n","        if len(custom_names) == df.shape[1]:\n","            df.columns = custom_names\n","        else:\n","            print(f\"Warning: Number of names provided ({len(custom_names)}) doesn't match number of columns ({df.shape[1]}). Using default names.\")\n","\n","    print(\"\\nSuccessfully read data into DataFrame:\")\n","    print(df)\n","    print(\"\\nExplanation:\")\n","    print(f\"The data was read using '{separator}' as the delimiter.\")\n","    if header_option == 0:\n","        print(\"The first row was used as column headers.\")\n","    else:\n","        print(\"No header row was assumed from the data; default or custom column names were used.\")\n","    if 'custom_names' in locals() and len(custom_names) == df.shape[1] and header_option is None:\n","        print(f\"Custom column names {custom_names} were applied.\")\n","\n","except Exception as e:\n","    print(f\"\\nAn error occurred: {e}\")\n","    print(\"This could be due to incorrect delimiter for the chosen content or other parsing issues.\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Let's simulate reading a CSV file!\n","Enter 'comma' for comma-separated or 'tab' for tab-separated content: anuj, chaudhary, is, beinganujchaudhary\n","Does the file have a header row? (yes/no): y\n","\n","Using simulated COMMA-separated content:\n","Name,Age,City\n","Alice,30,New York\n","Bob,24,Paris\n","Charlie,22,London\n","\n","Attempting to read with separator=',' and header=None\n","No header specified. Enter 3 custom column names separated by commas (e.g., Col1,Col2,Col3): name, surname, psuedoname\n","\n","Successfully read data into DataFrame:\n","      name  surname  psuedoname\n","0     Name      Age        City\n","1    Alice       30    New York\n","2      Bob       24       Paris\n","3  Charlie       22      London\n","\n","Explanation:\n","The data was read using ',' as the delimiter.\n","No header row was assumed from the data; default or custom column names were used.\n","Custom column names ['name', ' surname', ' psuedoname'] were applied.\n"]}],"execution_count":1,"metadata":{"id":"A1B851M11HnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747782418434,"user_tz":-330,"elapsed":47711,"user":{"displayName":"Anuj Chaudhary","userId":"14648035130959477596"}},"outputId":"0f82bac1-2fb6-495c-ff77-353c762e30c0"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","The code first asks you whether the simulated data should be comma or tab-separated and if it includes a header.\n","\n","  * If you choose 'tab' and 'yes' for header, `pd.read_csv` will use `sep='\\t'` and `header=0` (default for header).\n","  * If you choose 'comma' and 'no' for header, it will use `sep=','` and `header=None`. In this case, it then prompts for custom column names.\n","    The output DataFrame will reflect these choices. If an error occurs (e.g., specifying 'tab' for comma-separated content without a header), an error message will explain the likely cause.\n","\n","---"],"metadata":{"id":"OhdqjJBV1HnY"}},{"cell_type":"markdown","source":["\n","#### 2\\. JSON (JavaScript Object Notation) Files\n","\n","JSON is a lightweight data-interchange format. It's human-readable and easy for machines to parse and generate. It's widely used in web applications and APIs. JSON data is built on key-value pairs, much like Python dictionaries.\n","\n","**Core Process:**\n","\n","1.  Import `pandas`.\n","2.  Use `pd.read_json()` to load the JSON file.\n","3.  The `orient` argument in `pd.read_json()` is crucial as it tells Pandas how the JSON string/file is structured (e.g., 'split', 'records', 'index', 'columns', 'values').\n","4.  You can write a DataFrame to JSON using `df.to_json()`.\n","\n","**Interactive Example: Reading a JSON File with `orient`**"],"metadata":{"id":"CAKlBopl5VTg"}},{"cell_type":"code","source":["import pandas as pd\n","import json\n","\n","# Simulate user input for JSON orientation\n","print(\"Let's simulate reading a JSON file with different orientations.\")\n","orient_choice = input(\"Choose a JSON orientation ('split', 'records', 'index', 'columns', 'values'): \").lower()\n","\n","# Sample data\n","data = {\n","    'col1': {'row1': 1, 'row2': 2},\n","    'col2': {'row1': 3, 'row2': 4}\n","}\n","df_original = pd.DataFrame(data)\n","print(\"\\nOriginal DataFrame for reference:\")\n","print(df_original)\n","\n","# Generate JSON string based on chosen orientation\n","try:\n","    json_string = df_original.to_json(orient=orient_choice)\n","    print(f\"\\nSimulated JSON string with orient='{orient_choice}':\")\n","    print(json_string)\n","\n","    # Read it back\n","    df_read = pd.read_json(io.StringIO(json_string), orient=orient_choice)\n","    print(\"\\nDataFrame read back from JSON:\")\n","    print(df_read)\n","\n","    print(\"\\nExplanation:\")\n","    print(f\"The JSON data was first created from a sample DataFrame using the '{orient_choice}' orientation.\")\n","    print(f\"Then, pd.read_json was used with the same orient='{orient_choice}' to reconstruct the DataFrame.\")\n","    if orient_choice == 'split':\n","        print(\"'split' orient stores data as a dictionary with 'index', 'columns', and 'data'.\")\n","    elif orient_choice == 'records':\n","        print(\"'records' orient stores data as a list of dictionaries, where each dictionary is a row.\")\n","    elif orient_choice == 'index':\n","        print(\"'index' orient stores data as a dictionary of dictionaries, where outer keys are index labels.\")\n","    elif orient_choice == 'columns':\n","        print(\"'columns' orient stores data as a dictionary of dictionaries, where outer keys are column labels.\")\n","    elif orient_choice == 'values':\n","        print(\"'values' orient stores data as a list of lists (just the data values). Column and index names are lost unless specified separately.\")\n","    else:\n","        print(f\"The chosen orientation '{orient_choice}' dictates the structure of the JSON and how it's parsed.\")\n","\n","except ValueError as e:\n","    print(f\"\\nError: {e}. This orientation might not be suitable for direct read_json without further parameters or the data structure.\")\n","except Exception as e:\n","    print(f\"\\nAn unexpected error occurred: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"id":"TNFgQkxS1HnZ"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","The script will:\n","\n","1.  Ask you to choose a JSON `orient` parameter.\n","2.  Create a sample DataFrame.\n","3.  Convert this DataFrame to a JSON string using your chosen `orient`.\n","4.  Print the generated JSON string so you can see its structure.\n","5.  Read this JSON string back into a new DataFrame using `pd.read_json()` with the specified `orient`.\n","6.  Display the reconstructed DataFrame.\n","    The explanation clarifies how the chosen `orient` affects the JSON structure and its parsing. This demonstrates the importance of knowing the JSON file's organization.\n","\n","-----\n"],"metadata":{"id":"QnN7rKVl1HnZ"}},{"cell_type":"markdown","source":["#### 3\\. SQL Databases üíæ\n","\n","SQL (Structured Query Language) databases are relational databases that store data in tables with predefined schemas (columns and data types). Examples include PostgreSQL, MySQL, SQL Server, Oracle, and SQLite."],"metadata":{"id":"9CaGGLKT5lE5"}},{"cell_type":"markdown","source":["**Core Process (using `sqlite3` as an example):**\n","\n","1.  Import `pandas` and the database-specific library (e.g., `sqlite3`, `psycopg2` for PostgreSQL, `mysql.connector` for MySQL).\n","2.  Establish a connection to the database. This often involves a connection string with details like hostname, database name, username, and password.\n","3.  Write your SQL query as a string.\n","4.  Use `pd.read_sql_query(query_string, connection_object)` to execute the query and load results into a DataFrame.\n","5.  Always close the database connection."],"metadata":{"id":"djcYEW6i5tYO"}},{"cell_type":"markdown","source":["\n","\n","\n","\n","\n","**Key `read_sql_query()` Arguments:**\n","\n","  * `sql`: The SQL query to be executed.\n","  * `con`: The database connection object.\n","  * `index_col`: Column to set as DataFrame index.\n","  * `parse_dates`: List of column names to parse as dates.\n","  * `chunksize`: If specified, returns an iterator where `chunksize` is the number of rows to include in each chunk. This is memory-efficient for large tables.\n","\n","**Interactive Example: Querying an SQLite Database**"],"metadata":{"id":"UXWSTIXr5ws-"}},{"cell_type":"code","source":["import pandas as pd\n","import sqlite3\n","import os\n","\n","# --- Setup a dummy SQLite database for the example ---\n","db_name = 'interactive_example.db'\n","if os.path.exists(db_name):\n","    os.remove(db_name) # Start fresh for the example\n","\n","conn = sqlite3.connect(db_name)\n","cursor = conn.cursor()\n","cursor.execute('''\n","CREATE TABLE IF NOT EXISTS employees (\n","    id INTEGER PRIMARY KEY,\n","    name TEXT NOT NULL,\n","    department TEXT,\n","    salary REAL\n",")\n","''')\n","cursor.execute(\"INSERT INTO employees (name, department, salary) VALUES ('Alice', 'Engineering', 70000)\")\n","cursor.execute(\"INSERT INTO employees (name, department, salary) VALUES ('Bob', 'Sales', 60000)\")\n","cursor.execute(\"INSERT INTO employees (name, department, salary) VALUES ('Charlie', 'Engineering', 75000)\")\n","cursor.execute(\"INSERT INTO employees (name, department, salary) VALUES ('Diana', 'HR', 65000)\")\n","conn.commit()\n","conn.close()\n","# --- End of setup ---\n","\n","print(\"Simulating querying an SQLite database.\")\n","print(\"Available columns in 'employees' table: id, name, department, salary\")\n","\n","columns_to_select = input(\"Enter column names to select, separated by commas (e.g., name,salary or * for all): \")\n","department_filter = input(\"Enter a department to filter by (or press Enter to skip filtering): \")\n","\n","# Construct the query\n","query = f\"SELECT {columns_to_select} FROM employees\"\n","params = []\n","if department_filter:\n","    query += \" WHERE department = ?\"\n","    params.append(department_filter)\n","\n","print(f\"\\nExecuting SQL Query: {query}\")\n","if params:\n","    print(f\"With parameters: {params}\")\n","\n","try:\n","    conn = sqlite3.connect(db_name) # Re-establish connection\n","    # For pd.read_sql_query, if using parameters, they are often passed to the connection's execute method,\n","    # or the query string is formatted. For simplicity with pd.read_sql_query, we'll use f-string for non-parameterized parts\n","    # and handle parameterized queries carefully.\n","    # A safer way for user inputs is to use parameterized queries with the DB-API cursor.\n","    # However, for pd.read_sql_query, one common pattern is to pass the full query.\n","\n","    if department_filter: # If filtering, it's safer to use the params argument of read_sql_query if supported directly,\n","                           # or construct the query string carefully.\n","                           # For sqlite3 and pandas, this is a common way:\n","        df = pd.read_sql_query(query, conn, params=tuple(params) if params else None)\n","    else:\n","        df = pd.read_sql_query(query, conn)\n","\n","\n","    print(\"\\nQuery Results:\")\n","    if not df.empty:\n","        print(df)\n","    else:\n","        print(\"No data returned for your query.\")\n","\n","    print(\"\\nExplanation:\")\n","    print(f\"The query selected columns '{columns_to_select}'.\")\n","    if department_filter:\n","        print(f\"Results were filtered for the '{department_filter}' department.\")\n","    else:\n","        print(\"No department filter was applied.\")\n","    print(f\"The results were loaded into a Pandas DataFrame.\")\n","\n","except sqlite3.Error as e:\n","    print(f\"\\nSQLite error: {e}\")\n","    print(\"This could be due to incorrect column names or SQL syntax.\")\n","except Exception as e:\n","    print(f\"\\nAn unexpected error occurred: {e}\")\n","finally:\n","    if 'conn' in locals() and conn:\n","        conn.close()\n","    if os.path.exists(db_name): # Clean up the dummy database\n","        os.remove(db_name)"],"outputs":[],"execution_count":null,"metadata":{"id":"S3aStNtZ1Hna"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  A temporary SQLite database `interactive_example.db` with an `employees` table is created.\n","2.  You'll be asked which columns you want to select (e.g., `name, salary` or `*`).\n","3.  You'll be asked if you want to filter by a specific department.\n","4.  The Python script constructs the SQL query based on your input.\n","5.  `pd.read_sql_query` executes this query against the database.\n","6.  The resulting data is displayed as a DataFrame.\n","    The explanation will then summarize the query that was run. For instance, if you select `name, department` and filter by `Engineering`, the output will show only the names and departments of employees in Engineering.\n","\n","-----\n","\n"],"metadata":{"id":"aOrupUCO1Hnb"}},{"cell_type":"markdown","source":["#### 4\\. NoSQL Databases (e.g., MongoDB) üìÑ\n","\n","NoSQL databases are non-relational and offer more flexible data models than SQL databases. Many, like MongoDB, store data in JSON-like documents.\n","\n","**Core Process (MongoDB with `pymongo`):**\n","\n","1.  Import `MongoClient` from `pymongo` and `pandas`.\n","2.  Establish a connection to the MongoDB instance using a connection string.\n","3.  Access the desired database and then the specific collection (analogous to a table).\n","4.  Define a query (often a dictionary for filtering).\n","5.  Use the `collection.find(query)` method, which returns a cursor (an iterable object).\n","6.  Convert the cursor to a list of dictionaries.\n","7.  Load this list into a Pandas DataFrame.\n","\n","**Interactive Example: Querying MongoDB (Simulated)**\n","\n","Since connecting to a live MongoDB requires a running instance and setup, this example will simulate the process and data."],"metadata":{"id":"ugXyADGr56-B"}},{"cell_type":"code","source":["import pandas as pd\n","# from pymongo import MongoClient # Would be used in a real scenario\n","\n","# --- Simulation Setup ---\n","# In a real scenario, you would connect to MongoDB:\n","# client = MongoClient('mongodb://localhost:27017/')\n","# db = client['mydatabase']\n","# collection = db['mycollection']\n","\n","# Simulated data that would come from a collection.find()\n","simulated_mongo_data = [\n","    {\"_id\": \"xyz123\", \"product_name\": \"Laptop\", \"category\": \"Electronics\", \"price\": 1200, \"stock\": 50},\n","    {\"_id\": \"abc456\", \"product_name\": \"Mouse\", \"category\": \"Electronics\", \"price\": 25, \"stock\": 200},\n","    {\"_id\": \"def789\", \"product_name\": \"Keyboard\", \"category\": \"Electronics\", \"price\": 75, \"stock\": 150},\n","    {\"_id\": \"ghi012\", \"product_name\": \"Desk Chair\", \"category\": \"Furniture\", \"price\": 150, \"stock\": 30},\n","    {\"_id\": \"jkl345\", \"product_name\": \"Notebook\", \"category\": \"Stationery\", \"price\": 5, \"stock\": 500}\n","]\n","# --- End of Simulation Setup ---\n","\n","print(\"Simulating querying a MongoDB collection.\")\n","print(\"Available fields in simulated data: _id, product_name, category, price, stock\")\n","\n","filter_category = input(\"Enter a category to filter products by (e.g., Electronics, Furniture, or press Enter to skip): \")\n","min_price_str = input(\"Enter a minimum price for products (e.g., 50, or press Enter to skip): \")\n","\n","# Simulate the .find() query and filtering\n","# query_dict = {} #\n","# if filter_category:\n","# query_dict['category'] = filter_category\n","# if min_price_str:\n","# try:\n","# query_dict['price'] = {'$gte': float(min_price_str)} # $gte is \"greater than or equal to\"\n","# except ValueError:\n","# print(\"Invalid price, skipping price filter.\")\n","\n","# In a real scenario: cursor = collection.find(query_dict)\n","# results_list = list(cursor)\n","\n","# Simulate filtering on our list of dictionaries\n","results_list = simulated_mongo_data\n","if filter_category:\n","    results_list = [doc for doc in results_list if doc.get('category') == filter_category]\n","\n","if min_price_str:\n","    try:\n","        min_price = float(min_price_str)\n","        results_list = [doc for doc in results_list if doc.get('price', 0) >= min_price]\n","    except ValueError:\n","        print(\"Invalid price format. Price filter not applied.\")\n","\n","\n","df = pd.DataFrame(results_list)\n","\n","print(\"\\nQuery Results (Simulated):\")\n","if not df.empty:\n","    print(df)\n","else:\n","    print(\"No data matched your criteria.\")\n","\n","print(\"\\nExplanation:\")\n","print(\"This example simulates querying a MongoDB collection.\")\n","if filter_category:\n","    print(f\"Data was filtered for category: '{filter_category}'.\")\n","# In a real MongoDB query, you'd use a query document like {'category': filter_category}.\n","if min_price_str and 'min_price' in locals():\n","    print(f\"Data was filtered for price >= {min_price}.\")\n","# In MongoDB, this would be {'price': {'$gte': min_price}}.\n","print(\"The resulting list of documents (dictionaries) was then converted into a Pandas DataFrame.\")\n","# client.close() # In a real scenario"],"outputs":[],"execution_count":null,"metadata":{"id":"G3fR66S01Hnb"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  The code uses a predefined list of Python dictionaries (`simulated_mongo_data`) to mimic what you'd get from a MongoDB `collection.find()` operation.\n","2.  It asks you if you want to filter by `category` and/or a minimum `price`.\n","3.  Based on your input, it filters the `simulated_mongo_data` list.\n","4.  This filtered list is then converted into a Pandas DataFrame and displayed.\n","    The explanation clarifies how your input criteria were used to filter the data and how this process relates to actual MongoDB queries (e.g., using `{'category': 'Electronics'}` or `{'price': {'$gte': 50}}` in the `find()` method).\n","\n","-----\n"],"metadata":{"id":"dXVNjhjP1Hnc"}},{"cell_type":"markdown","source":["\n","#### 5\\. APIs and Cloud Data Sources ‚òÅÔ∏è\n","\n","Many data providers offer access via APIs (Application Programming Interfaces), allowing direct data retrieval into your Python environment. Additionally, datasets are often hosted online (e.g., in CSV or JSON format). Pandas can often read these directly if a URL is provided.\n","\n","**Core Process (Reading CSV from URL):**\n","\n","1.  Import `pandas`.\n","2.  Provide the direct URL of the data file to `pd.read_csv()` (or `pd.read_json()`, etc.).\n","3.  You might need to specify `header=None` and provide `names` if the online file doesn't have a header row.\n","\n","**Interactive Example: Reading CSV from a URL**"],"metadata":{"id":"LATKmXNJ6Ulg"}},{"cell_type":"code","source":["import pandas as pd\n","\n","print(\"Let's try to read a CSV file directly from a URL.\")\n","# A well-known, simple CSV file for example purposes\n","default_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n","data_url = input(f\"Enter the URL of a CSV file (or press Enter to use default Iris dataset URL: {default_url}): \")\n","if not data_url:\n","    data_url = default_url\n","\n","# For the Iris dataset, column names are typically not in the file itself\n","known_iris_columns = ['sepal_length_cm', 'sepal_width_cm', 'petal_length_cm', 'petal_width_cm', 'species']\n","has_header_str = input(\"Does the CSV at this URL have a header row? (yes/no/unknown): \").lower()\n","\n","df = None\n","column_names_to_use = None\n","\n","try:\n","    if has_header_str == 'yes':\n","        print(f\"\\nAttempting to read CSV from {data_url} assuming it has a header...\")\n","        df = pd.read_csv(data_url)\n","    elif has_header_str == 'no':\n","        print(f\"\\nAttempting to read CSV from {data_url} assuming NO header...\")\n","        if data_url == default_url:\n","            column_names_to_use = known_iris_columns\n","            print(f\"Using known column names for Iris dataset: {column_names_to_use}\")\n","        else:\n","            num_cols_str = input(\"Enter the number of columns you expect: \")\n","            try:\n","                num_cols = int(num_cols_str)\n","                column_names_to_use = [f'col_{i+1}' for i in range(num_cols)]\n","                print(f\"Using generic column names: {column_names_to_use}\")\n","            except ValueError:\n","                print(\"Invalid number of columns. Will attempt to read without specifying names.\")\n","        df = pd.read_csv(data_url, header=None, names=column_names_to_use if column_names_to_use else None)\n","    else: # 'unknown' or anything else\n","        print(f\"\\nAttempting to read CSV from {data_url} (auto-detecting header)...\")\n","        df = pd.read_csv(data_url) # Let pandas try to infer\n","\n","    print(\"\\nFirst 5 rows of the DataFrame from URL:\")\n","    print(df.head())\n","    print(\"\\nShape of the DataFrame:\", df.shape)\n","\n","    print(\"\\nExplanation:\")\n","    print(f\"Pandas attempted to read data directly from the URL: {data_url}.\")\n","    if has_header_str == 'yes':\n","        print(\"It was assumed the first row contained headers.\")\n","    elif has_header_str == 'no':\n","        if column_names_to_use:\n","            print(f\"No header was assumed, and the columns were named: {df.columns.tolist()}.\")\n","        else:\n","            print(\"No header was assumed, and default integer column names were likely assigned if no names were provided.\")\n","    else:\n","        print(\"Pandas auto-detected the header status.\")\n","    print(\"This is a common way to access publicly available datasets on the web.\")\n","\n","except pd.errors.EmptyDataError:\n","    print(f\"\\nError: No data found at the URL, or the file is empty.\")\n","except pd.errors.ParserError:\n","    print(f\"\\nError: Could not parse the file. It might not be a standard CSV or the delimiter is unexpected.\")\n","except Exception as e:\n","    print(f\"\\nAn error occurred: {e}\")\n","    print(\"This could be due to an invalid URL, network issues, or the file not being a readable CSV format.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"IW16sZ1P1Hnc"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  The script asks for a URL to a CSV file (defaulting to the Iris dataset URL).\n","2.  It then asks if the CSV at that URL has a header.\n","3.  Based on your response:\n","      * 'yes': Reads normally.\n","      * 'no': Reads with `header=None`. If it's the default Iris URL, it uses known column names; otherwise, it asks for the number of columns to generate generic names.\n","      * 'unknown': Lets Pandas try to infer the header.\n","4.  It then attempts to read the CSV using `pd.read_csv()` and displays the head of the resulting DataFrame and its shape.\n","    The explanation clarifies how the header information was used and the general utility of reading directly from URLs.\n","\n","-----\n"],"metadata":{"id":"yDB156mF1Hnd"}},{"cell_type":"markdown","source":["\n","-----\n","\n","### Data Cleaning üßπüßº\n","\n","Once data is retrieved, cleaning is the next crucial step. Messy data leads to unreliable models and flawed insights (\"garbage-in, garbage-out\").  The goal of data cleaning is to identify and address errors, inconsistencies, and missing information to improve data quality for analysis and modeling.\n","\n","**Why is Cleaning So Important?**\n","\n","  * **Accurate Representation**: Clean observations (rows) and features (columns) are needed to accurately model relationships.\n","  * **Reliable Labels**: Output variables (labels) must be correctly categorized.\n","  * **Model Assumptions**: Algorithms assume the data is a reasonable reflection of reality.\n","\n","**Common Problems with Messy Data:**\n","\n","  * **Lack of Data**: Insufficient relevant data.\n","  * **Too Much Data (Disparate Systems)**: Data scattered and unorganized.\n","  * **Bad Data Quality**: A primary challenge for many organizations.\n","\n","**How Can Data Be Messy?**\n","\n","  * **Duplicate or Unnecessary Data**: Can skew results or add noise.\n","  * **Inconsistent Text and Typos**: Leads to the same value being treated as different.\n","  * **Missing Data**: Gaps in information that models can't handle directly.\n","  * **Outliers**: Extreme values that can disproportionately influence models.\n","  * **Data Sourcing Issues**: Problems combining data from different sources.\n","\n","-----\n"],"metadata":{"id":"6PcxBwhb6Z-b"}},{"cell_type":"markdown","source":["\n","#### 1\\. Handling Duplicate and Unnecessary Data\n","\n","Duplicates can be exact copies of rows or rows that are contextually duplicates (e.g., the same transaction entered twice). It's important to determine if duplicates are genuine occurrences or errors.\n","\n","**Methods:**\n","\n","  * `df.duplicated()`: Returns a boolean Series indicating duplicate rows.\n","  * `df.drop_duplicates()`: Removes duplicate rows.\n","      * `subset`: Consider only certain columns for identifying duplicates.\n","      * `keep`: Which duplicate to keep ('first', 'last', `False` to drop all).\n","\n","**Interactive Example: Identifying and Handling Duplicates**"],"metadata":{"id":"Y-TOK4oD6orI"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data with duplicates\n","data = {\n","    'ID': [1, 2, 3, 1, 4, 5, 2],\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'David', 'Eve', 'Bob'],\n","    'Value': [100, 150, 200, 100, 250, 100, 150]\n","}\n","df_duplicates = pd.DataFrame(data)\n","print(\"Original DataFrame with duplicates:\")\n","print(df_duplicates)\n","\n","# Ask user for columns to consider for identifying duplicates\n","print(\"\\nIdentify duplicates based on specific columns or all columns?\")\n","subset_choice_str = input(\"Enter column names (comma-separated) or 'all' for all columns: \").lower()\n","\n","if subset_choice_str == 'all':\n","    subset_cols = None\n","    print(\"Checking for duplicates based on all columns.\")\n","else:\n","    subset_cols = [col.strip() for col in subset_choice_str.split(',')]\n","    # Validate if provided columns exist\n","    valid_cols = [col for col in subset_cols if col in df_duplicates.columns]\n","    if len(valid_cols) != len(subset_cols):\n","        print(f\"Warning: Some specified columns not found. Using valid columns: {valid_cols}\")\n","    subset_cols = valid_cols if valid_cols else None # Use None if no valid cols provided\n","    if subset_cols:\n","        print(f\"Checking for duplicates based on columns: {subset_cols}\")\n","    else:\n","        print(\"No valid columns specified for subset, checking based on all columns.\")\n","\n","\n","# Identify duplicates\n","duplicate_rows = df_duplicates.duplicated(subset=subset_cols, keep=False) # keep=False marks all duplicates as True\n","print(\"\\nRows identified as duplicates (True means it's a duplicate based on criteria):\")\n","print(df_duplicates[duplicate_rows])\n","if df_duplicates[duplicate_rows].empty:\n","    print(\"No duplicates found based on your criteria.\")\n","\n","\n","# Ask user whether to drop duplicates and which to keep\n","drop_choice = input(\"\\nDo you want to remove these duplicates? (yes/no): \").lower()\n","if drop_choice == 'yes':\n","    keep_option = input(\"Which duplicate to keep? ('first', 'last', or 'none' to drop all identical duplicates): \").lower()\n","    if keep_option not in ['first', 'last', 'none']:\n","        print(\"Invalid keep option. Defaulting to 'first'.\")\n","        keep_option = 'first'\n","\n","    keep_param = keep_option if keep_option != 'none' else False\n","    df_cleaned = df_duplicates.drop_duplicates(subset=subset_cols, keep=keep_param)\n","    print(\"\\nDataFrame after handling duplicates:\")\n","    print(df_cleaned)\n","    print(\"\\nExplanation:\")\n","    if not df_duplicates[duplicate_rows].empty:\n","        print(f\"Duplicates were identified based on {subset_cols if subset_cols else 'all columns'}.\")\n","        if keep_param == 'first':\n","            print(\"The first occurrence of each duplicate set was kept.\")\n","        elif keep_param == 'last':\n","            print(\"The last occurrence of each duplicate set was kept.\")\n","        else: # False\n","            print(\"All occurrences of duplicate sets were removed.\")\n","        print(f\"{len(df_duplicates) - len(df_cleaned)} duplicate row(s) were removed.\")\n","    else:\n","        print(\"No duplicates were found or removed based on your criteria.\")\n","\n","else:\n","    print(\"\\nDuplicates were identified but not removed.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"agNG2cHu1Hnd"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  A sample DataFrame with duplicate rows is created.\n","2.  You'll be asked which columns to use for identifying duplicates (e.g., just 'ID' and 'Name', or 'all').\n","3.  The script will show you the rows it considers duplicates based on your input using `df.duplicated(keep=False)` (which marks *all* occurrences of a duplicate).\n","4.  You'll then be asked if you want to remove them and which occurrence to `keep` (`first`, `last`, or `none` to drop all identified duplicates).\n","5.  The DataFrame after applying `drop_duplicates()` will be shown.\n","    The explanation will clarify which rows were considered duplicates based on the chosen subset of columns and how the `keep` parameter influenced the result. For example, if rows with ID 1 and Name 'Alice' appear twice, specifying `subset=['ID', 'Name']` and `keep='first'` will remove the second occurrence.\n"],"metadata":{"id":"fu35reAr1Hnd"}},{"cell_type":"markdown","source":["\n","-----\n","\n","#### 2\\. Handling Inconsistent Text and Typos ‚úçÔ∏è\n","\n","Variations in spelling, capitalization, or extra spaces can cause the same conceptual value to be treated as distinct categories by algorithms.\n","\n","**Common Techniques:**\n","\n","  * **Convert to a consistent case**: `str.lower()` or `str.upper()`.\n","  * **Strip whitespace**: `str.strip()` (removes leading/trailing), `str.lstrip()`, `str.rstrip()`.\n","  * **Replace values**: `series.replace({'old_value': 'new_value'}, regex=True/False)`.\n","  * **String splitting and manipulation**: `str.split()`, `str.contains()`, `str.extract()` (with regular expressions).\n","\n","**Interactive Example: Cleaning Inconsistent Text**"],"metadata":{"id":"xYiyPiLb6xG1"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Sample data with inconsistent text\n","data = {\n","    'City': [' New York ', 'london', 'Paris', 'new york', ' London', 'tokyo', '  paris  '],\n","    'Status': ['Active', 'active', ' Inactive ', 'Pending', 'ACTIVE', '  pending', 'Active']\n","}\n","df_text = pd.DataFrame(data)\n","print(\"Original DataFrame with inconsistent text:\")\n","print(df_text)\n","\n","# Ask user for column and cleaning action\n","column_to_clean = input(f\"\\nEnter the column name to clean from {df_text.columns.tolist()}: \")\n","\n","if column_to_clean in df_text.columns:\n","    print(f\"\\nUnique values in '{column_to_clean}' before cleaning:\")\n","    print(df_text[column_to_clean].unique())\n","\n","    action = input(\"Choose a cleaning action: 'lowercase', 'strip_whitespace', 'standardize_status' (specific to 'Status' column for demo): \").lower()\n","    df_cleaned_text = df_text.copy()\n","\n","    if action == 'lowercase':\n","        df_cleaned_text[column_to_clean] = df_cleaned_text[column_to_clean].str.lower()\n","        print(f\"\\nApplied lowercase to '{column_to_clean}'.\")\n","    elif action == 'strip_whitespace':\n","        df_cleaned_text[column_to_clean] = df_cleaned_text[column_to_clean].str.strip()\n","        print(f\"\\nStripped leading/trailing whitespace from '{column_to_clean}'.\")\n","    elif action == 'standardize_status' and column_to_clean == 'Status':\n","        # Example of a more complex standardization:\n","        # Lowercase, strip, then map to consistent values\n","        df_cleaned_text[column_to_clean] = df_cleaned_text[column_to_clean].str.lower().str.strip()\n","        status_map = {'active': 'Active', 'inactive': 'Inactive', 'pending': 'Pending'}\n","        # Only replace known values, keep others as is or mark them.\n","        # For simplicity, we'll use .map which will turn unmapped values to NaN if not handled.\n","        # A safer way is to use .replace or a custom function.\n","        # Let's use replace for more robustness:\n","        df_cleaned_text[column_to_clean] = df_cleaned_text[column_to_clean].replace(status_map)\n","        print(f\"\\nStandardized '{column_to_clean}' (lowercase, strip, map to consistent values).\")\n","    else:\n","        print(\"Invalid action or action not applicable to the chosen column. No changes made.\")\n","\n","    print(f\"\\nUnique values in '{column_to_clean}' after cleaning:\")\n","    print(df_cleaned_text[column_to_clean].unique())\n","    print(\"\\nDataFrame after text cleaning:\")\n","    print(df_cleaned_text)\n","    print(\"\\nExplanation:\")\n","    print(f\"The '{column_to_clean}' column was processed. Depending on your choice:\")\n","    if action == 'lowercase':\n","        print(\"All text was converted to lowercase, making 'New York' and 'new york' identical.\")\n","    elif action == 'strip_whitespace':\n","        print(\"Leading and trailing spaces were removed, e.g., ' New York ' became 'New York'.\")\n","    elif action == 'standardize_status' and column_to_clean == 'Status':\n","        print(\"Text was lowercased, stripped of whitespace, and then mapped to standard capitalizations like 'Active', 'Inactive'.\")\n","else:\n","    print(f\"Column '{column_to_clean}' not found in DataFrame.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"-qS2YfU91Hne"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  A DataFrame with inconsistent `City` and `Status` names is created.\n","2.  You'll be asked to choose a column (`City` or `Status`) and a cleaning action (`lowercase`, `strip_whitespace`, or `standardize_status` for the 'Status' column).\n","3.  The script applies the chosen transformation.\n","      * `lowercase`: Converts all text in the chosen column to lowercase (e.g., \"London\" and \"london\" become \"london\").\n","      * `strip_whitespace`: Removes spaces from the beginning and end of strings (e.g., \" New York \" becomes \"New York\").\n","      * `standardize_status`: (If 'Status' column is chosen) Converts to lowercase, strips whitespace, and then maps variations like 'active' to a standard 'Active'.\n","4.  The unique values before and after cleaning, and the modified DataFrame, are shown. The explanation details the effect of your chosen action.\n","\n","-----\n"],"metadata":{"id":"GB5BWOhA1Hne"}},{"cell_type":"markdown","source":["\n","#### 3\\. Handling Missing Values (NaN) üëª\n","\n","Missing values (often represented as `NaN` in Pandas) can cause errors in many machine learning algorithms. They must be handled.\n","\n","**Common Policies:**\n","\n","1.  **Remove the Data**:\n","      * Delete rows with missing values (`df.dropna(axis=0)`).\n","      * Delete columns with too many missing values (`df.dropna(axis=1)`).\n","      * **Pros**: Quick, no artificial data.\n","      * **Cons**: Loss of information, potential bias if missingness is not random.\n","2.  **Impute Missing Data**:\n","      * Fill NaNs with a calculated value (e.g., mean, median, mode, or a constant).\n","          * `df['col'].fillna(df['col'].mean())`\n","          * `df['col'].fillna(df['col'].median())`\n","          * `df['col'].fillna(df['col'].mode()[0])` (mode can return multiple values)\n","      * More advanced: regression imputation, k-NN imputation.\n","      * **Pros**: Retains data rows/columns.  **Cons**: Adds estimated values, potentially reducing variance or introducing bias.\n","3.  **Mask the Data**:\n","      * Treat missingness as a separate category. For example, fill NaN in a categorical column with \"Missing\".\n","      * **Assumption**: The fact that data is missing is informative.\n","      * **Pros**: No data loss, captures information from missingness.\n","      * **Cons**: Adds a new category; assumption might be incorrect.\n","\n","**Interactive Example: Handling Missing Values**"],"metadata":{"id":"_mqwxF3G61qz"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Sample data with missing values\n","data_messy = {\n","    'ID': [1, 2, 3, 4, 5, 6],\n","    'Age': [25, 30, np.nan, 35, 40, np.nan],\n","    'Salary': [50000, 60000, 55000, np.nan, 70000, 65000],\n","    'Category': ['A', 'B', np.nan, 'A', 'C', 'B']\n","}\n","df_missing = pd.DataFrame(data_messy)\n","print(\"Original DataFrame with missing values (NaN):\")\n","print(df_missing)\n","print(\"\\nMissing values per column:\")\n","print(df_missing.isnull().sum())\n","\n","# Ask user for handling strategy\n","column_to_handle = input(f\"\\nEnter column name to handle missing values from {df_missing.columns.tolist()}: \")\n","\n","if column_to_handle in df_missing.columns:\n","    strategy = input(f\"Choose a strategy for NaNs in '{column_to_handle}': 'drop_row_if_nan_in_col' (drops rows where this column is NaN), 'fill_mean' (if numeric), 'fill_median' (if numeric), 'fill_mode', 'fill_specific': \").lower()\n","    df_handled = df_missing.copy()\n","    action_taken = \"None\"\n","\n","    if strategy == 'drop_row_if_nan_in_col':\n","        df_handled.dropna(subset=[column_to_handle], inplace=True)\n","        action_taken = f\"Dropped rows where '{column_to_handle}' was NaN.\"\n","    elif strategy == 'fill_mean' and pd.api.types.is_numeric_dtype(df_handled[column_to_handle]):\n","        fill_value = df_handled[column_to_handle].mean()\n","        df_handled[column_to_handle].fillna(fill_value, inplace=True)\n","        action_taken = f\"Filled NaNs in '{column_to_handle}' with mean ({fill_value:.2f}).\"\n","    elif strategy == 'fill_median' and pd.api.types.is_numeric_dtype(df_handled[column_to_handle]):\n","        fill_value = df_handled[column_to_handle].median()\n","        df_handled[column_to_handle].fillna(fill_value, inplace=True)\n","        action_taken = f\"Filled NaNs in '{column_to_handle}' with median ({fill_value:.2f}).\"\n","    elif strategy == 'fill_mode':\n","        fill_value = df_handled[column_to_handle].mode()\n","        if not fill_value.empty:\n","            fill_value = fill_value[0] # Mode can return multiple values\n","            df_handled[column_to_handle].fillna(fill_value, inplace=True)\n","            action_taken = f\"Filled NaNs in '{column_to_handle}' with mode ('{fill_value}').\"\n","        else:\n","            action_taken = f\"Could not determine mode for '{column_to_handle}'. No changes made.\"\n","    elif strategy == 'fill_specific':\n","        specific_value = input(f\"Enter the specific value to fill NaNs in '{column_to_handle}': \")\n","        # Try to convert to column's type if possible, or use as string\n","        try:\n","            if pd.api.types.is_numeric_dtype(df_handled[column_to_handle]):\n","                specific_value = float(specific_value) # Or int, depending on desired precision\n","            elif pd.api.types.is_datetime64_any_dtype(df_handled[column_to_handle]):\n","                specific_value = pd.to_datetime(specific_value)\n","        except ValueError:\n","            print(f\"Warning: Could not convert '{specific_value}' to a numeric/datetime type for column '{column_to_handle}'. Using as string or original type if conversion fails.\")\n","        df_handled[column_to_handle].fillna(specific_value, inplace=True)\n","        action_taken = f\"Filled NaNs in '{column_to_handle}' with '{specific_value}'.\"\n","    else:\n","        action_taken = \"Invalid strategy or strategy not applicable for column type. No changes made.\"\n","\n","    print(\"\\nDataFrame after handling missing values:\")\n","    print(df_handled)\n","    print(\"\\nMissing values per column after handling:\")\n","    print(df_handled.isnull().sum())\n","    print(\"\\nExplanation:\")\n","    print(action_taken)\n","\n","else:\n","    print(f\"Column '{column_to_handle}' not found.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"RyJPPGgF1Hne"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  A DataFrame with `NaN` values in `Age`, `Salary`, and `Category` is created.\n","2.  The script shows the count of missing values per column.\n","3.  You'll be asked to pick a column and a strategy:\n","      * `drop_row_if_nan_in_col`: Removes any row where the chosen column has `NaN`.\n","      * `fill_mean`/`fill_median`: If the column is numeric, fills `NaN` with the column's mean or median.\n","      * `fill_mode`: Fills `NaN` with the column's mode (most frequent value).\n","      * `fill_specific`: Prompts you for a value to fill NaNs with.\n","4.  The DataFrame after applying the strategy is shown, along with the new count of missing values and an explanation of what was done. For example, if you choose to `fill_mean` for the `Age` column, the `NaN`s in `Age` will be replaced by the average age.\n"],"metadata":{"id":"AG9hsVhk1Hne"}},{"cell_type":"markdown","source":["#### 4\\. Handling Outliers üìàüìâ\n","\n","Outliers are data points that are significantly different from other observations. They can skew statistical analyses and degrade model performance, especially for algorithms sensitive to extreme values (e.g., linear regression, anything using mean). However, some outliers can be genuine and informative.\n","\n","**Methods to Find Outliers:**\n","\n","* **Visualization**: Box plots, histograms, scatter plots. Box plots visually show the Interquartile Range (IQR) and identify points beyond $1.5 \\times IQR$.\n","* **Statistical Methods**:\n","    * **IQR Method**: Calculate $Q1$ (25th percentile) and $Q3$ (75th percentile). $IQR = Q3 - Q1$. Outliers are typically values below $Q1 - 1.5 \\times IQR$ or above $Q3 + 1.5 \\times IQR$.\n","    * **Z-score**: Measures how many standard deviations a data point is from the mean. A common threshold for outliers is a Z-score greater than 3 or less than -3.\n","        $Z = (X - \\mu) / \\sigma$\n","        where $X$ is the data point, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n","* **Model-Based Methods**: Residual analysis from models.\n","\n","**Policies to Deal with Outliers:**\n","\n","1.  **Remove Them**: Delete the outlier observations.\n","    * **Pros**: Simple. **Cons**: Loss of data, which might be important.\n","2.  **Assign a Different Value (Capping/Winsorizing)**: Replace outliers with a less extreme value (e.g., the boundary value from IQR/Z-score, or a percentile like 1st/99th).\n","    * **Pros**: Keeps the row data. **Cons**: Modifies original data, might introduce bias.\n","3.  **Transform the Column**: Apply a mathematical transformation (e.g., log, square root, Box-Cox) to reduce skewness and the impact of outliers.\n","    * **Pros**: Can normalize distribution. **Cons**: Makes interpretation harder.\n","4.  **Keep the Value**: If the outlier is genuine and informative, or if using outlier-robust algorithms (e.g., tree-based models, median-based regression).\n","\n","**Interactive Example: Finding and Handling Outliers using IQR**"],"metadata":{"id":"jV_xprqg7mFD"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Sample data with a potential outlier\n","data_values = {'Sales': [10, 12, 15, 14, 16, 18, 20, 22, 25, 30, 3000]} # 3000 is an outlier\n","df_sales = pd.DataFrame(data_values)\n","print(\"Original DataFrame with potential outlier:\")\n","print(df_sales)\n","\n","# Ask user for outlier handling choice for 'Sales' column\n","print(\"\\nHandling outliers in 'Sales' column using IQR method.\")\n","q1 = df_sales['Sales'].quantile(0.25)\n","q3 = df_sales['Sales'].quantile(0.75)\n","iqr = q3 - q1\n","lower_bound = q1 - 1.5 * iqr\n","upper_bound = q3 + 1.5 * iqr\n","\n","print(f\"Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n","print(f\"Lower bound for outliers: {lower_bound}\")\n","print(f\"Upper bound for outliers: {upper_bound}\")\n","\n","outliers_found = df_sales[(df_sales['Sales'] < lower_bound) | (df_sales['Sales'] > upper_bound)]\n","print(\"\\nOutliers identified using IQR method:\")\n","if not outliers_found.empty:\n","    print(outliers_found)\n","else:\n","    print(\"No outliers found based on IQR.\")\n","\n","df_handled_outliers = df_sales.copy()\n","action_taken_outlier = \"No action taken or no outliers found to handle.\"\n","\n","if not outliers_found.empty:\n","    outlier_strategy = input(\"Choose how to handle these outliers: 'remove', 'cap_to_bounds', 'transform_log', 'keep': \").lower()\n","\n","    if outlier_strategy == 'remove':\n","        df_handled_outliers = df_sales[~((df_sales['Sales'] < lower_bound) | (df_sales['Sales'] > upper_bound))]\n","        action_taken_outlier = f\"Removed {len(outliers_found)} outlier(s).\"\n","    elif outlier_strategy == 'cap_to_bounds':\n","        df_handled_outliers['Sales'] = np.where(\n","            df_handled_outliers['Sales'] < lower_bound,\n","            lower_bound,\n","            np.where(\n","                df_handled_outliers['Sales'] > upper_bound,\n","                upper_bound,\n","                df_handled_outliers['Sales']\n","            )\n","        )\n","        action_taken_outlier = f\"Capped {len(outliers_found)} outlier(s) to the IQR bounds [{lower_bound:.2f}, {upper_bound:.2f}].\"\n","    elif outlier_strategy == 'transform_log':\n","        # Adding 1 to avoid log(0) if Sales can be 0, though not in this specific sample\n","        df_handled_outliers['Sales_Log'] = np.log1p(df_handled_outliers['Sales'])\n","        action_taken_outlier = \"Applied log transformation (log1p) to 'Sales' column (new column: 'Sales_Log'). Outlier impact might be reduced in transformed scale.\"\n","    elif outlier_strategy == 'keep':\n","        action_taken_outlier = \"Outliers were kept as is. Consider using robust models.\"\n","    else:\n","        action_taken_outlier = \"Invalid strategy. Outliers were kept.\"\n","\n","    print(\"\\nDataFrame after handling outliers:\")\n","    print(df_handled_outliers)\n","else:\n","    print(\"\\nNo outliers to handle.\")\n","\n","\n","print(\"\\nExplanation:\")\n","print(action_taken_outlier)\n","if 'outlier_strategy' in locals() and outlier_strategy == 'transform_log' and not outliers_found.empty :\n","    # Check for outliers in the new log-transformed column\n","    if 'Sales_Log' in df_handled_outliers.columns:\n","        q1_log = df_handled_outliers['Sales_Log'].quantile(0.25)\n","        q3_log = df_handled_outliers['Sales_Log'].quantile(0.75)\n","        iqr_log = q3_log - q1_log\n","        lower_bound_log = q1_log - 1.5 * iqr_log\n","        upper_bound_log = q3_log + 1.5 * iqr_log\n","        log_outliers = df_handled_outliers[(df_handled_outliers['Sales_Log'] < lower_bound_log) | (df_handled_outliers['Sales_Log'] > upper_bound_log)]\n","        if not log_outliers.empty:\n","            print(f\"After log transformation, these are outliers in 'Sales_Log' based on its own IQR: \\n{log_outliers}\")\n","        else:\n","            print(\"After log transformation, no outliers detected in 'Sales_Log' based on its own IQR.\")"],"outputs":[],"execution_count":null,"metadata":{"id":"HdhtDcwP1Hne"}},{"cell_type":"markdown","source":["**Explanation of Output:**\n","\n","1.  A DataFrame `df_sales` is created with a clear outlier (3000).\n","2.  The IQR, Q1, Q3, and outlier boundaries ($Q1 - 1.5 \\\\times IQR$, $Q3 + 1.5 \\\\times IQR$) are calculated and displayed for the `Sales` column.\n","3.  Any values falling outside these boundaries are identified and shown.\n","4.  You'll be asked to choose a strategy:\n","      * `remove`: Deletes rows containing outliers.\n","      * `cap_to_bounds`: Replaces outliers with the calculated lower or upper boundary value.\n","      * `transform_log`: Applies a log transformation (`np.log1p` which calculates $\\\\log(1+x)$ to handle potential zeros gracefully) to the `Sales` column, creating a new `Sales_Log` column. This can compress the range of values, making outliers less extreme in the transformed scale.\n","      * `keep`: Makes no changes to the outliers.\n","5.  The resulting DataFrame is shown, and the explanation details which strategy was applied and its effect. If log transformation was chosen, it might also comment on whether the value is still an outlier in the transformed scale.\n","\n","-----\n"],"metadata":{"id":"BZ3lTKjw1Hne"}},{"cell_type":"markdown","source":["\n","-----\n","\n","### Summary and Review üìù\n","\n","We've covered retrieving data from diverse sources like CSVs, JSON files, SQL and NoSQL databases, and web URLs, primarily using the Pandas library. Key aspects included file formats, database connection, querying, and function parameters. We then emphasized the \"garbage-in, garbage-out\" principle and the criticality of data cleaning. Common data issues like duplicates, inconsistencies, missing values, and outliers were identified. Finally, we detailed policies and interactive examples for handling these issues, such as removing, imputing, or masking missing data, and various strategies for detecting and addressing outliers. These foundational steps are paramount for preparing high-quality data, which is the bedrock of successful machine learning models.\n","\n","-----\n"],"metadata":{"id":"CR9vGv938SMw"}},{"cell_type":"markdown","source":["\n","-----\n","\n","### Exercise: 50 Questions on Data Retrieval and Cleaning üß†\n","\n","Here are 50 questions based on the covered topics, with hints pointing to relevant concepts.\n","\n","-----\n","\n","**Question 1:**\n","\n","What is often considered the initial and crucial phase in a machine learning workflow that involves obtaining and preparing data?\n","\n","  * Hint: It's about getting your raw materials ready. [cite: 1, 3, 194]\n","\n","-----\n","\n","**Question 2:**\n","\n","Explain the \"garbage-in, garbage-out\" principle in the context of machine learning.\n","\n","  * Hint: How does input data quality affect model output? [cite: 5, 195]\n","\n","-----\n","\n","**Question 3:**\n","\n","List at least three common problems that arise from working with messy data.\n","\n","  * Hint: Think about data sufficiency, organization, and inherent quality. [cite: 86, 87, 88, 196]\n","\n","-----\n","\n","**Question 4:**\n","\n","Name four different types of sources from which data can be retrieved as discussed.\n","\n","  * Hint: Consider files, structured databases, flexible databases, and web access. [cite: 6, 197]\n","\n","-----\n","\n","**Question 5:**\n","\n","What does the acronym CSV stand for?\n","\n","  * Hint: It describes the file's content structure and common delimiter. [cite: 199]\n","\n","-----\n","\n","**Question 6:**\n","\n","In a standard CSV file, how are individual data values within a row typically separated?\n","\n","  * Hint: The name itself provides a clue. [cite: 7, 200]\n","\n","-----\n","\n","**Question 7:**\n","\n","Which Pandas function is most commonly used to read data from CSV files into a DataFrame?\n","\n","  * Hint: `pd.read_???` [cite: 8, 200]\n","\n","-----\n","\n","**Question 8:**\n","\n","If you have a file where values are separated by tabs instead of commas, which argument in `pd.read_csv()` would you use to specify this?\n","\n","  * Hint: It's related to the separator or delimiter. [cite: 15, 201]\n","\n","-----\n","\n","**Question 9:**\n","\n","When data in a file is separated by multiple or inconsistent spaces, what value can you use for the `sep` (or `delimiter`) argument in `pd.read_csv()`?\n","\n","  * Hint: It's a regular expression for one or more whitespace characters. [cite: 16, 202]\n","\n","-----\n","\n","**Question 10:**\n","\n","How can you instruct `pd.read_csv()` to use the second row of a CSV file as the header (column names)?\n","\n","  * Hint: This argument takes an integer row index. [cite: 17, 204]\n","\n","-----\n","\n","**Question 11:**\n","\n","Which argument in `pd.read_csv()` allows you to provide your own list of column names, especially useful if the file has no header?\n","\n","  * Hint: It takes a list of strings. [cite: 19, 205]\n","\n","-----\n","\n","**Question 12:**\n","\n","How can you tell `pd.read_csv()` to interpret specific strings like \"Not Available\" or \"-99\" in your CSV file as missing (NaN) values?\n","\n","  * Hint: This argument accepts a list of strings to be treated as NA. [cite: 21, 207]\n","\n","-----\n","\n","**Question 13:**\n","\n","What does the acronym JSON stand for?\n","\n","  * Hint: It relates to JavaScript and how data is structured. [cite: 208]\n","\n","-----\n","\n","**Question 14:**\n","\n","JSON files are a common data format associated with which broad category of databases, often contrasted with SQL databases?\n","\n","  * Hint: These databases are generally non-relational. [cite: 24, 209]\n","\n","-----\n","\n","**Question 15:**\n","\n","The key-value pair structure of JSON data is most similar to which fundamental Python data structure?\n","\n","  * Hint: Think of how you store data with named keys in Python. [cite: 24, 211]\n","\n","-----\n","\n","**Question 16:**\n","\n","What is the primary Pandas function used for reading data from JSON files?\n","\n","  * Hint: `pd.read_???` [cite: 25, 212]\n","\n","-----\n","\n","**Question 17:**\n","\n","When reading a JSON file with `pd.read_json()`, if the default parsing doesn't work, which important argument should you investigate to specify the JSON object's structure (e.g., 'records', 'columns')?\n","\n","  * Hint: It defines the expected JSON string format. [cite: 25, 213]\n","\n","-----\n","\n","**Question 18:**\n","\n","What DataFrame method would you use to save its contents into a JSON file?\n","\n","  * Hint: `your_dataframe.to_???()` [cite: 25, 214]\n","\n","-----\n","\n","**Question 19:**\n","\n","What does SQL stand for?\n","\n","  * Hint: It's a language for managing and querying relational databases. [cite: 215]\n","\n","-----\n","\n","**Question 20:**\n","\n","Briefly describe the typical structure of SQL databases regarding their schema and data organization.\n","\n","  * Hint: Are they flexible or rigid in structure? Relational? [cite: 29, 30, 216]\n","\n","-----\n","\n","**Question 21:**\n","\n","Name three specific examples of SQL database systems mentioned in the text.\n","\n","  * Hint: Examples include systems from Microsoft, open-source communities, and IBM. [cite: 30, 217]\n","\n","-----\n","\n","**Question 22:**\n","\n","Which Pandas function is generally used to execute a SQL query and return its results as a DataFrame?\n","\n","  * Hint: `pd.read_sql_???` or `pd.read_sql_query`. [cite: 32, 218]\n","\n","-----\n","\n","**Question 23:**\n","\n","What are the two essential pieces of information you must provide to the `pd.read_sql_query()` function?\n","\n","  * Hint: One is the query itself, and the other is how to access the database. [cite: 33, 219]\n","\n","-----\n","\n","**Question 24:**\n","\n","When using a library like `sqlite3` to interact with an SQL database, what is the first step you typically take after importing the library, usually involving the database file or server details?\n","\n","  * Hint: You need to create a link or pathway to the database. [cite: 33, 34]\n","\n","-----\n","\n","**Question 25:**\n","\n","In the process of reading from an SQL database, after establishing a connection, what do you typically define or write before using `pd.read_sql_query()`?\n","\n","  * Hint: It's the command that tells the database what data you want. [cite: 33, 35]\n","\n","-----\n","\n","**Question 26:**\n","\n","If an SQL table contains columns with date information stored as text, which argument in `pd.read_sql_query()` can you use to convert these into proper datetime objects in your DataFrame?\n","\n","  * Hint: This argument takes a list of column names. [cite: 40, 223]\n","\n","-----\n","\n","**Question 27:**\n","\n","How does the `chunksize` argument in `pd.read_sql_query()` help when dealing with very large datasets from an SQL database?\n","\n","  * Hint: It affects how much data is loaded into memory at once. [cite: 40, 41, 225]\n","\n","-----\n","\n","**Question 28:**\n","\n","When `chunksize` is specified in `pd.read_sql_query()`, what kind of object does the function return instead of a complete DataFrame?\n","\n","  * Hint: It's something you can loop through to process data piece by piece. [cite: 40, 41, 226]\n","\n","-----\n","\n","**Question 29:**\n","\n","How do NoSQL databases generally differ from SQL databases in terms of their schema and data structure?\n","\n","  * Hint: Consider flexibility and whether they are relational. [cite: 53, 227]\n","\n","-----\n","\n","**Question 30:**\n","\n","What is a common format used for storing data within many NoSQL databases, such as MongoDB?\n","\n","  * Hint: It's a format also popular for web APIs and configuration files. [cite: 53, 231]\n","\n","-----\n","\n","**Question 31:**\n","\n","Name two types of NoSQL database models mentioned in the text.\n","\n","  * Hint: One organizes data in JSON-like structures, another focuses on relationships. [cite: 53, 230]\n","\n","-----\n","\n","**Question 32:**\n","\n","Which Python library is specifically mentioned for connecting to and interacting with MongoDB?\n","\n","  * Hint: Its name is derived from \"Python\" and \"MongoDB\". [cite: 55, 232]\n","\n","-----\n","\n","**Question 33:**\n","\n","In MongoDB, what is the term for a grouping of documents, which is analogous to a 'table' in SQL databases?\n","\n","  * Hint: It's a gathering or assemblage of documents. [cite: 58, 234]\n","\n","-----\n","\n","**Question 34:**\n","\n","When you execute a `find()` query on a MongoDB collection using `pymongo`, what kind of object is typically returned?\n","\n","  * Hint: It's an iterable object that yields documents one by one, not the full list immediately. [cite: 59, 235]\n","\n","-----\n","\n","**Question 35:**\n","\n","How do you typically convert the iterable result (cursor) from a MongoDB `find()` query into a standard Python list of dictionaries?\n","\n","  * Hint: Use a built-in Python function for type conversion. [cite: 60, 236]\n","\n","-----\n","\n","**Question 36:**\n","\n","Once you have a list of Python dictionaries (e.g., from a NoSQL query), how can you load this data into a Pandas DataFrame?\n","\n","  * Hint: The DataFrame constructor can directly accept this. [cite: 60, 237]\n","\n","-----\n","\n","**Question 37:**\n","\n","What does API stand for?\n","\n","  * Hint: It's a way for different software applications to communicate. [cite: 238]\n","\n","-----\n","\n","**Question 38:**\n","\n","If a CSV file is hosted on a website and directly accessible via a URL, how can you read it into a Pandas DataFrame without downloading it first?\n","\n","  * Hint: The `read_csv` function can accept more than just local file paths. [cite: 72, 239]\n","\n","-----\n","\n","**Question 39:**\n","\n","According to the text, identify at least three distinct ways data can be 'messy'.\n","\n","  * Hint: Think about extra rows, formatting issues, empty cells, and extreme values. [cite: 90, 91, 94, 96, 241]\n","\n","-----\n","\n","**Question 40:**\n","\n","How might duplicate data entries (e.g., the same transaction recorded multiple times) negatively affect a machine learning model?\n","\n","  * Hint: Consider how it might unfairly influence the model's learning about certain patterns. [cite: 91, 243]\n","\n","-----\n","\n","**Question 41:**\n","\n","Should all duplicate observations in a dataset always be removed? Explain with an example.\n","\n","  * Hint: Consider if the duplicates are errors or represent genuine, repeated events/observations. [cite: 92, 100, 101, 245]\n","\n","-----\n","\n","**Question 42:**\n","\n","Why is it generally necessary to handle missing values (NaNs) before feeding data into most machine learning algorithms?\n","\n","  * Hint: Algorithms usually expect a full set of inputs. [cite: 94, 109, 246]\n","\n","-----\n","\n","**Question 43:**\n","\n","What are the three primary policies discussed for dealing with missing data in a dataset?\n","\n","  * Hint: One involves removal, another involves filling, and the last treats missingness as information. [cite: 110, 247]\n","\n","-----\n","\n","**Question 44:**\n","\n","What is a significant drawback of the policy of removing entire rows that contain missing values?\n","\n","  * Hint: Think about the potential loss of valuable information and the size of your dataset. [cite: 113, 249]\n","\n","-----\n","\n","**Question 45:**\n","\n","What is the main conceptual downside of imputing missing data using statistical measures like the mean or median?\n","\n","  * Hint: You are introducing values that were not originally measured. [cite: 117, 251]\n","\n","-----\n","\n","**Question 46:**\n","\n","What underlying assumption is made when you choose to \"mask\" missing data by creating a new category (e.g., \"Missing\") for NaN values?\n","\n","  * Hint: It implies that the absence of data is itself meaningful. [cite: 119, 120, 252]\n","\n","-----\n","\n","**Question 47:**\n","\n","Define an \"outlier\" in the context of a dataset.\n","\n","  * Hint: It's a data point that significantly deviates from other observations. [cite: 96, 140, 253]\n","\n","-----\n","\n","**Question 48:**\n","\n","Name at least two distinct methods or tools mentioned for identifying potential outliers in data.\n","\n","  * Hint: One is visual, another is statistical (e.g., using quartiles). [cite: 143, 145, 254]\n","\n","-----\n","\n","**Question 49:**\n","\n","List three common policies or strategies for dealing with identified outliers in a dataset.\n","\n","  * Hint: Options include removal, adjustment, or changing the data's scale. [cite: 159, 161, 164, 255]\n","\n","-----\n","\n","**Question 50:**\n","\n","When might it be appropriate to *keep* an outlier in your dataset rather than removing or altering it?\n","\n","  * Hint: Consider the nature of the outlier and the type of model you plan to use. [cite: 169]"],"metadata":{"id":"NA9WoKHC8UaS"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's continue building on the statistical concepts.\n",
        "\n",
        "-----\n",
        "\n",
        "### 7\\. Hypothesis Testing 🧐\n",
        "\n",
        "#### Explanation\n",
        "\n",
        "**Hypothesis testing** is a formal procedure used in statistical inference to decide between two competing statements or claims about a population parameter, based on sample data[cite: 103]. It's like a statistical court case: you have an initial claim, and you look at the evidence (data) to decide if there's enough to reject that initial claim.\n",
        "\n",
        "The process involves formulating two hypotheses:\n",
        "\n",
        "1.  **Null Hypothesis ($H\\_0$)**: This is typically a statement of \"no effect,\" \"no difference,\" or that a parameter is equal to a specific baseline value[cite: 104, 105]. It represents the status quo or the assumption you're trying to challenge[cite: 105]. If one hypothesis is very specific (e.g., the mean is exactly 5), it's usually the null hypothesis[cite: 106].\n",
        "      * *Example*: $H\\_0$: The average response time of a website is 2 seconds.\n",
        "2.  **Alternative Hypothesis ($H\\_a$ or $H\\_1$)**: This statement contradicts the null hypothesis[cite: 107]. It suggests there *is* an effect, a difference, or that the parameter is not equal to the baseline value (it could be greater than, less than, or simply not equal to)[cite: 108]. If one hypothesis is less specific (e.g., the mean is greater than 5 or not equal to 5), it's usually the alternative[cite: 108].\n",
        "      * *Example*: $H\\_a$: The average response time of a website is greater than 2 seconds.\n",
        "\n",
        "Based on sample data, a **test statistic** is calculated. This is a value derived from the sample that summarizes the evidence against the null hypothesis[cite: 112]. The procedure then determines whether to \"reject the null hypothesis\" in favor of the alternative or \"fail to reject the null hypothesis\"[cite: 109]. It's important to note that failing to reject $H\\_0$ doesn't mean $H\\_0$ is definitively true; it just means there wasn't enough evidence in *this particular sample* to reject it.\n",
        "\n",
        "**Frequentist vs. Bayesian Hypothesis Testing:**\n",
        "\n",
        "  * **Frequentist Approach**: This often involves setting a **decision boundary** based on a chosen **significance level** (alpha, $\\\\alpha$)[cite: 111]. If the test statistic falls into a \"rejection region\" (an area of values unlikely if $H\\_0$ is true), $H\\_0$ is rejected[cite: 112, 113]. The **null distribution** (the distribution of the test statistic assuming $H\\_0$ is true) is key here[cite: 115].\n",
        "  * **Bayesian Approach**: Instead of a strict decision boundary, this approach calculates the **posterior probabilities** of both the null and alternative hypotheses given the data[cite: 117]. It involves using prior probabilities for each hypothesis and updating them with the likelihood of the data under each hypothesis (often using a likelihood ratio)[cite: 118]. The hypothesis with the higher posterior probability is considered more likely[cite: 117].\n",
        "\n",
        "#### Example: A/B Testing a New Website Button\n",
        "\n",
        "Imagine a company wants to test if a new button color (\"New Green\") on their website leads to more clicks than the current button color (\"Old Blue\").\n",
        "\n",
        "  * **Null Hypothesis ($H\\_0$)**: The click-through rate (CTR) for the New Green button is the same as the CTR for the Old Blue button. (No effect)\n",
        "  * **Alternative Hypothesis ($H\\_a$)**: The CTR for the New Green button is different from (or specifically, higher than) the CTR for the Old Blue button. (There is an effect)\n",
        "\n",
        "They would then collect data (show different versions to different users), calculate a test statistic (e.g., based on the difference in CTRs), and make a decision.\n",
        "\n",
        "#### Example: Coin Toss (Bayesian, from source)\n",
        "\n",
        "The source provides a Bayesian hypothesis testing example with two coins: Coin 1 (e.g., 70% heads) and Coin 2 (e.g., 50% heads)[cite: 120].\n",
        "\n",
        "  * **Hypotheses**: $H\\_1$: The chosen coin is Coin 1. $H\\_2$: The chosen coin is Coin 2[cite: 123, 124].\n",
        "  * **Prior Probabilities**: Your initial belief about which coin was picked (e.g., 50/50 if chosen randomly)[cite: 126].\n",
        "  * **Data**: Observe, say, 3 heads in 10 tosses[cite: 128].\n",
        "  * **Likelihoods**: Calculate the probability of getting 3 heads given Coin 1, and given Coin 2 (using the binomial distribution)[cite: 129].\n",
        "  * **Likelihood Ratio**: The ratio of these likelihoods tells you how much more likely the data is under one hypothesis compared to the other[cite: 131].\n",
        "  * **Posterior Probabilities**: Using Bayes' Rule, combine the priors and the likelihood ratio to get updated probabilities for $H\\_1$ and $H\\_2$ given the data[cite: 135, 136]. If the posterior probability for $H\\_1$ is much higher, you'd favor the conclusion that it was Coin 1.\n",
        "\n",
        "#### Interactive Python Code: Simple Frequentist Z-test for Mean\n",
        "\n",
        "This code performs a one-sample Z-test for the mean when the population standard deviation is known. This is a frequentist test."
      ],
      "metadata": {
        "id": "cflPJLTrMp7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def simple_z_test():\n",
        "    print(\"--- One-Sample Z-test for the Mean (Population Std Dev Known) ---\")\n",
        "    print(\"This test helps decide if a sample mean is significantly different from a hypothesized population mean.\")\n",
        "\n",
        "    try:\n",
        "        hypothesized_mean_str = input(\"Enter the hypothesized population mean (H0 value, e.g., 100): \")\n",
        "        population_std_dev_str = input(\"Enter the known population standard deviation (sigma > 0, e.g., 15): \")\n",
        "        sample_mean_str = input(\"Enter the observed sample mean (e.g., 105): \")\n",
        "        sample_size_str = input(\"Enter the sample size (n > 0, e.g., 30): \")\n",
        "        alpha_str = input(\"Enter the significance level (alpha, e.g., 0.05): \")\n",
        "\n",
        "        h0_mean = float(hypothesized_mean_str)\n",
        "        pop_std = float(population_std_dev_str)\n",
        "        sample_mean = float(sample_mean_str)\n",
        "        n = int(sample_size_str)\n",
        "        alpha = float(alpha_str)\n",
        "\n",
        "        if pop_std <= 0 or n <= 0:\n",
        "            print(\"Population standard deviation and sample size must be positive.\")\n",
        "            return\n",
        "        if not (0 < alpha < 1):\n",
        "            print(\"Alpha must be between 0 and 1 (exclusive).\")\n",
        "            return\n",
        "\n",
        "        # Calculate the Z-statistic\n",
        "        z_statistic = (sample_mean - h0_mean) / (pop_std / np.sqrt(n))\n",
        "\n",
        "        # Calculate the P-value (two-tailed test for this example: Ha: mean != h0_mean)\n",
        "        # Probability of observing a Z-statistic as extreme or more extreme than calculated\n",
        "        p_value = 2 * (1 - norm.cdf(abs(z_statistic))) # Multiply by 2 for two-tailed\n",
        "\n",
        "        print(f\"\\n--- Results ---\")\n",
        "        print(f\"Null Hypothesis (H0): Population mean = {h0_mean}\")\n",
        "        print(f\"Alternative Hypothesis (Ha): Population mean != {h0_mean} (two-tailed)\")\n",
        "        print(f\"Z-statistic: {z_statistic:.4f}\")\n",
        "        print(f\"P-value: {p_value:.4f}\")\n",
        "        print(f\"Significance level (alpha): {alpha}\")\n",
        "\n",
        "        if p_value < alpha:\n",
        "            print(\"\\nDecision: Reject the Null Hypothesis (H0).\")\n",
        "            print(f\"Reasoning: The P-value ({p_value:.4f}) is less than alpha ({alpha}).\")\n",
        "            print(f\"This suggests that the observed sample mean ({sample_mean}) is statistically significantly different from the hypothesized mean ({h0_mean}).\")\n",
        "        else:\n",
        "            print(\"\\nDecision: Fail to reject the Null Hypothesis (H0).\")\n",
        "            print(f\"Reasoning: The P-value ({p_value:.4f}) is greater than or equal to alpha ({alpha}).\")\n",
        "            print(\"There is not enough statistical evidence to conclude that the population mean is different from the hypothesized mean.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please ensure all inputs are numbers.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the Z-test example\n",
        "simple_z_test()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QIB5SghdMp7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You input parameters for your test: the hypothesized population mean ($H\\_0$), the known population standard deviation, your sample mean, sample size, and significance level ($\\\\alpha$).\n",
        "2.  The code calculates the **Z-statistic**, which measures how many standard errors your sample mean is away from the hypothesized population mean.\n",
        "3.  It then calculates the **P-value**. For a two-tailed test, this is the probability of observing a Z-statistic as extreme as (or more extreme than) the one calculated, in either direction (positive or negative), assuming the null hypothesis is true.\n",
        "4.  Finally, it compares the P-value to your chosen $\\\\alpha$:\n",
        "      * If P-value \\< $\\\\alpha$: You reject $H\\_0$. The result is \"statistically significant,\" meaning it's unlikely to have occurred by chance if $H\\_0$ were true.\n",
        "      * If P-value $\\\\ge \\\\alpha$: You fail to reject $H\\_0$. The result is \"not statistically significant.\"\n",
        "\n",
        "-----\n",
        "\n",
        "### 8\\. Type I and Type II Errors ⚠️\n",
        "\n",
        "In frequentist hypothesis testing (specifically, the Neyman-Pearson paradigm), when you make a decision to reject or not reject the null hypothesis, you can make two types of mistakes[cite: 140]:\n",
        "\n",
        "1.  **Type I Error ($\\\\alpha$)**:\n",
        "\n",
        "      * **Definition**: This occurs when you **incorrectly reject a true null hypothesis**[cite: 140, 141]. So, $H\\_0$ was actually true, but your sample data led you to conclude it was false[cite: 141].\n",
        "      * **Probability**: The probability of making a Type I error is denoted by $\\\\alpha$, which is the **significance level** you choose for your test[cite: 142]. If you set $\\\\alpha = 0.05$, you're accepting a 5% chance of making a Type I error if $H\\_0$ is true.\n",
        "      * **Analogy**: A \"false alarm\" or convicting an innocent person.\n",
        "\n",
        "2.  **Type II Error ($\\\\beta$)**:\n",
        "\n",
        "      * **Definition**: This occurs when you **incorrectly fail to reject a false null hypothesis**[cite: 143]. So, $H\\_0$ was false (and the alternative hypothesis $H\\_a$ was true), but your sample data wasn't strong enough to convince you to reject $H\\_0$[cite: 144].\n",
        "      * **Probability**: The probability of making a Type II error is denoted by $\\\\beta$.\n",
        "      * **Analogy**: A \"missed detection\" or letting a guilty person go free.\n",
        "\n",
        "**Power of a Test**:\n",
        "\n",
        "  * The **power** of a hypothesis test is the probability that you **correctly reject a false null hypothesis**[cite: 145].\n",
        "  * Power is equal to **$1 - \\\\beta$** (one minus the probability of a Type II error)[cite: 146].\n",
        "  * Ideally, you want high power (e.g., 0.80 or 80%), meaning a good chance of detecting an effect if it truly exists[cite: 147].\n",
        "\n",
        "**Trade-off**:\n",
        "There's an inherent **trade-off between Type I and Type II errors**[cite: 148].\n",
        "\n",
        "  * If you make your criterion for rejecting $H\\_0$ very strict (e.g., lower $\\\\alpha$ from 0.05 to 0.01), you reduce the chance of a Type I error[cite: 149]. But, this also makes it harder to reject $H\\_0$ overall, so you increase the chance of a Type II error (reduce power)[cite: 149].\n",
        "  * Conversely, if you make it easier to reject $H\\_0$ (e.g., increase $\\\\alpha$), you increase the chance of a Type I error but decrease the chance of a Type II error (increase power)[cite: 150].\n",
        "\n",
        "#### Example: Customer Churn (from source)\n",
        "\n",
        "Consider a hypothesis about customer tenure:\n",
        "\n",
        "  * $H\\_0$: Customer churn after two years is just due to chance (tenure has no protective effect).\n",
        "\n",
        "  * $H\\_a$: Customers who stay longer than two years are less likely to churn (tenure has a protective effect)[cite: 153, 154].\n",
        "\n",
        "  * **Type I Error**: Concluding that longer tenure reduces churn ($H\\_a$ is accepted) when, in reality, tenure has no effect ($H\\_0$ is true)[cite: 155]. The company might waste resources on retention strategies based on tenure that don't actually work.\n",
        "\n",
        "  * **Type II Error**: Concluding that tenure has no effect on churn ($H\\_0$ is not rejected) when, in reality, longer tenure *does* reduce churn ($H\\_a$ is true)[cite: 156, 157]. The company might miss an opportunity to implement effective tenure-based retention strategies.\n",
        "\n",
        "Understanding these errors is crucial for setting an appropriate significance level based on the real-world consequences of each type of error in your specific context[cite: 158].\n",
        "\n",
        "#### Interactive Python Code: Visualizing Type I Error Rate (Alpha)\n",
        "\n",
        "This simulation repeatedly samples from a known population (where H0 is true) and performs a test to see how often we (incorrectly) reject H0."
      ],
      "metadata": {
        "id": "qa33V9FKMp7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def simulate_type_i_error():\n",
        "    print(\"--- Simulation of Type I Error Rate (Alpha) ---\")\n",
        "    print(\"We will simulate many hypothesis tests where H0 is TRUE,\")\n",
        "    print(\"and count how often we incorrectly reject H0.\")\n",
        "\n",
        "    try:\n",
        "        h0_mean_population = float(input(\"Enter the true population mean (this will be our H0, e.g., 50): \"))\n",
        "        population_std_dev = float(input(\"Enter the true population standard deviation (sigma > 0, e.g., 10): \"))\n",
        "        sample_size = int(input(\"Enter the sample size for each test (n > 0, e.g., 25): \"))\n",
        "        alpha_level = float(input(\"Enter the chosen significance level (alpha, e.g., 0.05): \"))\n",
        "        num_simulations = int(input(\"Enter the number of simulations to run (e.g., 1000): \"))\n",
        "\n",
        "        if population_std_dev <= 0 or sample_size <= 0 or num_simulations <=0:\n",
        "            print(\"Std dev, sample size, and simulations must be positive.\")\n",
        "            return\n",
        "        if not (0 < alpha_level < 1):\n",
        "            print(\"Alpha must be between 0 and 1 (exclusive).\")\n",
        "            return\n",
        "\n",
        "        type_i_errors = 0\n",
        "        for _ in range(num_simulations):\n",
        "            # Generate a sample from the population where H0 is true\n",
        "            sample = np.random.normal(loc=h0_mean_population, scale=population_std_dev, size=sample_size)\n",
        "            sample_mean = np.mean(sample)\n",
        "\n",
        "            # Perform a Z-test (H0: mean = h0_mean_population)\n",
        "            z_statistic = (sample_mean - h0_mean_population) / (population_std_dev / np.sqrt(sample_size))\n",
        "            p_value = 2 * (1 - norm.cdf(abs(z_statistic))) # Two-tailed\n",
        "\n",
        "            if p_value < alpha_level:\n",
        "                type_i_errors += 1\n",
        "\n",
        "        observed_type_i_rate = type_i_errors / num_simulations\n",
        "\n",
        "        print(f\"\\n--- Simulation Results ---\")\n",
        "        print(f\"Number of tests simulated: {num_simulations}\")\n",
        "        print(f\"Chosen alpha (desired Type I error rate): {alpha_level:.3f}\")\n",
        "        print(f\"Number of times H0 was incorrectly rejected: {type_i_errors}\")\n",
        "        print(f\"Observed Type I error rate in simulation: {observed_type_i_rate:.3f}\")\n",
        "\n",
        "        print(\"\\nReasoning:\")\n",
        "        print(\"When the null hypothesis is true, we expect to incorrectly reject it (commit a Type I error)\")\n",
        "        print(f\"approximately alpha ({alpha_level*100}%) of the time over many repeated tests.\")\n",
        "        print(\"The observed Type I error rate from the simulation should be close to your chosen alpha.\")\n",
        "        print(\"If you run more simulations, this observed rate will typically get closer to alpha.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the simulation\n",
        "simulate_type_i_error()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8u0PBr1lMp7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You define a population where the null hypothesis is known to be true (e.g., true mean is 50, and $H\\_0$ states mean is 50). You also set the significance level ($\\\\alpha$).\n",
        "2.  The code simulates performing many hypothesis tests (e.g., 1000). In each test:\n",
        "      * It draws a random sample from this \"true $H\\_0$\" population.\n",
        "      * It conducts a Z-test to see if the sample mean is significantly different from the $H\\_0$ mean.\n",
        "3.  It counts how many times the test (incorrectly) rejects the true $H\\_0$.\n",
        "    The output shows the proportion of tests that resulted in a Type I error. This proportion should be close to the $\\\\alpha$ level you initially set, illustrating that $\\\\alpha$ is indeed the long-run probability of making a Type I error when $H\\_0$ is true.\n",
        "\n",
        "-----\n",
        "\n",
        "### 9\\. Business Examples of Hypothesis Testing 📈\n",
        "\n",
        "Hypothesis testing is widely applicable in business decision-making[cite: 159]:\n",
        "\n",
        "1.  **Marketing Intervention**:\n",
        "      * **Scenario**: Testing if a new direct mail campaign impacts customer purchasing behavior[cite: 159].\n",
        "      * **$H\\_0$**: The campaign has no impact on purchasing (e.g., average purchase amount remains the same)[cite: 160].\n",
        "      * **$H\\_a$**: The campaign does have an impact on purchasing (e.g., average purchase amount increases)[cite: 160].\n",
        "2.  **Website Layout Change**:\n",
        "      * **Scenario**: Testing whether a change in website layout affects user engagement (e.g., time spent on site, bounce rate, or conversion rate)[cite: 161]. This is a classic A/B test.\n",
        "      * **$H\\_0$**: The layout change had no impact on user engagement[cite: 162].\n",
        "      * **$H\\_a$**: The layout change does have an impact on user engagement[cite: 162].\n",
        "3.  **Product Quality/Manufacturing**:\n",
        "      * **Scenario**: Testing if a product meets a specific quality threshold, like an expected size or weight[cite: 163].\n",
        "      * **$H\\_0$**: The product's average size is not significantly different from the expected size S[cite: 164].\n",
        "      * **$H\\_a$**: There is a significant deviation in the product's average size from S[cite: 165].\n",
        "4.  **Pricing Strategy**:\n",
        "      * **Scenario**: Testing if a new pricing model affects sales volume or revenue.\n",
        "      * **$H\\_0$**: The new pricing model does not change the average daily sales volume.\n",
        "      * **$H\\_a$**: The new pricing model does change the average daily sales volume.\n",
        "5.  **Employee Training Program**:\n",
        "      * **Scenario**: Testing if a new training program improves employee performance metrics.\n",
        "      * **$H\\_0$**: The new training program does not change the average employee performance score.\n",
        "      * **$H\\_a$**: The new training program increases the average employee performance score.\n",
        "\n",
        "In each case, data would be collected (often through controlled experiments), a test statistic calculated, and compared to a critical value or p-value to decide whether the evidence is strong enough to reject the null hypothesis[cite: 109, 112, 174].\n",
        "\n",
        "-----\n",
        "\n",
        "### 10\\. Significance Level and P-Values 🎯\n",
        "\n",
        "#### Significance Level ($\\\\alpha$)\n",
        "\n",
        "  * In frequentist hypothesis testing, the **significance level**, denoted by $\\\\alpha$, is a probability threshold **chosen *before* testing the data**[cite: 166].\n",
        "  * It represents the **maximum probability of making a Type I error** (incorrectly rejecting a true null hypothesis) that you are willing to tolerate[cite: 142, 167].\n",
        "  * It defines how unlikely the observed data must be under the null hypothesis to justify rejecting $H\\_0$[cite: 167]. A lower $\\\\alpha$ (e.g., 0.01 instead of 0.05) means you require stronger evidence (more extreme data) to reject $H\\_0$, thus reducing the chance of a Type I error[cite: 168].\n",
        "  * Common values for $\\\\alpha$ are 0.10 (10%), 0.05 (5%), and 0.01 (1%)[cite: 169]. The choice depends on the context and the consequences of making a Type I error[cite: 169]. For example, testing for dangerous side effects of a new medication might warrant a very low $\\\\alpha$ (e.g., 0.001) because a Type I error (concluding the drug is safe when it has dangerous side effects) is very costly[cite: 170].\n",
        "  * Choosing $\\\\alpha$ *after* seeing the data or the P-value is called **P-hacking** and is considered poor scientific practice as it can lead to biased conclusions[cite: 171, 172].\n",
        "\n",
        "#### P-Value\n",
        "\n",
        "  * The **P-value** (or probability value) is the probability, assuming the null hypothesis ($H\\_0$) is true, of observing a test statistic **as extreme as, or more extreme than**, the one actually calculated from your sample data[cite: 172].\n",
        "  * It can also be interpreted as the **smallest significance level ($\\\\alpha$) at which you would reject the null hypothesis** for the given data[cite: 173].\n",
        "\n",
        "**Decision Rule using P-value:**\n",
        "\n",
        "1.  Choose your significance level $\\\\alpha$ (e.g., 0.05) *before* looking at the data.\n",
        "2.  Calculate the P-value from your data.\n",
        "3.  Compare:\n",
        "      * **If P-value $\\\\le \\\\alpha$**: You **reject the null hypothesis ($H\\_0$)**[cite: 174]. The observed data is considered \"statistically significant\" at the $\\\\alpha$ level[cite: 175]. This means the result is unlikely to have occurred by random chance if $H\\_0$ were true[cite: 175].\n",
        "      * **If P-value $ \\> \\\\alpha$**: You **fail to reject the null hypothesis ($H\\_0$)**[cite: 176]. The observed data is \"not statistically significant\" at the $\\\\alpha$ level[cite: 177]. This means the result is reasonably likely to have occurred by chance even if $H\\_0$ were true.\n",
        "\n",
        "**P-values and Confidence Intervals:**\n",
        "There's a direct link between P-values and confidence intervals. A confidence interval provides a range of plausible values for the population parameter. If a (1-$\\\\alpha$)% confidence interval for a parameter *does not* contain the value specified by the null hypothesis, then a hypothesis test with significance level $\\\\alpha$ would reject $H\\_0$. Conversely, if the null hypothesis value *falls within* the confidence interval, you would fail to reject $H\\_0$[cite: 178, 179]. For a normal null distribution, a P-value of 0.05 roughly corresponds to the test statistic being about two standard deviations away from the mean under the null[cite: 180].\n",
        "\n",
        "#### Example: Coin Toss (Frequentist, from source) [cite: 181]\n",
        "\n",
        "  * **Hypotheses**: $H\\_0$: The coin is fair ($P(Heads)=0.5$). $H\\_a$: The coin is unfair, biased towards tails ($P(Heads)\\<0.5$) (a one-sided test)[cite: 181, 182].\n",
        "  * **Data**: Observe 3 heads in 10 flips[cite: 182].\n",
        "  * **Null Distribution**: Under $H\\_0$, the number of heads in 10 flips follows a Binomial distribution with $n=10, p=0.5$[cite: 183].\n",
        "  * **Significance Level**: Choose $\\\\alpha = 0.05$ beforehand[cite: 184].\n",
        "  * **Test Statistic**: The number of heads observed = 3[cite: 185].\n",
        "  * **P-value Calculation**: Probability of getting 3 heads or *fewer* (because $H\\_a$ is $P(H)\\<0.5$) if the coin is fair. This is $P(X \\\\le 3 | n=10, p=0.5)$. The source states this is 0.171 (or 17.1%).\n",
        "  * **Decision**: Compare P-value (0.171) to $\\\\alpha$ (0.05). Since $0.171 \\> 0.05$, you **fail to reject $H\\_0$**[cite: 185].\n",
        "  * **Conclusion**: The observed result of 3 heads in 10 flips is not extreme enough at the 5% significance level to conclude the coin is unfair and biased towards tails[cite: 186]. It could reasonably happen by chance with a fair coin[cite: 187].\n",
        "\n",
        "#### Interactive Python Code: P-value interpretation\n",
        "\n",
        "This code takes a P-value and alpha, then makes a decision."
      ],
      "metadata": {
        "id": "ExPcBOzvMp7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_p_value():\n",
        "    print(\"--- P-value Interpretation Tool ---\")\n",
        "    try:\n",
        "        p_value_str = input(\"Enter the calculated P-value (e.g., 0.03 or 0.21): \")\n",
        "        alpha_str = input(\"Enter your chosen significance level (alpha, e.g., 0.05): \")\n",
        "\n",
        "        p_value = float(p_value_str)\n",
        "        alpha = float(alpha_str)\n",
        "\n",
        "        if not (0 <= p_value <= 1): # P-value can be 0 or 1 in some cases\n",
        "            print(\"P-value must be between 0 and 1.\")\n",
        "            return\n",
        "        if not (0 < alpha < 1):\n",
        "            print(\"Alpha must be between 0 and 1 (exclusive).\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n--- Decision ---\")\n",
        "        print(f\"Calculated P-value: {p_value:.4f}\")\n",
        "        print(f\"Chosen significance level (alpha): {alpha:.4f}\")\n",
        "\n",
        "        if p_value < alpha: # Strictly less than for common convention\n",
        "            print(\"\\nOutcome: Reject the Null Hypothesis (H0).\")\n",
        "            print(f\"Reasoning: The P-value ({p_value:.4f}) is LESS THAN alpha ({alpha:.4f}).\")\n",
        "            print(\"This means the observed data is statistically significant at this alpha level.\")\n",
        "            print(\"It is unlikely to have occurred by random chance if the Null Hypothesis were true.\")\n",
        "        elif p_value == alpha:\n",
        "             print(\"\\nOutcome: Reject the Null Hypothesis (H0) (P-value equals alpha).\")\n",
        "             print(f\"Reasoning: The P-value ({p_value:.4f}) is EQUAL TO alpha ({alpha:.4f}).\")\n",
        "             print(\"This is the boundary case. By convention (P <= alpha), H0 is rejected.\")\n",
        "             print(\"The observed data is considered statistically significant.\")\n",
        "        else: # P-value > alpha\n",
        "            print(\"\\nOutcome: Fail to reject the Null Hypothesis (H0).\")\n",
        "            print(f\"Reasoning: The P-value ({p_value:.4f}) is GREATER THAN alpha ({alpha:.4f}).\")\n",
        "            print(\"This means the observed data is NOT statistically significant at this alpha level.\")\n",
        "            print(\"The result is reasonably likely to occur by random chance even if the Null Hypothesis were true.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers for P-value and alpha.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the P-value interpretation tool\n",
        "interpret_p_value()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "naZKt3_aMp7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You input a P-value (that would have been calculated from a statistical test) and your pre-chosen significance level ($\\\\alpha$).\n",
        "2.  The code compares them and tells you whether to \"Reject the Null Hypothesis\" or \"Fail to reject the Null Hypothesis\" based on the standard rule (P-value $\\\\le \\\\alpha$ leads to rejection).\n",
        "3.  It provides a brief reasoning for the decision, explaining what the comparison implies about the statistical significance of your (hypothetical) data.\n",
        "\n",
        "-----\n",
        "\n",
        "**(Continued in next response due to length limitations)**"
      ],
      "metadata": {
        "id": "JMmNt-D0Mp7L"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
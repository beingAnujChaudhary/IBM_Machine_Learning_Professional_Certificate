{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's proceed with the next set of topics.\n",
        "\n",
        "-----\n",
        "\n",
        "### 11\\. Power and Sample Size üí™ N\n",
        "\n",
        "#### Explanation\n",
        "\n",
        "The **power** of a statistical test is its ability to correctly detect a true effect. More formally, it's the probability of **correctly rejecting a false null hypothesis ($H\\_0$)** when a specific alternative hypothesis ($H\\_a$) is true[cite: 145]. Power is equal to $1 - \\\\beta$, where $\\\\beta$ is the probability of a Type II error (failing to detect an effect when one exists)[cite: 146].\n",
        "\n",
        "**Factors Influencing Power:**\n",
        "\n",
        "  * **Effect Size**: The larger the true difference between the null hypothesis and the actual state of affairs (the \"effect size\"), the easier it is to detect, and thus the higher the power. A small, subtle effect requires more power to detect.\n",
        "  * **Sample Size (N)**: Increasing the sample size generally increases the power of a test[cite: 189]. With more data, you get more precise estimates, making it easier to distinguish a true effect from random variation[cite: 190]. Larger sample sizes reduce the overlap between the null distribution and the distribution under the alternative hypothesis[cite: 190].\n",
        "  * **Significance Level ($\\\\alpha$)**: A higher significance level (e.g., $\\\\alpha=0.10$ instead of $\\\\alpha=0.05$) makes it easier to reject the null hypothesis. This increases power but also increases the probability of a Type I error.\n",
        "  * **Variance of the Data**: Lower variance in the data (less \"noise\") leads to higher power because effects are easier to discern.\n",
        "  * **One-tailed vs. Two-tailed Test**: A one-tailed test (where you specify the direction of the effect, e.g., \"mean is *greater than* X\") generally has more power to detect an effect in that specific direction than a two-tailed test (e.g., \"mean is *not equal to* X\"), assuming the effect is indeed in that direction.\n",
        "\n",
        "**Importance of Power Analysis:**\n",
        "Power analysis is often done *before* conducting a study or experiment. It helps determine:\n",
        "\n",
        "  * The **sample size needed** to achieve a desired level of power (e.g., 80%) to detect an expected effect size. This helps avoid wasting resources on underpowered studies (too small a sample to find anything) or overpowered studies (unnecessarily large samples).\n",
        "  * The **power of a test** given a fixed sample size and effect size.\n",
        "  * The **minimum effect size** that can be detected with a given sample size and power.\n",
        "\n",
        "An underpowered study might conclude there's no effect when one actually exists (Type II error), simply because the sample was too small to detect it.\n",
        "\n",
        "#### Example: Testing a New Drug\n",
        "\n",
        "Suppose a pharmaceutical company is testing a new drug to reduce blood pressure.\n",
        "\n",
        "  * **$H\\_0$**: The new drug has no effect on blood pressure.\n",
        "  * **$H\\_a$**: The new drug reduces blood pressure.\n",
        "  * **Effect Size**: The actual average reduction in blood pressure caused by the drug (e.g., 5 mmHg, 10 mmHg).\n",
        "  * **Power**: The probability that the clinical trial will correctly conclude the drug is effective, if it truly reduces blood pressure by a certain amount.\n",
        "\n",
        "If the study is underpowered (e.g., too few participants), they might fail to detect a real, beneficial effect of the drug, leading to a Type II error and potentially abandoning a useful medication. A power analysis beforehand would help determine the number of participants needed to have a good chance (e.g., 80% power) of detecting a clinically meaningful reduction in blood pressure.\n",
        "\n",
        "#### Interactive Python Code: Power and Sample Size (Conceptual for Z-test)\n",
        "\n",
        "This code demonstrates how power changes with sample size and effect size for a one-sample Z-test (conceptually). We'll fix alpha and standard deviation."
      ],
      "metadata": {
        "id": "jPBh5StHNAaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def power_simulation_ztest():\n",
        "    print(\"--- Conceptual Power Simulation for a One-Sample Z-test (One-Tailed) ---\")\n",
        "    print(\"Investigate how power changes with sample size and true effect size.\")\n",
        "\n",
        "    try:\n",
        "        h0_mean = float(input(\"Enter the hypothesized population mean under H0 (e.g., 100): \"))\n",
        "        population_std_dev = float(input(\"Enter the population standard deviation (sigma > 0, e.g., 20): \"))\n",
        "        alpha = float(input(\"Enter the significance level (alpha, e.g., 0.05): \"))\n",
        "\n",
        "        if population_std_dev <= 0 or not (0 < alpha < 1) :\n",
        "            print(\"Sigma must be positive, and alpha between 0 and 1.\")\n",
        "            return\n",
        "\n",
        "        # Calculate the critical Z-value for H0 rejection (one-tailed, upper tail Ha: mu > h0_mean)\n",
        "        critical_z = norm.ppf(1 - alpha)\n",
        "        print(f\"Critical Z-value (one-tailed, upper): {critical_z:.3f}\")\n",
        "        print(f\"To reject H0, sample mean must lead to Z > {critical_z:.3f}\")\n",
        "\n",
        "        sample_sizes_input = input(\"Enter a few sample sizes (comma-separated, e.g., 20,50,100,200): \")\n",
        "        sample_sizes = [int(s.strip()) for s in sample_sizes_input.split(',')]\n",
        "\n",
        "        true_effect_sizes_input = input(\"Enter a few true differences from H0 mean (effect sizes, comma-separated, e.g., 2,5,10): \")\n",
        "        true_effect_sizes = [float(e.strip()) for e in true_effect_sizes_input.split(',')]\n",
        "\n",
        "        plt.figure(figsize=(12, 7))\n",
        "\n",
        "        for es_idx, effect_size in enumerate(true_effect_sizes):\n",
        "            true_mean_ha = h0_mean + effect_size # True mean under Ha\n",
        "            powers = []\n",
        "            for n in sample_sizes:\n",
        "                if n <= 0:\n",
        "                    powers.append(float('nan')) # Invalid sample size\n",
        "                    continue\n",
        "                # Standard error of the mean\n",
        "                sem = population_std_dev / np.sqrt(n)\n",
        "                # Critical sample mean value to reject H0\n",
        "                critical_sample_mean = h0_mean + (critical_z * sem)\n",
        "\n",
        "                # Now, find the probability of observing a sample mean >= critical_sample_mean\n",
        "                # IF the true mean is actually true_mean_ha (i.e., Ha is true)\n",
        "                # This is 1 - CDF of (critical_sample_mean) under the Ha distribution\n",
        "                z_for_power = (critical_sample_mean - true_mean_ha) / sem\n",
        "                power = 1 - norm.cdf(z_for_power)\n",
        "                powers.append(power)\n",
        "            plt.plot(sample_sizes, powers, marker='o', linestyle='-', label=f'Effect Size = {effect_size}')\n",
        "\n",
        "        plt.title(f'Power vs. Sample Size for Different Effect Sizes (Z-test, alpha={alpha})')\n",
        "        plt.xlabel('Sample Size (N)')\n",
        "        plt.ylabel('Power (1 - Beta)')\n",
        "        plt.ylim(0, 1.05)\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nReasoning:\")\n",
        "        print(\"The plot shows power curves. For a given effect size:\")\n",
        "        print(\" - As Sample Size (N) increases, power increases.\")\n",
        "        print(\"For a given sample size:\")\n",
        "        print(\" - As True Effect Size (difference from H0) increases, power increases.\")\n",
        "        print(\"Higher power means a better chance of detecting a true effect if it exists.\")\n",
        "        print(\"This illustrates why planning sample size is crucial for research.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the power simulation\n",
        "power_simulation_ztest()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "LISz48pINAai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You input parameters for a one-tailed Z-test: the null hypothesis mean, population standard deviation, and significance level ($\\\\alpha$).\n",
        "2.  You also provide a list of sample sizes and a list of \"true effect sizes\" (how different the actual population mean is from the null hypothesis mean).\n",
        "3.  The code calculates and plots the statistical power for each combination of sample size and effect size.\n",
        "    The resulting plot will show several curves. Each curve represents a specific effect size. You'll observe:\n",
        "\n",
        "<!-- end list -->\n",
        "\n",
        "  * For any given effect size, power increases as the sample size increases.\n",
        "  * For any given sample size, power increases as the effect size increases (it's easier to detect larger effects).\n",
        "    This demonstrates the interplay between sample size, effect size, and the ability of a test to detect a true effect.\n",
        "\n",
        "-----\n",
        "\n",
        "### 12\\. Multiple Comparisons and Bonferroni Correction üìäüìäüìä\n",
        "\n",
        "#### Explanation\n",
        "\n",
        "The problem of **multiple comparisons** (also known as multiple testing or the look-elsewhere effect) arises when you perform many hypothesis tests simultaneously on the same dataset.\n",
        "\n",
        "**The Issue**:\n",
        "If you conduct one hypothesis test at a significance level of $\\\\alpha = 0.05$, there's a 5% chance of making a Type I error (rejecting a true null hypothesis) if $H\\_0$ is true. However, if you perform many tests (e.g., 10 tests), the probability of making *at least one* Type I error across all these tests becomes much higher than 5%[cite: 192]. This is because each test has an independent chance of producing a false positive. The overall Type I error rate (often called the Family-Wise Error Rate or FWER) inflates. For example, with 10 tests at $\\\\alpha=0.05$, the probability of at least one Type I error is approximately $1 - (1-0.05)^{10} \\\\approx 0.40$, or roughly $\\\\alpha \\\\times \\\\text{number of tests}$ for a small number of tests[cite: 193].\n",
        "\n",
        "**Bonferroni Correction**:\n",
        "The **Bonferroni correction** is a common and simple method to control the FWER when performing multiple hypothesis tests[cite: 194].\n",
        "\n",
        "  * **Method**: It adjusts the significance level for each individual test to be much stricter[cite: 195]. If you want to maintain an overall FWER of $\\\\alpha\\_{\\\\text{desired}}$ (e.g., 0.05) across '$m$' number of tests, you set the significance level for each individual test to:\n",
        "    $\\\\alpha\\_{\\\\text{adjusted}} = \\\\alpha\\_{\\\\text{desired}} / m$[cite: 196].\n",
        "  * **Example**: If you're conducting $m=10$ tests and want an overall $\\\\alpha$ of 0.05, each individual test must use $\\\\alpha\\_{\\\\text{adjusted}} = 0.05 / 10 = 0.005$[cite: 197]. So, you would only reject the null hypothesis for an individual test if its P-value is less than or equal to 0.005.\n",
        "\n",
        "**Trade-off**:\n",
        "The main trade-off with the Bonferroni correction is that it significantly **reduces the power** of the individual tests[cite: 199]. By making the criterion for significance much stricter, you are less likely to detect a true effect (i.e., you increase the probability of Type II errors). To detect an effect after applying the correction, the effect needs to be larger, or the tests require larger sample sizes[cite: 200].\n",
        "\n",
        "**Best Practice**:\n",
        "While corrections like Bonferroni are useful, a general best practice is to **limit the number of comparisons** to a few well-motivated hypotheses rather than \"fishing\" for significant results by testing everything possible[cite: 201]. Pre-registering your hypotheses before data collection also helps. Other methods for controlling FWER or False Discovery Rate (FDR, e.g., Benjamini-Hochberg procedure) exist and can be more powerful than Bonferroni in certain situations, but Bonferroni is the simplest to understand and apply.\n",
        "\n",
        "#### Example: Gene Expression Analysis\n",
        "\n",
        "Imagine researchers are testing 1000 different genes to see if their expression levels differ between a treatment group and a control group.\n",
        "\n",
        "  * If they test each gene at $\\\\alpha = 0.05$, they might expect around $0.05 \\\\times 1000 = 50$ genes to show up as \"significant\" by chance alone, even if no genes were actually affected by the treatment (50 Type I errors).\n",
        "  * Using Bonferroni correction: They would set the significance level for each gene test to $\\\\alpha\\_{\\\\text{adjusted}} = 0.05 / 1000 = 0.00005$. A gene would only be considered significantly different if its P-value is less than 0.00005. This greatly reduces the chance of false positives but makes it harder to detect truly affected genes unless their effects are very strong or the sample size is very large.\n",
        "\n",
        "#### Interactive Python Code: Simulating Multiple Comparisons Problem\n",
        "\n",
        "This code simulates running many tests where H0 is true and shows how the chance of getting *at least one* false positive increases with the number of tests."
      ],
      "metadata": {
        "id": "UaSPT2wHNAaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def multiple_comparisons_simulation():\n",
        "    print(\"--- Simulation of the Multiple Comparisons Problem ---\")\n",
        "    print(\"We'll simulate multiple 'experiments', each with multiple 'tests'.\")\n",
        "    print(\"In all tests, H0 is true. We'll see how often we get at least one false positive per experiment.\")\n",
        "\n",
        "    try:\n",
        "        num_tests_per_experiment = int(input(\"Enter the number of tests per experiment (e.g., 10 or 20): \"))\n",
        "        alpha_per_test = float(input(\"Enter the alpha level for each individual test (e.g., 0.05): \"))\n",
        "        num_experiments = int(input(\"Enter the number of experiments to simulate (e.g., 1000): \"))\n",
        "\n",
        "        if num_tests_per_experiment <= 0 or num_experiments <=0:\n",
        "            print(\"Number of tests and experiments must be positive.\")\n",
        "            return\n",
        "        if not (0 < alpha_per_test < 1):\n",
        "            print(\"Alpha must be between 0 and 1.\")\n",
        "            return\n",
        "\n",
        "        experiments_with_at_least_one_fp = 0\n",
        "\n",
        "        for _ in range(num_experiments):\n",
        "            false_positives_in_this_experiment = 0\n",
        "            for _ in range(num_tests_per_experiment):\n",
        "                # Simulate a P-value under H0 (uniformly distributed between 0 and 1)\n",
        "                # A simpler way than generating data and testing:\n",
        "                # If H0 is true, P-values should be uniformly distributed.\n",
        "                p_value_simulated = np.random.uniform(0, 1)\n",
        "                if p_value_simulated < alpha_per_test:\n",
        "                    false_positives_in_this_experiment += 1\n",
        "\n",
        "            if false_positives_in_this_experiment > 0:\n",
        "                experiments_with_at_least_one_fp += 1\n",
        "\n",
        "        observed_fwer = experiments_with_at_least_one_fp / num_experiments\n",
        "        # Theoretical FWER (approx for small alpha, or exact $1 - (1-alpha)^m$)\n",
        "        theoretical_fwer = 1 - (1 - alpha_per_test)**num_tests_per_experiment\n",
        "\n",
        "        print(f\"\\n--- Simulation Results ---\")\n",
        "        print(f\"Number of tests per experiment: {num_tests_per_experiment}\")\n",
        "        print(f\"Alpha for each individual test: {alpha_per_test}\")\n",
        "        print(f\"Number of experiments simulated: {num_experiments}\")\n",
        "        print(f\"Number of experiments with at least one false positive (Type I error): {experiments_with_at_least_one_fp}\")\n",
        "        print(f\"Observed Family-Wise Error Rate (FWER): {observed_fwer:.4f}\")\n",
        "        print(f\"Theoretical FWER (approx.): {theoretical_fwer:.4f}\")\n",
        "\n",
        "        # With Bonferroni\n",
        "        bonferroni_alpha = alpha_per_test / num_tests_per_experiment\n",
        "        experiments_with_fp_bonferroni = 0\n",
        "        for _ in range(num_experiments):\n",
        "            fp_this_experiment_bonf = 0\n",
        "            for _ in range(num_tests_per_experiment):\n",
        "                p_value_simulated = np.random.uniform(0, 1)\n",
        "                if p_value_simulated < bonferroni_alpha:\n",
        "                    fp_this_experiment_bonf +=1\n",
        "            if fp_this_experiment_bonf > 0:\n",
        "                experiments_with_fp_bonferroni +=1\n",
        "        observed_fwer_bonferroni = experiments_with_fp_bonferroni / num_experiments\n",
        "\n",
        "        print(f\"\\nWith Bonferroni correction (individual alpha = {bonferroni_alpha:.6f}):\")\n",
        "        print(f\"Observed FWER with Bonferroni: {observed_fwer_bonferroni:.4f}\")\n",
        "\n",
        "\n",
        "        print(\"\\nReasoning:\")\n",
        "        print(\"When conducting multiple tests, the chance of getting at least one false positive (Type I error)\")\n",
        "        print(\"across all tests (the Family-Wise Error Rate) increases significantly beyond the per-test alpha.\")\n",
        "        print(\"The Bonferroni correction adjusts the alpha for individual tests to be much stricter,\")\n",
        "        print(\"thereby controlling this overall FWER to be closer to the desired level (e.g., your original per-test alpha).\")\n",
        "        print(\"Notice how the observed FWER with Bonferroni is much lower and closer to your initial per-test alpha target.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the simulation\n",
        "multiple_comparisons_simulation()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Hsvd9sQINAaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You specify the number of tests you'd run in one \"experiment\" (e.g., testing 20 different hypotheses), the $\\\\alpha$ level for each individual test (e.g., 0.05), and how many such \"experiments\" to simulate.\n",
        "2.  The simulation assumes the null hypothesis is true for *all* tests.\n",
        "3.  For each experiment, it checks if *any* of the individual tests result in a (false) positive by comparing a simulated P-value to `alpha_per_test`.\n",
        "4.  It calculates:\n",
        "      * The **Observed Family-Wise Error Rate (FWER)**: The proportion of experiments that had at least one false positive. This will typically be much higher than your `alpha_per_test`.\n",
        "      * The **Theoretical FWER**.\n",
        "      * It then recalculates the observed FWER if a Bonferroni correction were applied, showing how it effectively controls the FWER.\n",
        "        The output demonstrates that without correction, your chance of making at least one Type I error across many tests is high. With Bonferroni, this FWER is brought down, but at the cost of making each individual test more conservative.\n",
        "\n",
        "-----\n",
        "\n",
        "### 13\\. Correlation vs. Causation üîó‚â†‚û°Ô∏è\n",
        "\n",
        "This is one of the most critical concepts in data analysis and interpretation.\n",
        "\n",
        "#### Explanation\n",
        "\n",
        "**Correlation**:\n",
        "\n",
        "  * Correlation means that two variables (X and Y) tend to move together or are associated with each other[cite: 203]. If they are correlated, knowing the value of X can be useful for predicting the value of Y[cite: 204].\n",
        "  * Correlation is measured by a correlation coefficient (e.g., Pearson's r), which ranges from -1 to +1.\n",
        "      * \\+1: Perfect positive linear correlation (as X increases, Y increases).\n",
        "      * \\-1: Perfect negative linear correlation (as X increases, Y decreases).\n",
        "      * 0: No linear correlation.\n",
        "  * **Correlation is valuable for predictive modeling**[cite: 205]. If X predicts Y, you can use X in a model even if it doesn't cause Y.\n",
        "\n",
        "**Causation**:\n",
        "\n",
        "  * Causation means that a change in one variable (X) **directly causes** a change in another variable (Y)[cite: 208]. This implies a mechanism or process by which X influences Y.\n",
        "  * Understanding causation is crucial if you want to intervene or make changes to affect an outcome[cite: 208]. If X causes Y, then manipulating X should lead to a change in Y.\n",
        "\n",
        "**Correlation does NOT imply causation**[cite: 205].\n",
        "Just because two variables are correlated does not mean one causes the other[cite: 206]. Relying solely on correlation can be misleading when trying to understand cause-and-effect or when trying to manipulate X to change Y[cite: 207].\n",
        "\n",
        "**Why Variables Can Be Correlated Without Direct Causation:**\n",
        "\n",
        "1.  **X causes Y (True Causation)**: This is what we often hope to find (e.g., increased study time *causes* higher exam scores)[cite: 209].\n",
        "2.  **Y causes X (Reverse Causation)**: The direction of causality is the opposite of what might be assumed (e.g., people who are already sick (Y) are more likely to visit a doctor (X), rather than visiting a doctor causing sickness)[cite: 210].\n",
        "3.  **Confounding Variable (Common Cause)**: A third, unobserved variable (Z) influences both X and Y independently, creating a correlation between X and Y even though there's no direct causal link between them[cite: 211, 213]. This is a very common source of misleading correlations.\n",
        "      * *Example (from source)*: Ice cream sales (X) and drowning incidents (Y) are positively correlated. The confounding variable is temperature (Z); warmer weather leads to more ice cream sales AND more people swimming (increasing drowning risk)[cite: 217]. Selling less ice cream won't reduce drownings[cite: 218].\n",
        "      * *Example (from source)*: Number of factories a chip manufacturer owns (X) and number of chips sold (Y) are positively correlated. The confounding variable is market demand (Z); high demand leads to building more factories and also leads to more sales[cite: 220, 221].\n",
        "4.  **Coincidence (Spurious Correlation)**: The correlation observed in a particular dataset is purely coincidental and has no underlying mechanism[cite: 212]. These correlations are unlikely to hold in other samples or over time[cite: 223]. Confounding can be considered a type of spurious correlation[cite: 224].\n",
        "      * *Example (from source)*: Age of Miss America correlated with murders by steam/hot vapors[cite: 226]. This is clearly coincidental.\n",
        "      * *Example (from source)*: Worldwide non-commercial space launches correlated with sociology doctorates awarded[cite: 227].\n",
        "\n",
        "**Implication**: When making recommendations or taking action, it's vital to consider potential confounding variables and not assume causation from correlation alone[cite: 229]. Establishing causation often requires carefully designed experiments (e.g., randomized controlled trials) rather than just observational data.\n",
        "\n",
        "#### Example: Firefighters and Damage\n",
        "\n",
        "  * **Observation**: The number of firefighters at a fire (X) is positively correlated with the amount of damage caused by the fire (Y).\n",
        "  * **Incorrect Causal Conclusion**: Sending more firefighters causes more damage.\n",
        "  * **Actual Explanation (Confounding Variable)**: The size/severity of the fire (Z) is the confounding variable. Larger fires (Z) cause more damage (Y) AND require more firefighters to be sent (X).\n",
        "\n",
        "#### Interactive Python Code: Generating Correlated Data (with potential confounding)\n",
        "\n",
        "This code generates two variables that are correlated because they both depend on a third (confounding) variable."
      ],
      "metadata": {
        "id": "b8SN9ixPNAak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def correlation_confounding_example():\n",
        "    print(\"--- Simulating Correlated Data due to a Confounder ---\")\n",
        "\n",
        "    try:\n",
        "        num_points = int(input(\"Enter the number of data points to generate (e.g., 100): \"))\n",
        "        if num_points <= 0:\n",
        "            print(\"Number of points must be positive.\")\n",
        "            return\n",
        "\n",
        "        # Simulate a confounding variable Z (e.g., temperature, market demand)\n",
        "        # Let's say Z influences both X and Y\n",
        "        confounder_z = np.random.normal(50, 10, num_points)\n",
        "\n",
        "        # Variable X (e.g., ice cream sales) depends on Z + some noise\n",
        "        noise_x = np.random.normal(0, 5, num_points)\n",
        "        variable_x = 2 * confounder_z + 10 + noise_x # X increases with Z\n",
        "\n",
        "        # Variable Y (e.g., drownings) also depends on Z + some noise\n",
        "        noise_y = np.random.normal(0, 3, num_points)\n",
        "        variable_y = 0.5 * confounder_z + 5 + noise_y # Y also increases with Z\n",
        "\n",
        "        # Create a DataFrame\n",
        "        df = pd.DataFrame({'Confounder_Z': confounder_z, 'Variable_X': variable_x, 'Variable_Y': variable_y})\n",
        "\n",
        "        correlation_xy = df['Variable_X'].corr(df['Variable_Y'])\n",
        "        correlation_xz = df['Variable_X'].corr(df['Confounder_Z'])\n",
        "        correlation_yz = df['Variable_Y'].corr(df['Confounder_Z'])\n",
        "\n",
        "        print(f\"\\n--- Correlations ---\")\n",
        "        print(f\"Correlation between X and Y: {correlation_xy:.3f}\")\n",
        "        print(f\"Correlation between X and Confounder Z: {correlation_xz:.3f}\")\n",
        "        print(f\"Correlation between Y and Confounder Z: {correlation_yz:.3f}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.scatter(df['Variable_X'], df['Variable_Y'], alpha=0.6)\n",
        "        plt.title(f'Scatter Plot: Variable X vs. Variable Y\\nCorr = {correlation_xy:.2f}')\n",
        "        plt.xlabel('Variable X (e.g., Ice Cream Sales)')\n",
        "        plt.ylabel('Variable Y (e.g., Drownings)')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # Color points by the confounder to make it clearer\n",
        "        scatter = plt.scatter(df['Variable_X'], df['Variable_Y'], c=df['Confounder_Z'], cmap='viridis', alpha=0.7)\n",
        "        plt.title(f'X vs. Y (Colored by Confounder Z)')\n",
        "        plt.xlabel('Variable X')\n",
        "        plt.ylabel('Variable Y')\n",
        "        cbar = plt.colorbar(scatter)\n",
        "        cbar.set_label('Confounder Z Value (e.g., Temperature)')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nReasoning:\")\n",
        "        print(\"Variable X and Variable Y are correlated (see the first scatter plot and correlation coefficient).\")\n",
        "        print(\"However, in this simulation, neither X causes Y nor Y causes X directly.\")\n",
        "        print(\"Both X and Y were generated to depend on the 'Confounder_Z'.\")\n",
        "        print(\"This means Z is a common cause that makes X and Y appear related.\")\n",
        "        print(\"The second plot, where points are colored by Z, might help visualize how Z influences the X-Y relationship.\")\n",
        "        print(\"This demonstrates how a confounding variable can create a correlation, highlighting why correlation doesn't imply causation.\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a positive integer.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the simulation\n",
        "correlation_confounding_example()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Y0qmxa-FNAak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Code Output:**\n",
        "\n",
        "1.  You specify the number of data points to generate.\n",
        "2.  The code creates a \"confounder\" variable `Z`.\n",
        "3.  It then generates `Variable_X` and `Variable_Y` such that both are influenced by `Z` (plus some random noise). `X` and `Y` are not directly set to influence each other.\n",
        "4.  It calculates and prints the correlation between `X` and `Y`, which will likely be noticeable. It also shows correlations with `Z`.\n",
        "5.  It displays two scatter plots:\n",
        "      * The first shows `X` vs. `Y`, likely revealing a trend.\n",
        "      * The second shows the same `X` vs. `Y` but colors the points based on the value of the `Confounder_Z`. This can help visualize how `Z` might be driving the observed relationship between `X` and `Y`.\n",
        "        The output illustrates how a strong correlation between two variables (`X` and `Y`) can emerge simply because they are both driven by a third variable (`Z`), even if there's no direct causal link between `X` and `Y`.\n",
        "\n",
        "-----\n",
        "\n",
        "**(Continued in next response due to length limitations)**"
      ],
      "metadata": {
        "id": "-ZLaJSveNAal"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
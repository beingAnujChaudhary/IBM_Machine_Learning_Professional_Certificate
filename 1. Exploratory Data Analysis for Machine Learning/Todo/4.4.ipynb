{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's move on to the Python code examples from your source material and then the extensive Q\\&A section.\n",
        "\n",
        "-----\n",
        "\n",
        "### 14\\. Python Code Examples (Interactive Hypothesis Testing from Source) 💻\n",
        "\n",
        "The following Python code examples are based on the interactive coin flip hypothesis test scenario detailed in your provided source material[cite: 231]. These cells demonstrate setting up a hypothesis test, calculating a P-value, making a decision, simulating user interaction, and calculating critical values.\n",
        "\n",
        "#### Cell 1: Setting up the Coin Flip Scenario\n",
        "\n",
        "**Explanation:** This cell sets the context for the coin flip example discussed in the source[cite: 233]. It defines the null and alternative hypotheses, the experiment (10 flips), the observed data (3 heads), and the chosen significance level (alpha = 0.05)[cite: 234]."
      ],
      "metadata": {
        "id": "RMnFQxdoNeOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import binom\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def setup_coin_flip_scenario():\n",
        "    print(\"Let's simulate the coin flip hypothesis test.\")\n",
        "    print(\"Scenario: We are testing if a coin is fair (P(Heads)=0.5).\") # [cite: 232]\n",
        "    print(\"Null Hypothesis (H0): P(Heads) = 0.5\") # [cite: 232]\n",
        "    print(\"Alternative Hypothesis (Ha): P(Heads) < 0.5 (one-sided test)\") # [cite: 232]\n",
        "    print(\"We will flip the coin 10 times.\") # [cite: 232]\n",
        "    print(\"We choose a significance level (alpha) of 0.05.\") # [cite: 233]\n",
        "\n",
        "    # The data we observed in the source example: 3 heads in 10 flips [cite: 233]\n",
        "    observed_heads = 3\n",
        "    total_flips = 10\n",
        "    alpha = 0.05\n",
        "\n",
        "    print(f\"\\nWe observed {observed_heads} heads out of {total_flips} flips.\") # [cite: 233]\n",
        "    print(f\"Our chosen significance level (alpha) is {alpha}.\") # [cite: 233]\n",
        "    return observed_heads, total_flips, alpha\n",
        "\n",
        "# Run setup\n",
        "observed_heads, total_flips, alpha_level = setup_coin_flip_scenario()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HgeOjrGXNeOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning (from source context):**\n",
        "This initial setup is crucial for any hypothesis test. It clearly states the assumptions and the question being investigated. The null hypothesis represents the \"no effect\" or \"status quo\" (a fair coin), while the alternative hypothesis is what we're trying to find evidence for (an unfair coin, biased towards tails in this one-sided test example)[cite: 232, 234]. The significance level $\\\\alpha$ sets our threshold for how unlikely an event must be under $H\\_0$ for us to reject $H\\_0$[cite: 233].\n",
        "\n",
        "#### Cell 2: Calculating the P-value\n",
        "\n",
        "**Explanation:** This cell performs the core calculation of the P-value[cite: 241]. It uses the `binom.cdf` function from `scipy.stats` to calculate the cumulative probability of getting 3 or fewer heads in 10 flips, assuming the null hypothesis ($P(H)=0.5$) is true[cite: 242]. This calculated probability is the P-value for this one-sided test (Ha: P(Heads) \\< 0.5)[cite: 236, 243]. The source states this P-value is 17.1%[cite: 185, 243]."
      ],
      "metadata": {
        "id": "q77VdmXINeOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_p_value_coin_flip(observed_heads, total_flips, assumed_prob_h0=0.5):\n",
        "    # The null distribution is Binomial(n=total_flips, p=assumed_prob_h0) [cite: 235]\n",
        "    # We need the cumulative probability of getting our observed value (observed_heads)\n",
        "    # or less extreme values in the direction of the alternative hypothesis (less than 0.5). [cite: 235, 236]\n",
        "    # This is the CDF at observed_heads. [cite: 237]\n",
        "    p_value = binom.cdf(observed_heads, total_flips, assumed_prob_h0) # [cite: 238]\n",
        "    p_value_percent = round(p_value * 100, 1)\n",
        "\n",
        "    print(f\"\\nNull Hypothesis (H0): P(Heads) = {assumed_prob_h0}\") # [cite: 238]\n",
        "    print(f\"Observed Data: {observed_heads} heads in {total_flips} flips\") # [cite: 238]\n",
        "    print(f\"\\nCalculating the P-value...\") # [cite: 238]\n",
        "    print(f\"The probability of observing {observed_heads} or fewer heads, assuming the coin is fair (P(Heads)={assumed_prob_h0}), is approximately {p_value_percent}%.\") # [cite: 238]\n",
        "    # Source calculation check [cite: 240]\n",
        "    # calculated_p_value = binom.cdf(3, 10, 0.5)\n",
        "    # calculated_p_value_percent = round(calculated_p_value * 100, 1)\n",
        "    # print(f\"(Calculation check: binom.cdf(3, 10, 0.5) = {calculated_p_value_percent}%) - This matches the source's value.\")\n",
        "    print(f\"\\nP-value = {p_value:.4f}\") # [cite: 240]\n",
        "    return p_value\n",
        "\n",
        "# Run P-value calculation using values from setup\n",
        "# (H0 is fair coin, P(H)=0.5, Ha is P(H)<0.5)\n",
        "# Observed 3 heads in 10 flips\n",
        "p_value_calculated = calculate_p_value_coin_flip(observed_heads, total_flips)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rxUK5c8VNeOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning (from source context):**\n",
        "The P-value quantifies the evidence against the null hypothesis. A small P-value indicates that the observed data (or data even more extreme) is unlikely if the null hypothesis is true[cite: 173]. In this specific one-sided test (Ha: $P(H) \\< 0.5$), \"as or more extreme\" than 3 heads means 3, 2, 1, or 0 heads[cite: 236]. The `binom.cdf` function calculates exactly this cumulative probability[cite: 238, 242].\n",
        "\n",
        "#### Cell 3: Decision Based on P-value\n",
        "\n",
        "**Explanation:** This cell demonstrates the final step of a frequentist hypothesis test: comparing the calculated P-value to the pre-determined significance level ($\\\\alpha$)[cite: 248]. The source explicitly states that if the P-value is 17.1% and $\\\\alpha$ is 0.05, you do not reject the null hypothesis[cite: 185, 247]. This code implements that logic[cite: 248]."
      ],
      "metadata": {
        "id": "KFEZ-kuNNeOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_decision_coin_flip(p_value, alpha):\n",
        "    print(f\"\\nSignificance Level (alpha) = {alpha}\") # [cite: 241] (implicitly using alpha from setup)\n",
        "    print(f\"P-value ({p_value:.4f}) vs Alpha ({alpha})\") # [cite: 244]\n",
        "\n",
        "    if p_value <= alpha:\n",
        "        print(\"\\nDecision: P-value <= Alpha. Reject the Null Hypothesis.\") # [cite: 244]\n",
        "        print(f\"Reasoning: The observed data is statistically significant at the {alpha*100}% level.\") # [cite: 244]\n",
        "        print(\"It is unlikely to have occurred by random chance if the coin were fair (as per H0).\") # [cite: 244]\n",
        "    else:\n",
        "        print(\"\\nDecision: P-value > Alpha. Fail to reject the Null Hypothesis.\") # [cite: 244]\n",
        "        print(f\"Reasoning: The observed data is NOT statistically significant at the {alpha*100}% level.\") # [cite: 244]\n",
        "        # Line below adapted from source [cite: 245]\n",
        "        print(f\"The result is reasonably likely to occur by random chance even if the coin were fair (as per H0).\")\n",
        "        # Line below adapted from source for the specific example [cite: 246]\n",
        "        print(f\"In this case, since the P-value ({round(p_value*100,1)}%) is > Alpha ({alpha*100}%), we Fail to reject the Null Hypothesis.\")\n",
        "        print(\"We conclude that we do not have enough evidence to say the coin is unfair (biased towards tails) based on this sample.\") # [cite: 246]\n",
        "\n",
        "# Make decision using calculated p_value and alpha_level from setup\n",
        "make_decision_coin_flip(p_value_calculated, alpha_level)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fOAXHkY-NeOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning (from source context):**\n",
        "The decision rule is straightforward: if the P-value is less than or equal to $\\\\alpha$, the result is deemed statistically significant, and we reject $H\\_0$[cite: 174, 175]. If the P-value is greater than $\\\\alpha$, the result is not statistically significant, and we fail to reject $H\\_0$[cite: 176, 177]. Failing to reject $H\\_0$ doesn't prove $H\\_0$ is true; it simply means the current data doesn't provide strong enough evidence to discard it[cite: 110].\n",
        "\n",
        "#### Cell 4: Simulating Interactive Input for Coin Flip Test\n",
        "\n",
        "**Explanation:** This cell encapsulates the hypothesis testing process into a function that takes user input, simulating interactivity[cite: 249, 257]. It guides the user through defining the parameters of their own coin flip experiment (number of flips, observed heads, significance level)[cite: 249]. It then calculates the P-value using `binom.cdf` (assuming $H\\_0: p=0.5$ and a one-sided test for $p\\<0.5$, consistent with the source's example calculation)[cite: 256, 258, 259]. Finally, it applies the decision rule and provides a conclusion[cite: 258]."
      ],
      "metadata": {
        "id": "A_VR4-iINeOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_interactive_coin_flip_test():\n",
        "    \"\"\"Runs a simulated interactive coin flip hypothesis test (one-sided, Ha: p < 0.5).\"\"\" # [cite: 249]\n",
        "    print(\"\\n\\n--- Interactive Coin Flip Hypothesis Test ---\") # [cite: 249]\n",
        "    print(\"Null Hypothesis (H0): Coin is Fair (P(Heads) = 0.5)\") # [cite: 249]\n",
        "    print(\"Alternative Hypothesis (Ha): Coin is Unfair (P(Heads) < 0.5) - One-sided test\") # [cite: 249]\n",
        "\n",
        "    try:\n",
        "        total_flips_str = input(\"\\nEnter the total number of coin flips: \") # [cite: 249]\n",
        "        total_flips = int(total_flips_str)\n",
        "        observed_heads_str = input(f\"Enter the number of heads observed out of {total_flips} flips: \") # [cite: 249]\n",
        "        observed_heads = int(observed_heads_str)\n",
        "        alpha_str = input(\"Enter the significance level (e.g., 0.05 for 5%): \") # [cite: 250]\n",
        "        alpha = float(alpha_str)\n",
        "\n",
        "        if observed_heads > total_flips or observed_heads < 0: # [cite: 250]\n",
        "            print(\"Invalid input: Number of heads cannot be more than total flips or less than 0.\") # [cite: 250]\n",
        "            return\n",
        "        if not (0 < alpha < 1): # [cite: 250]\n",
        "            print(\"Invalid input: Significance level must be between 0 and 1 (exclusive).\") # [cite: 250]\n",
        "            return\n",
        "        if total_flips <= 0:\n",
        "            print(\"Invalid input: Total flips must be positive.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        print(f\"\\nSummary of your test:\") # [cite: 250]\n",
        "        print(f\"Total Flips: {total_flips}\") # [cite: 250]\n",
        "        print(f\"Observed Heads: {observed_heads}\") # [cite: 250]\n",
        "        print(f\"Significance Level (alpha): {alpha}\") # [cite: 250]\n",
        "        print(f\"Null Distribution: Binomial(n={total_flips}, p=0.5)\") # [cite: 250]\n",
        "\n",
        "        # Calculate the P-value for Ha: p < 0.5 [cite: 250, 256]\n",
        "        p_value = binom.cdf(observed_heads, total_flips, 0.5) # [cite: 256]\n",
        "        print(f\"\\nCalculated P-value: {p_value:.4f}\") # [cite: 256]\n",
        "\n",
        "        print(f\"\\nDecision Rule: Reject H0 if P-value <= Alpha ({alpha})\") # [cite: 256]\n",
        "\n",
        "        if p_value <= alpha:\n",
        "            print(\"Decision: Reject the Null Hypothesis.\") # [cite: 256]\n",
        "            print(f\"Conclusion: The observed data ({observed_heads} heads) is statistically significant.\") # [cite: 256]\n",
        "            print(\"It is unlikely to occur by chance if the coin were fair.\") # [cite: 256]\n",
        "            print(\"We have evidence to suggest the coin is unfair (specifically, biased towards tails).\") # [cite: 256]\n",
        "        else:\n",
        "            print(\"Decision: Fail to reject the Null Hypothesis.\") # [cite: 256]\n",
        "            print(f\"Conclusion: The observed data ({observed_heads} heads) is NOT statistically significant.\") # [cite: 257]\n",
        "            print(\"The result is reasonably likely to occur by chance even if the coin were fair.\") # [cite: 257]\n",
        "            print(\"We do not have enough evidence to suggest the coin is unfair based on this sample.\") # [cite: 257]\n",
        "\n",
        "        print(\"\\n--- Test Complete ---\") # [cite: 257]\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers.\") # [cite: 250]\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# To run the interactive test:\n",
        "# run_interactive_coin_flip_test()\n",
        "print(\"\\nTo run the interactive coin flip test, uncomment 'run_interactive_coin_flip_test()' in the code and re-run the cell.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8xspWPV3NeOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning (from source context):**\n",
        "This interactive function allows a user to experiment with different inputs (number of flips, observed heads, alpha) and see how the outcome of the hypothesis test changes[cite: 258]. This directly demonstrates the relationship between observed data, P-value, chosen significance level, and the resulting statistical decision, reinforcing the concepts of hypothesis testing in a hands-on manner[cite: 258]. The code is set up for a one-sided test ($P(H)\\<0.5$) to align with the P-value calculation example in the source[cite: 259].\n",
        "\n",
        "#### Cell 5: Calculating the Critical Value / Cutoff\n",
        "\n",
        "**Explanation:** This cell allows the user to determine the **critical value** or cutoff number of heads needed to reject the null hypothesis of a fair coin at a specified significance level for a one-sided test ($P(H)\\<0.5$)[cite: 262, 278]. It uses the inverse cumulative distribution function (`binom.ppf` from `scipy.stats`)[cite: 263, 279]. The `ppf(q, n, p)` function returns the largest integer `k` such that $P(X \\\\le k) \\\\le q$[cite: 263, 279]. This defines the boundary of the rejection region[cite: 280]. The cell also includes logic to discuss the right-sided test cutoff for comparison, as mentioned in the source[cite: 267, 276, 280]."
      ],
      "metadata": {
        "id": "GM3BttUKNeOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_coin_flip_cutoff():\n",
        "    \"\"\"Calculates the number of heads needed to reject the null hypothesis (Ha: p < 0.5).\"\"\" # [cite: 262]\n",
        "    print(\"\\n\\n--- Calculate Coin Flip Cutoff (Critical Value) ---\") # [cite: 262]\n",
        "    print(\"Null Hypothesis (H0): Coin is Fair (P(Heads) = 0.5)\") # [cite: 262]\n",
        "    print(\"Alternative Hypothesis (Ha): Coin is Unfair (P(Heads) < 0.5) - One-sided test\") # [cite: 262]\n",
        "\n",
        "    try:\n",
        "        total_flips_str = input(\"\\nEnter the total number of coin flips (e.g., 10 or 100): \") # [cite: 262]\n",
        "        total_flips = int(total_flips_str)\n",
        "        alpha_str = input(\"Enter the significance level (alpha, e.g., 0.05 for 5%): \") # [cite: 262]\n",
        "        alpha = float(alpha_str)\n",
        "\n",
        "        if total_flips <= 0: # [cite: 262]\n",
        "            print(\"Invalid input: Total flips must be positive.\") # [cite: 262]\n",
        "            return\n",
        "        if not (0 < alpha < 1): # [cite: 262]\n",
        "            print(\"Invalid input: Significance level must be between 0 and 1 (exclusive).\") # [cite: 262]\n",
        "            return\n",
        "\n",
        "        print(f\"\\nSummary of your calculation:\") # [cite: 263]\n",
        "        print(f\"Total Flips: {total_flips}\") # [cite: 263]\n",
        "        print(f\"Significance Level (alpha): {alpha}\") # [cite: 263]\n",
        "        print(f\"Null Distribution: Binomial(n={total_flips}, p=0.5)\") # [cite: 263]\n",
        "\n",
        "        # For Ha: p < 0.5 (left tail), reject H0 if observed_heads <= k\n",
        "        # k is the largest integer such that P(X <= k | H0) <= alpha\n",
        "        cutoff_left_tail = binom.ppf(alpha, total_flips, 0.5) # [cite: 264]\n",
        "\n",
        "        print(f\"\\nCalculating the cutoff number of heads for Ha: P(Heads) < 0.5...\") # [cite: 264]\n",
        "        # print(f\"Using the inverse CDF (percentile function) to find the value k such that P(X <= k | H0) <= alpha\") # [cite: 264] (simplified in output)\n",
        "        print(f\"\\nCritical Value (Cutoff) for left-tailed test = {cutoff_left_tail:.0f} heads\") # [cite: 264]\n",
        "        print(f\"Rejection Region: Observe {cutoff_left_tail:.0f} or fewer heads.\") # [cite: 265]\n",
        "        print(f\"Acceptance Region (Fail to Reject): Observe more than {cutoff_left_tail:.0f} heads.\") # [cite: 265]\n",
        "\n",
        "        prob_at_cutoff = binom.cdf(cutoff_left_tail, total_flips, 0.5) # [cite: 266]\n",
        "        print(f\"\\nExplanation for left-tailed test:\") # [cite: 266]\n",
        "        print(f\"If you observe {cutoff_left_tail:.0f} heads or fewer, the P-value (P(X<={cutoff_left_tail:.0f})) will be {prob_at_cutoff:.4f}, which is <= alpha ({alpha}).\") # [cite: 266]\n",
        "        print(f\"Thus, observing {cutoff_left_tail:.0f} or fewer heads leads to rejecting H0 at the {alpha*100}% significance level.\") # [cite: 266]\n",
        "\n",
        "        # For comparison: Right-sided test (Ha: P(Heads) > 0.5) cutoff logic from source [cite: 275, 276]\n",
        "        # Reject H0 if observed_heads >= k\n",
        "        # k is the smallest integer such that P(X >= k | H0) <= alpha\n",
        "        # This means P(X < k | H0) >= 1 - alpha\n",
        "        # So, k-1 = binom.ppf(1-alpha, total_flips, 0.5)\n",
        "        # k = binom.ppf(1-alpha, total_flips, 0.5) + 1\n",
        "        if alpha == 0.05 and total_flips == 100: # Matching source example conditions [cite: 267, 277]\n",
        "            cutoff_right_tail = binom.ppf(1 - alpha, total_flips, 0.5) + 1 # [cite: 276] (This logic matches source for n=100, alpha=0.05 to get k=59)\n",
        "            print(f\"\\nFor a right-sided test (Ha: P(Heads) > 0.5) with n=100, alpha=5%, the cutoff would be {cutoff_right_tail:.0f} heads or more.\") # [cite: 277]\n",
        "            print(f\"Observing {cutoff_right_tail:.0f} or more heads would lead to rejecting H0 in that specific right-tailed test scenario.\") # [cite: 277]\n",
        "\n",
        "\n",
        "        print(\"\\n--- Calculation Complete ---\") # [cite: 277]\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter numbers.\") # [cite: 262]\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# To run the interactive cutoff calculation:\n",
        "# calculate_coin_flip_cutoff()\n",
        "print(\"\\nTo run the interactive cutoff calculation, uncomment 'calculate_coin_flip_cutoff()' in the code and re-run the cell.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8IATZ9rhNeOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning (from source context):**\n",
        "Instead of calculating a P-value from observed data, this function determines the threshold (critical value) for the data itself[cite: 262]. If your observed number of heads falls into the rejection region (e.g., at or below the calculated cutoff for a left-tailed test), you reject $H\\_0$[cite: 264, 265]. This is an alternative way to perform a hypothesis test if you prefer to define the rejection region on the scale of the data rather than the P-value scale. The source explains this using `ppf` (percent point function, or inverse CDF) for a binomial distribution[cite: 263, 264]. The comparison for a right-tailed test, as discussed in the source for 100 flips, is also included to show how the cutoff changes with the direction of the alternative hypothesis.\n",
        "\n",
        "-----\n",
        "\n",
        "### 15\\. Exercise: 50 Questions with Hints (from Source Material) 📝\n",
        "\n",
        "Here are 50 questions covering the topics discussed, along with hints, based on your provided source material[cite: 281]. Each question and its hint are presented in separate blocks for clarity[cite: 282].\n",
        "\n",
        "**Cell 1: Question 1** [cite: 283]\n",
        "Question 1: What is the primary difference between estimation and statistical inference as described in the sources? [cite: 283]\n",
        "\n",
        "**Cell 1: Hint 1** [cite: 284]\n",
        "Hint 1: One provides a value for a parameter from sample data, while the other seeks to understand the properties of the underlying population distribution based on that sample data. [cite: 284]\n",
        "\n",
        "**Cell 2: Question 2** [cite: 285]\n",
        "Question 2: According to the sources, how does the calculation of the mean from sample data relate to the concept of estimation? [cite: 285]\n",
        "\n",
        "**Cell 2: Hint 2** [cite: 286]\n",
        "Hint 2: The mean is a common parameter; its calculation is an example of obtaining a value for this parameter from the sample. [cite: 286]\n",
        "\n",
        "**Cell 3: Question 3** [cite: 287]\n",
        "Question 3: In the customer churn example, how does providing a 95% confidence interval for the impact of customer tenure exemplify statistical inference beyond just estimation? [cite: 287]\n",
        "\n",
        "**Cell 3: Hint 3** [cite: 288]\n",
        "Hint 3: A point estimate gives a single value (e.g., 20% less likely to churn)[cite: 288]. An interval provides a measure of uncertainty and reliability for that estimate regarding the population[cite: 289].\n",
        "\n",
        "**Cell 4: Question 4** [cite: 290]\n",
        "Question 4: How are machine learning and statistical inference described as being similar in the sources? [cite: 290]\n",
        "\n",
        "**Cell 4: Hint 4** [cite: 291]\n",
        "Hint 4: Both use sample data to learn about the characteristics of the underlying population and the processes that generated the data. [cite: 291]\n",
        "\n",
        "**Cell 5: Question 5** [cite: 292]\n",
        "Question 5: What distinguishes a parametric statistical model from a non-parametric model according to the sources? [cite: 292]\n",
        "\n",
        "**Cell 5: Hint 5** [cite: 293]\n",
        "Hint 5: One relies on strict assumptions about the data's distribution and a finite number of parameters, while the other is distribution-free and makes fewer assumptions. [cite: 293]\n",
        "\n",
        "**Cell 6: Question 6** [cite: 294]\n",
        "Question 6: The sources mention using a histogram to create a distribution based on actual data[cite: 294]. Is this an example of a parametric or non-parametric approach, and why? [cite: 295]\n",
        "\n",
        "**Cell 6: Hint 6** [cite: 296]\n",
        "Hint 6: This approach relies directly on the data shape rather than assuming a specific distribution family with predefined parameters. [cite: 296]\n",
        "\n",
        "**Cell 7: Question 7** [cite: 297]\n",
        "Question 7: What is Maximum Likelihood Estimation (MLE), and what is its goal in parametric modeling? [cite: 297]\n",
        "\n",
        "**Cell 7: Hint 7** [cite: 298]\n",
        "Hint 7: It's a common estimation method that finds the parameter values which make the observed sample data most probable. [cite: 298]\n",
        "\n",
        "**Cell 8: Question 8** [cite: 299]\n",
        "Question 8: Describe the key characteristic of a Uniform distribution and provide the real-world example given in the sources. [cite: 299]\n",
        "\n",
        "**Cell 8: Hint 8** [cite: 300]\n",
        "Hint 8: All outcomes within a range have equal probability[cite: 300]. Think about games involving random numbers[cite: 300].\n",
        "\n",
        "**Cell 9: Question 9** [cite: 301]\n",
        "Question 9: What makes the Normal (Gaussian) distribution so popular in statistics, according to the sources? [cite: 301]\n",
        "\n",
        "**Cell 9: Hint 9** [cite: 302]\n",
        "Hint 9: Think about the distribution of sample averages and a key theorem related to it. [cite: 302]\n",
        "\n",
        "**Cell 10: Question 10** [cite: 303]\n",
        "Question 10: In a Normal distribution, how do the mean and standard deviation affect the shape and position of the curve? [cite: 303]\n",
        "\n",
        "**Cell 10: Hint 10** [cite: 304]\n",
        "Hint 10: One determines the center, the other determines the spread. [cite: 304]\n",
        "\n",
        "**Cell 11: Question 11** [cite: 305]\n",
        "Question 11: The sources mention household income as a real-world example of a Log-Normal distribution[cite: 305]. Explain why this distribution often fits income data. [cite: 306]\n",
        "\n",
        "**Cell 11: Hint 11** [cite: 306]\n",
        "Hint 11: Think about the typical shape of income data - where do most values cluster, and where do extreme values occur? [cite: 306]\n",
        "\n",
        "**Cell 12: Question 12** [cite: 307]\n",
        "Question 12: What real-world phenomenon is the Exponential distribution often used to model? [cite: 307]\n",
        "\n",
        "**Cell 12: Hint 12** [cite: 308]\n",
        "Hint 12: Think about the timing of events in a continuous process. [cite: 308]\n",
        "\n",
        "**Cell 13: Question 13** [cite: 309]\n",
        "Question 13: What does the Poisson distribution model, and what is special about its parameter Lambda ($\\\\lambda$)? [cite: 309]\n",
        "\n",
        "**Cell 13: Hint 13** [cite: 310]\n",
        "Hint 13: It models the count of events in a fixed interval[cite: 310]. Its parameter has two important statistical properties[cite: 311].\n",
        "\n",
        "**Cell 14: Question 14** [cite: 311]\n",
        "Question 14: In the context of queuing theory (like customer arrivals), which distribution is mentioned as being relevant for modeling the number of events over a fixed time? [cite: 311]\n",
        "\n",
        "**Cell 14: Hint 14** [cite: 312]\n",
        "Hint 14: This is the distribution specifically used for counting events in a fixed period. [cite: 312]\n",
        "\n",
        "**Cell 15: Question 15** [cite: 313]\n",
        "Question 15: What is the core difference in how Frequentist and Bayesian statistics view probability and parameters? [cite: 313]\n",
        "\n",
        "**Cell 15: Hint 15** [cite: 314]\n",
        "Hint 15: One sees probability based on long-run frequency and parameters as fixed, while the other uses probability to represent belief about parameters and updates this belief with data. [cite: 314]\n",
        "\n",
        "**Cell 16: Question 16** [cite: 315]\n",
        "Question 16: How do Bayesian statistics incorporate prior beliefs into the analysis? [cite: 315]\n",
        "\n",
        "**Cell 16: Hint 16** [cite: 316]\n",
        "Hint 16: They start with a distribution representing prior knowledge and update it with observed data. [cite: 316]\n",
        "\n",
        "**Cell 17: Question 17** [cite: 317]\n",
        "Question 17: What are the null hypothesis ($H\\_0$) and the alternative hypothesis ($H\\_a$) in hypothesis testing? [cite: 317]\n",
        "\n",
        "**Cell 17: Hint 17** [cite: 318]\n",
        "Hint 17: One states no effect or a specific value, the other contradicts it. [cite: 318]\n",
        "\n",
        "**Cell 18: Question 18** [cite: 319]\n",
        "Question 18: When formulating a null and alternative hypothesis, which one is usually the less specific statement (e.g., \\> 5 or $\\\\neq$ 5)? [cite: 319]\n",
        "\n",
        "**Cell 18: Hint 18** [cite: 320]\n",
        "Hint 18: The null hypothesis is typically the statement of equality or a specific value. [cite: 320]\n",
        "\n",
        "**Cell 19: Question 19** [cite: 321]\n",
        "Question 19: What is a test statistic in hypothesis testing? [cite: 321]\n",
        "\n",
        "**Cell 19: Hint 19** [cite: 322]\n",
        "Hint 19: It's a value calculated from sample data used to make a decision about the null hypothesis. [cite: 322]\n",
        "\n",
        "**Cell 20: Question 20** [cite: 323]\n",
        "Question 20: How does the Bayesian interpretation of hypothesis testing differ from the Frequentist approach in terms of the final outcome or decision? [cite: 323]\n",
        "\n",
        "**Cell 20: Hint 20** [cite: 324]\n",
        "Hint 20: One results in a binary decision (accept/reject null), the other provides probabilities for each hypothesis. [cite: 324]\n",
        "\n",
        "**Cell 21: Question 21** [cite: 325]\n",
        "Question 21: Explain the relationship between prior probabilities, the likelihood ratio, and posterior probabilities in Bayesian hypothesis testing. [cite: 325]\n",
        "\n",
        "**Cell 21: Hint 21** [cite: 326]\n",
        "Hint 21: Priors are updated by the likelihood ratio based on the data to yield posteriors. [cite: 326]\n",
        "\n",
        "**Cell 22: Question 22** [cite: 327]\n",
        "Question 22: What is a Type I error? [cite: 327]\n",
        "\n",
        "**Cell 22: Hint 22** [cite: 328]\n",
        "Hint 22: It's about incorrectly rejecting the null hypothesis. [cite: 328]\n",
        "\n",
        "**Cell 23: Question 23** [cite: 329]\n",
        "Question 23: What is a Type II error? [cite: 329]\n",
        "\n",
        "**Cell 23: Hint 23** [cite: 330]\n",
        "Hint 23: It's about incorrectly failing to reject (accepting) the null hypothesis. [cite: 330]\n",
        "\n",
        "**Cell 24: Question 24** [cite: 331]\n",
        "Question 24: How does the concept of \"power\" relate to Type II error? [cite: 331]\n",
        "\n",
        "**Cell 24: Hint 24** [cite: 332]\n",
        "Hint 24: Power is the probability of correctly rejecting the null, and it's mathematically linked to the probability of a Type II error. [cite: 332]\n",
        "\n",
        "**Cell 25: Question 25** [cite: 333]\n",
        "Question 25: In the customer churn example (HO: tenure has no effect on churn, Ha: longer tenure reduces churn), describe a Type I error. [cite: 334]\n",
        "\n",
        "**Cell 25: Hint 25** [cite: 335]\n",
        "Hint 25: The null is true (no real effect), but you conclude the alternative is true (tenure does matter). [cite: 335]\n",
        "\n",
        "**Cell 26: Question 26** [cite: 336]\n",
        "Question 26: In the customer churn example (HO: tenure has no effect on churn, Ha: longer tenure reduces churn), describe a Type II error. [cite: 336]\n",
        "\n",
        "**Cell 26: Hint 26** [cite: 337]\n",
        "Hint 26: The alternative is true (tenure does matter), but you conclude the null is true (no real effect). [cite: 337]\n",
        "\n",
        "**Cell 27: Question 27** [cite: 338]\n",
        "Question 27: What is the rejection region in hypothesis testing? [cite: 338]\n",
        "\n",
        "**Cell 27: Hint 27** [cite: 339]\n",
        "Hint 27: It's the range of test statistic values that lead you to reject the null hypothesis. [cite: 339]\n",
        "\n",
        "**Cell 28: Question 28** [cite: 340]\n",
        "Question 28: What is the null distribution? [cite: 340]\n",
        "\n",
        "**Cell 28: Hint 28** [cite: 341]\n",
        "Hint 28: It's the expected distribution of the test statistic if the null hypothesis were true. [cite: 341]\n",
        "\n",
        "**Cell 29: Question 29** [cite: 342]\n",
        "Question 29: In the marketing campaign example, what would be the null hypothesis? [cite: 342]\n",
        "\n",
        "**Cell 29: Hint 29** [cite: 343]\n",
        "Hint 29: The null represents the idea that the intervention had no impact. [cite: 343]\n",
        "\n",
        "**Cell 30: Question 30** [cite: 344]\n",
        "Question 30: What is the significance level ($\\\\alpha$) in frequentist hypothesis testing, and when should it be chosen? [cite: 344]\n",
        "\n",
        "**Cell 30: Hint 30** [cite: 345]\n",
        "Hint 30: It's a threshold for rejecting the null, representing the maximum acceptable probability of a Type I error[cite: 345]. Its choice happens before calculation[cite: 346].\n",
        "\n",
        "**Cell 31: Question 31** [cite: 346]\n",
        "Question 31: How does choosing a lower significance level ($\\\\alpha$) impact the likelihood of making a Type I error? [cite: 346]\n",
        "\n",
        "**Cell 31: Hint 31** [cite: 347]\n",
        "Hint 31: A stricter threshold means you need more extreme evidence to reject the null. [cite: 347]\n",
        "\n",
        "**Cell 32: Question 32** [cite: 348]\n",
        "Question 32: What is \"P-hacking,\" and why is choosing the significance level before computing the test statistic important to avoid it? [cite: 348]\n",
        "\n",
        "**Cell 32: Hint 32** [cite: 349]\n",
        "Hint 32: It involves manipulating the threshold after seeing results[cite: 349]. Pre-registration prevents this bias[cite: 349].\n",
        "\n",
        "**Cell 33: Question 33** [cite: 350]\n",
        "Question 33: What is the P-value, and how does it relate to the significance level in making a decision in frequentist hypothesis testing? [cite: 350]\n",
        "\n",
        "**Cell 33: Hint 33** [cite: 351]\n",
        "Hint 33: It's the probability of observing the data (or more extreme) under the null[cite: 351]. The decision compares this probability to the pre-set threshold[cite: 352].\n",
        "\n",
        "**Cell 34: Question 34** [cite: 352]\n",
        "Question 34: If your calculated P-value is 0.03 and your significance level ($\\\\alpha$) is 0.05, what is your decision regarding the null hypothesis? [cite: 352]\n",
        "\n",
        "**Cell 34: Hint 34** [cite: 353]\n",
        "Hint 34: Compare the P-value to alpha. Is P $\\\\le \\\\alpha$? [cite: 353]\n",
        "\n",
        "**Cell 35: Question 35** [cite: 354]\n",
        "Question 35: If your calculated P-value is 0.10 and your significance level ($\\\\alpha$) is 0.05, what is your decision regarding the null hypothesis? [cite: 354]\n",
        "\n",
        "**Cell 35: Hint 35** [cite: 355]\n",
        "Hint 35: Compare the P-value to alpha. Is P $\\\\le \\\\alpha$? [cite: 355]\n",
        "\n",
        "**Cell 36: Question 36** [cite: 356]\n",
        "Question 36: What does a small P-value (e.g., \\< 0.05) suggest about the observed data, assuming the null hypothesis is true? [cite: 356]\n",
        "\n",
        "**Cell 36: Hint 36** [cite: 357]\n",
        "Hint 36: It means the data is unlikely under the null. [cite: 357]\n",
        "\n",
        "**Cell 37: Question 37** [cite: 358]\n",
        "Question 37: What does a large P-value (e.g., \\> 0.05) suggest about the observed data, assuming the null hypothesis is true? [cite: 358]\n",
        "\n",
        "**Cell 37: Hint 37** [cite: 359]\n",
        "Hint 37: It means the data is reasonably likely under the null. [cite: 359]\n",
        "\n",
        "**Cell 38: Question 38** [cite: 360]\n",
        "Question 38: In the coin toss example (HO: $P(H)=0.5$, Ha: $P(H)\\<0.5$) observing 3 heads in 10 flips resulted in a P-value of 17.1%[cite: 360]. With $\\\\alpha=0.05$, what was the conclusion, and what does it mean? [cite: 361]\n",
        "\n",
        "**Cell 38: Hint 38** [cite: 361]\n",
        "Hint 38: Compare 17.1% to 5%[cite: 361]. Was HO rejected? What does failing to reject HO mean about the coin?[cite: 362].\n",
        "\n",
        "**Cell 39: Question 39** [cite: 363]\n",
        "Question 39: How does increasing the sample size generally affect the power of a statistical test? [cite: 363]\n",
        "\n",
        "**Cell 39: Hint 39** [cite: 364]\n",
        "Hint 39: More data usually makes it easier to detect a true effect. [cite: 364]\n",
        "\n",
        "**Cell 40: Question 40** [cite: 365]\n",
        "Question 40: What problem arises when you perform multiple hypothesis tests without adjusting the significance level? [cite: 365]\n",
        "\n",
        "**Cell 40: Hint 40** [cite: 366]\n",
        "Hint 40: The chance of incorrectly rejecting at least one true null hypothesis increases. [cite: 366]\n",
        "\n",
        "**Cell 41: Question 41** [cite: 367]\n",
        "Question 41: What is the purpose of the Bonferroni Correction when conducting multiple hypothesis tests? [cite: 367]\n",
        "\n",
        "**Cell 41: Hint 41** [cite: 368]\n",
        "Hint 41: It aims to control the overall Type I error rate across all tests. [cite: 368]\n",
        "\n",
        "**Cell 42: Question 42** [cite: 369]\n",
        "Question 42: How is the significance level typically adjusted for each individual test when using the Bonferroni Correction? [cite: 369]\n",
        "\n",
        "**Cell 42: Hint 42** [cite: 370]\n",
        "Hint 42: The overall alpha is divided by the number of tests. [cite: 370]\n",
        "\n",
        "**Cell 43: Question 43** [cite: 371]\n",
        "Question 43: What is the main trade-off or consequence of using the Bonferroni Correction? [cite: 371]\n",
        "\n",
        "**Cell 43: Hint 43** [cite: 372]\n",
        "Hint 43: Making it harder to reject the null reduces the ability to detect true effects. [cite: 372]\n",
        "\n",
        "**Cell 44: Question 44** [cite: 373]\n",
        "Question 44: According to the sources, what is a best practice when considering performing multiple hypothesis tests? [cite: 373]\n",
        "\n",
        "**Cell 44: Hint 44** [cite: 374]\n",
        "Hint 44: Don't just run tests on everything; focus your testing effort. [cite: 374]\n",
        "\n",
        "**Cell 45: Question 45** [cite: 375]\n",
        "Question 45: Explain the difference between correlation and causation. [cite: 375]\n",
        "\n",
        "**Cell 45: Hint 45** [cite: 376]\n",
        "Hint 45: One is about association, the other is about direct influence. [cite: 376]\n",
        "\n",
        "**Cell 46: Question 46** [cite: 377]\n",
        "Question 46: Why is correlation useful in data analysis, even if it doesn't imply causation? [cite: 377]\n",
        "\n",
        "**Cell 46: Hint 46** [cite: 378]\n",
        "Hint 46: If two variables are related, knowing one can help with something related to the other. [cite: 378]\n",
        "\n",
        "**Cell 47: Question 47** [cite: 379]\n",
        "Question 47: What is a confounding variable, and how can it cause a correlation between two variables that don't directly cause each other? [cite: 379]\n",
        "\n",
        "**Cell 47: Hint 47** [cite: 380]\n",
        "Hint 47: It's a third variable that influences both, creating an observed relationship even if there's no direct link. [cite: 380]\n",
        "\n",
        "**Cell 48: Question 48** [cite: 381]\n",
        "Question 48: Provide one of the real-world examples of a confounding variable causing correlation mentioned in the sources. [cite: 381]\n",
        "\n",
        "**Cell 48: Hint 48** [cite: 382]\n",
        "Hint 48: Think about summer treats and water-related incidents, or names and accidents. [cite: 382]\n",
        "\n",
        "**Cell 49: Question 49** [cite: 383]\n",
        "Question 49: What is spurious correlation? [cite: 383]\n",
        "\n",
        "**Cell 49: Hint 49** [cite: 383]\n",
        "Hint 49: It's a correlation that is merely a coincidence in a specific dataset. [cite: 383]\n",
        "\n",
        "**Cell 50: Question 50** [cite: 384]\n",
        "Question 50: The sources mention the correlation between the age of Miss America and murders by steam[cite: 384]. What does this example illustrate? [cite: 385]\n",
        "\n",
        "**Cell 50: Hint 50** [cite: 385]\n",
        "Hint 50: It's a classic, non-sensical example of a specific type of misleading correlation. [cite: 385]\n",
        "\n",
        "-----\n",
        "\n",
        "This concludes the elaboration of the topics from your provided source material, including additional explanations, new interactive Python examples, and the Q\\&A section formatted as requested, with citations back to your source where appropriate."
      ],
      "metadata": {
        "id": "4_YtEn0bNeOm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}